{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecab83a5-9a4c-4ccc-b568-2c3a7e97689d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Source data path:   C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\SOURCE_FILES\n",
      "✅ Output data path:   C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA\n",
      "📦 Locked weeks path:  C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA\\locked_weeks\n",
      "📄 Exports path:       C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA\\exports\n",
      "📊 PowerBI path:       C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA\\powerbi\n",
      "📅 Using ISO Week:     38 (2025-09-15 → 2025-09-21)\n",
      "🧩 Filters provided:   no   → resolved: {'loyalty': 'all', 'wine_type': None, 'bottle_size': 750, 'price_tier_bucket': '', 'last_stock': False, 'last_stock_threshold': 10, 'seasonality_boost': False, 'style': 'default', 'calendar_day': None}\n",
      "🔒 Locked snapshot:    yes\n",
      "✅ Environment & parameters initialized.\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 1 AVU_ignition_1.ipynb: Global Setup & Parameter Intake (Papermill/Env/UI-safe) ---\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime, date, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _to_py(o):\n",
    "    if o is None: return None\n",
    "    if isinstance(o, (np.integer,)):  return int(o)\n",
    "    if isinstance(o, (np.floating,)): return None if np.isnan(o) else float(o)\n",
    "    if isinstance(o, (np.bool_,)):    return bool(o)\n",
    "    if isinstance(o, (pd.Timestamp,)):return o.isoformat()\n",
    "    return o\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "\n",
    "# -------------------------------\n",
    "# 0) Constants used across the run\n",
    "# -------------------------------\n",
    "NUM_SLOTS = 5\n",
    "DAYS_FULL = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    "DAYS_LOWER = [d.lower() for d in DAYS_FULL]\n",
    "\n",
    "# When preferences can't be fully satisfied, later cells can consult these knobs\n",
    "ENGINE_FILL_POLICY = {\n",
    "    \"min_slots_per_day\": NUM_SLOTS,   # aim to fill all visible slots\n",
    "    \"relaxation_enabled\": True,\n",
    "    \"relaxation_order\": [\n",
    "        # later cells should progressively relax in this order until enough wines found:\n",
    "        \"loyalty\",          # treat 'all' if too strict\n",
    "        \"wine_type\",        # allow any type if filter blocks results\n",
    "        \"bottle_size\",      # allow any size if specific format too rare\n",
    "        \"price_tier\",       # widen tier ranges (Budget→Mid→Premium→Luxury→Ultra)\n",
    "        \"last_stock_only\",  # ignore last-stock constraint if needed\n",
    "        \"seasonality\"       # ignore seasonality boost as hard constraint (keep as soft weight)\n",
    "    ],\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# 1) Try to read Papermill parameters (if any)\n",
    "# -------------------------------\n",
    "_pm_params: Dict[str, Any] = {}\n",
    "try:\n",
    "    from papermill import get_parameters  # type: ignore\n",
    "    _pm_params = get_parameters() or {}\n",
    "except Exception:\n",
    "    _pm_params = {}\n",
    "\n",
    "def _coerce_int(value, fallback):\n",
    "    try:\n",
    "        return int(value)\n",
    "    except Exception:\n",
    "        return int(fallback)\n",
    "\n",
    "def _iso_week_now():\n",
    "    try:\n",
    "        return datetime.now().isocalendar().week\n",
    "    except Exception:\n",
    "        return int(datetime.now().strftime(\"%V\"))\n",
    "\n",
    "# -------------------------------\n",
    "# 2) Resolve week_number (priority: papermill -> env -> runtime default)\n",
    "# -------------------------------\n",
    "if \"week_number\" in globals():\n",
    "    _week_candidate = globals().get(\"week_number\")\n",
    "elif \"week_number\" in _pm_params:\n",
    "    _week_candidate = _pm_params.get(\"week_number\")\n",
    "else:\n",
    "    _week_candidate = os.getenv(\"WEEK_NUMBER\", _iso_week_now())\n",
    "\n",
    "week_number = _coerce_int(_week_candidate, _iso_week_now())\n",
    "# Clamp to ISO range\n",
    "if week_number < 1 or week_number > 53:\n",
    "    week_number = _iso_week_now()\n",
    "\n",
    "# Useful week helpers\n",
    "def week_bounds(iso_week: int, year: int = datetime.now().year):\n",
    "    # Monday as first day of ISO week\n",
    "    d = date.fromisocalendar(year, iso_week, 1)\n",
    "    return d, d + timedelta(days=6)\n",
    "\n",
    "week_start_date, week_end_date = week_bounds(week_number)\n",
    "\n",
    "# --- YEAR AWARENESS + EUROPE/ZURICH TZ + PARAM SUPPORT ---\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "def _iso_week_now_europe():\n",
    "    now = datetime.now(ZoneInfo(\"Europe/Zurich\"))\n",
    "    iso = now.isocalendar()\n",
    "    return iso.year, iso.week\n",
    "\n",
    "# Allow papermill/env to pass an explicit calendar year\n",
    "if \"calendar_year\" in globals():\n",
    "    _year_candidate = globals().get(\"calendar_year\")\n",
    "elif \"calendar_year\" in _pm_params:\n",
    "    _year_candidate = _pm_params.get(\"calendar_year\")\n",
    "else:\n",
    "    _year_candidate = os.getenv(\"CALENDAR_YEAR\", None)\n",
    "\n",
    "try:\n",
    "    calendar_year = int(_year_candidate) if _year_candidate is not None else _iso_week_now_europe()[0]\n",
    "except Exception:\n",
    "    calendar_year = _iso_week_now_europe()[0]\n",
    "\n",
    "# Week bounds should use the chosen calendar_year\n",
    "def week_bounds(iso_week: int, year: int):\n",
    "    # Guard invalid 53rd week for some years\n",
    "    try:\n",
    "        d = date.fromisocalendar(year, iso_week, 1)\n",
    "    except ValueError:\n",
    "        iso_week = min(max(iso_week, 1), 52)\n",
    "        d = date.fromisocalendar(year, iso_week, 1)\n",
    "    return d, d + timedelta(days=6)\n",
    "\n",
    "week_start_date, week_end_date = week_bounds(week_number, calendar_year)\n",
    "\n",
    "# -------------------------------\n",
    "# 3) Resolve IO paths (priority: papermill -> env -> sensible defaults)\n",
    "# -------------------------------\n",
    "def _resolve_path(param_key, env_key, default_path_str):\n",
    "    if param_key in globals():\n",
    "        p = globals().get(param_key)\n",
    "    elif param_key in _pm_params:\n",
    "        p = _pm_params.get(param_key)\n",
    "    else:\n",
    "        p = os.getenv(env_key, default_path_str)\n",
    "    return Path(p)\n",
    "\n",
    "# Defaults match your project structure\n",
    "_default_source = str(Path.home() / \"OneDrive - AVU SA\" / \"AVU CPI Campaign\" / \"Puzzle_control_Reports\" / \"SOURCE_FILES\")\n",
    "_default_output = str(Path.home() / \"OneDrive - AVU SA\" / \"AVU CPI Campaign\" / \"Puzzle_control_Reports\" / \"IRON_DATA\")\n",
    "\n",
    "SOURCE_PATH: Path = _resolve_path(\"input_path\", \"INPUT_PATH\", _default_source)\n",
    "OUTPUT_PATH: Path = _resolve_path(\"output_path\", \"OUTPUT_PATH\", _default_output)\n",
    "\n",
    "# Make sure all dirs exist (incl. subfolders we’ll use)\n",
    "SOURCE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "LOCKED_PATH: Path = OUTPUT_PATH / \"locked_weeks\"\n",
    "CALENDAR_PATH: Path = OUTPUT_PATH / \"calendar\"       # where schedule JSONs per week can live\n",
    "EXPORTS_PATH: Path = OUTPUT_PATH / \"exports\"         # generic exports\n",
    "POWERBI_PATH: Path = OUTPUT_PATH / \"powerbi\"         # enriched Excel for Power BI\n",
    "TMP_PATH: Path = OUTPUT_PATH / \"_tmp\"\n",
    "\n",
    "for p in (LOCKED_PATH, CALENDAR_PATH, EXPORTS_PATH, POWERBI_PATH, TMP_PATH):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Optional reports path (kept for compatibility)\n",
    "REPORTS_PATH: Path = Path.home() / \"OneDrive - AVU SA\" / \"AVU CPI Campaign\" / \"non_recipient_reports\"\n",
    "\n",
    "# -------------------------------\n",
    "# 4) Load lightweight UI inputs if present (non-fatal if missing)\n",
    "#    These files are written by the Flask app before launching the notebook.\n",
    "# -------------------------------\n",
    "NOTEBOOKS_DIR = Path(\"notebooks\")\n",
    "FILTERS_FILE = NOTEBOOKS_DIR / \"filters.json\"\n",
    "LOCKED_SNAPSHOT_FILE = NOTEBOOKS_DIR / \"locked_calendar.json\"  # snapshot of UI locks passed in\n",
    "\n",
    "def _load_json_or_empty(path: Path):\n",
    "    try:\n",
    "        if path.exists() and path.stat().st_size > 0:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return {}\n",
    "\n",
    "raw_filters: Dict[str, Any] = _load_json_or_empty(FILTERS_FILE)              # may be {}\n",
    "_locked_raw = _load_json_or_empty(LOCKED_SNAPSHOT_FILE)                       # may be {}\n",
    "\n",
    "# -------- Canonicalize UI/legacy locks to one shape --------\n",
    "# Accepts:\n",
    "#   - {\"weekly_calendar\": {day: [slot...]}}\n",
    "#   - {day: [slot...]}\n",
    "#   - {day: {\"main\":[slot...], \"overflow\":[...]}}\n",
    "# Returns:\n",
    "#   {day: {\"main\": [None-padded to NUM_SLOTS], \"overflow\": list}}\n",
    "def _coerce_locked_snapshot(snap, days=None, num_slots=NUM_SLOTS):\n",
    "    if days is None:\n",
    "        days = DAYS_FULL\n",
    "\n",
    "    # unwrap {\"weekly_calendar\": {...}} if present\n",
    "    if isinstance(snap, dict) and isinstance(snap.get(\"weekly_calendar\"), dict):\n",
    "        snap = snap[\"weekly_calendar\"]\n",
    "\n",
    "    out = {}\n",
    "    for d in days:\n",
    "        node = (snap or {}).get(d) or (snap or {}).get(d.lower())\n",
    "        main, overflow = [], []\n",
    "        if isinstance(node, list):\n",
    "            main = node\n",
    "        elif isinstance(node, dict):\n",
    "            if isinstance(node.get(\"main\"), list):\n",
    "                main = node.get(\"main\") or []\n",
    "            elif isinstance(node.get(\"slots\"), list):\n",
    "                main = node.get(\"slots\") or []\n",
    "            else:\n",
    "                # tolerate indexed dicts {0:...,1:...}\n",
    "                tmp = []\n",
    "                for i in range(num_slots):\n",
    "                    v = node.get(i, node.get(str(i)))\n",
    "                    tmp.append(v if v else None)\n",
    "                main = tmp\n",
    "            if isinstance(node.get(\"overflow\"), list):\n",
    "                overflow = node.get(\"overflow\") or []\n",
    "        # clamp & pad main\n",
    "        main = (main or [])[:num_slots]\n",
    "        if len(main) < num_slots:\n",
    "            main = main + [None] * (num_slots - len(main))\n",
    "        out[d] = {\"main\": [x if x else None for x in main], \"overflow\": list(overflow or [])}\n",
    "    return out\n",
    "\n",
    "def _load_json(path: Path):\n",
    "    try:\n",
    "        if path.exists() and path.stat().st_size > 0:\n",
    "            return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# Normalize the incoming UI snapshot\n",
    "LOCKED_SNAPSHOT_NORM = _coerce_locked_snapshot(_locked_raw, DAYS_FULL, NUM_SLOTS)\n",
    "\n",
    "# If empty, fallback to persisted week file\n",
    "def _is_empty_main(snap_norm):\n",
    "    return not any(any(snap_norm[d][\"main\"]) for d in DAYS_FULL)\n",
    "\n",
    "if _is_empty_main(LOCKED_SNAPSHOT_NORM):\n",
    "    persisted = _load_json(LOCKED_PATH / f\"locked_calendar_{calendar_year}_week_{week_number}.json\")\n",
    "    LOCKED_SNAPSHOT_NORM = _coerce_locked_snapshot(persisted, DAYS_FULL, NUM_SLOTS)\n",
    "\n",
    "# Export *both* forms for downstream cells:\n",
    "# - EFFECTIVE_LOCKS / LOCKED_CALENDAR: {day: [slot,..]} (Cell 6/7 expect this)\n",
    "# - locked_calendar_snapshot: {day: {\"main\":[...], \"overflow\":[...]}} (for any structured readers)\n",
    "EFFECTIVE_LOCKS = {d: LOCKED_SNAPSHOT_NORM[d][\"main\"] for d in DAYS_FULL}\n",
    "LOCKED_CALENDAR = dict(EFFECTIVE_LOCKS)  # alias used by some cells\n",
    "locked_calendar_snapshot = dict(LOCKED_SNAPSHOT_NORM)  # keep structured version for completeness\n",
    "\n",
    "# -------------------------------\n",
    "# 5) Helpers: normalize filters & day keys\n",
    "# -------------------------------\n",
    "PRICE_BUCKETS = [\"Budget\",\"Mid-range\",\"Premium\",\"Luxury\",\"Ultra Luxury\"]\n",
    "\n",
    "def price_tier_label(price: float) -> str:\n",
    "    try:\n",
    "        p = float(price)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    if p < 50: return \"Budget\"\n",
    "    elif p < 100: return \"Mid-range\"\n",
    "    elif p < 200: return \"Premium\"\n",
    "    elif p < 500: return \"Luxury\"\n",
    "    else: return \"Ultra Luxury\"\n",
    "\n",
    "DEFAULT_FILTERS: Dict[str, Any] = {\n",
    "    \"loyalty\": \"all\",\n",
    "    \"wine_type\": None,        # None = all\n",
    "    \"bottle_size\": 750,       # ml\n",
    "    \"price_tier_bucket\": \"\",  # \"\" = all\n",
    "    \"last_stock\": False,\n",
    "    \"last_stock_threshold\": 10,\n",
    "    \"seasonality_boost\": False,\n",
    "    \"style\": \"default\",\n",
    "    \"calendar_day\": None\n",
    "}\n",
    "\n",
    "def _as_bool(x): \n",
    "    if isinstance(x, bool): return x\n",
    "    if isinstance(x, (int, float)): return bool(x)\n",
    "    if isinstance(x, str): return x.strip().lower() in {\"1\",\"true\",\"yes\",\"y\",\"on\"}\n",
    "    return False\n",
    "\n",
    "def _as_int(x, default=None):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def normalize_filters(f: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    f = dict(f or {})\n",
    "    out = dict(DEFAULT_FILTERS)\n",
    "\n",
    "    out[\"loyalty\"] = str(f.get(\"loyalty\", out[\"loyalty\"])).strip().lower() or \"all\"\n",
    "\n",
    "    wt = f.get(\"wine_type\", out[\"wine_type\"])\n",
    "    if isinstance(wt, str) and wt.strip().lower() == \"all\":\n",
    "        wt = None\n",
    "    out[\"wine_type\"] = wt\n",
    "\n",
    "    out[\"bottle_size\"] = _as_int(f.get(\"bottle_size\", out[\"bottle_size\"]), 750)\n",
    "\n",
    "    pt = str(f.get(\"price_tier_bucket\", out[\"price_tier_bucket\"])).strip()\n",
    "    out[\"price_tier_bucket\"] = pt if pt in PRICE_BUCKETS or pt == \"\" else \"\"\n",
    "\n",
    "    out[\"last_stock\"] = _as_bool(f.get(\"last_stock\", out[\"last_stock\"]))\n",
    "    out[\"last_stock_threshold\"] = _as_int(f.get(\"last_stock_threshold\", out[\"last_stock_threshold\"]), 10)\n",
    "\n",
    "    out[\"seasonality_boost\"] = _as_bool(f.get(\"seasonality_boost\", out[\"seasonality_boost\"]))\n",
    "\n",
    "    style = str(f.get(\"style\", out[\"style\"])).strip().lower()\n",
    "    out[\"style\"] = style if style in {\"default\",\"cat\",\"nigo\"} else \"default\"\n",
    "\n",
    "    cd = f.get(\"calendar_day\", None)\n",
    "    out[\"calendar_day\"] = str(cd) if cd else None\n",
    "\n",
    "    return out\n",
    "\n",
    "filters = normalize_filters(raw_filters)\n",
    "\n",
    "# -------------------------------\n",
    "# 6) Echo configuration\n",
    "# -------------------------------\n",
    "print(f\"✅ Source data path:   {SOURCE_PATH.resolve()}\")\n",
    "print(f\"✅ Output data path:   {OUTPUT_PATH.resolve()}\")\n",
    "print(f\"📦 Locked weeks path:  {LOCKED_PATH.resolve()}\")\n",
    "print(f\"📄 Exports path:       {EXPORTS_PATH.resolve()}\")\n",
    "print(f\"📊 PowerBI path:       {POWERBI_PATH.resolve()}\")\n",
    "print(f\"📅 Using ISO Week:     {week_number} ({week_start_date} → {week_end_date})\")\n",
    "print(f\"🧩 Filters provided:   {'yes' if raw_filters else 'no'}   → resolved: {filters}\")\n",
    "print(f\"🔒 Locked snapshot:    {'yes' if any(EFFECTIVE_LOCKS.get(d) for d in DAYS_FULL) else 'no'}\")\n",
    "print(\"✅ Environment & parameters initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed343c89-8c60-4258-88c7-394df9e86fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 Week 38 (Sep 15 – Sep 21) | Season: Autumn\n",
      "🗂  Occasion: 'Notreceived'  → Folder: 2025_W38_Autumn_Notreceived\n",
      "✅ Report subfolder ready at: C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\non_recipient_reports\\2025_W38_Autumn_Notreceived\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 2: Set Occasion/Subfolder Path ---\n",
    "# Creates a deterministic subfolder for this run inside REPORTS_PATH.\n",
    "# Safe with/without papermill, and robust if OCCASION is empty.\n",
    "\n",
    "from datetime import datetime, timedelta, date\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Fallback for _iso_week_now_europe if Cell 1 wasn't executed\n",
    "try:\n",
    "    _iso_week_now_europe  # type: ignore\n",
    "except NameError:\n",
    "    try:\n",
    "        from zoneinfo import ZoneInfo\n",
    "    except Exception:\n",
    "        ZoneInfo = None  # type: ignore\n",
    "    def _iso_week_now_europe():\n",
    "        now = datetime.now(ZoneInfo(\"Europe/Zurich\")) if ZoneInfo else datetime.now()\n",
    "        iso = now.isocalendar()\n",
    "        return iso.year, iso.week\n",
    "\n",
    "# Use selected week/year from Cell 1 when available\n",
    "try:\n",
    "    _wk = int(week_number)\n",
    "except Exception:\n",
    "    _wk = _iso_week_now_europe()[1]\n",
    "\n",
    "try:\n",
    "    _year = int(calendar_year)\n",
    "except Exception:\n",
    "    _year = _iso_week_now_europe()[0]\n",
    "\n",
    "def _week_bounds(iso_week: int, year: int):\n",
    "    try:\n",
    "        start = date.fromisocalendar(year, iso_week, 1)\n",
    "    except ValueError:\n",
    "        start = date.fromisocalendar(year, min(max(iso_week, 1), 52), 1)\n",
    "    end = start + timedelta(days=6)\n",
    "    return start, end\n",
    "\n",
    "# Prefer globals from Cell 1 if present\n",
    "try:\n",
    "    _week_start = week_start_date; _week_end = week_end_date\n",
    "except Exception:\n",
    "    _week_start, _week_end = _week_bounds(_wk, _year)\n",
    "\n",
    "def _season_from_week(week: int) -> str:\n",
    "    if 9 <= week <= 21:\n",
    "        return \"Spring\"\n",
    "    elif 22 <= week <= 35:\n",
    "        return \"Summer\"\n",
    "    elif 36 <= week <= 48:\n",
    "        return \"Autumn\"\n",
    "    else:\n",
    "        return \"Winter\"\n",
    "\n",
    "_season = _season_from_week(_wk)\n",
    "\n",
    "# --- Read OCCASION from globals/papermill/env; keep your default ---\n",
    "try:\n",
    "    OCCASION  # type: ignore  # if already defined above, leave it\n",
    "except NameError:\n",
    "    OCCASION = os.getenv(\"OCCASION\", \"Notreceived\")\n",
    "\n",
    "# Optional “forced_day” kept for compatibility; ignored if falsy\n",
    "try:\n",
    "    forced_day  # type: ignore\n",
    "except NameError:\n",
    "    forced_day = None\n",
    "\n",
    "def _slugify(s: str) -> str:\n",
    "    s = (s or \"\").strip()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = s.replace(\"&\", \"and\").replace(\"/\", \"-\")\n",
    "    s = re.sub(r\"[^A-Za-z0-9_-]+\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s or \"\"\n",
    "\n",
    "# Compose a stable, human-readable folder name\n",
    "_occ_slug = _slugify(str(OCCASION))\n",
    "_base = f\"{_year}_W{_wk:02d}_{_season}\"\n",
    "folder_name = f\"{_base}_{_occ_slug}\" if _occ_slug else (_slugify(str(forced_day)) or _base)\n",
    "\n",
    "# Ensure REPORTS_PATH exists (from Cell 1); fall back to IRON_DATA/non_recipient_reports if missing\n",
    "try:\n",
    "    REPORTS_PATH\n",
    "except NameError:\n",
    "    REPORTS_PATH = Path.home() / \"OneDrive - AVU SA\" / \"AVU CPI Campaign\" / \"non_recipient_reports\"\n",
    "REPORTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "report_subfolder = REPORTS_PATH / folder_name\n",
    "report_subfolder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Also expose a canonical var some downstream cells may prefer\n",
    "REPORT_RUN_PATH = report_subfolder\n",
    "\n",
    "print(\n",
    "    f\"📅 Week {_wk} \"\n",
    "    f\"({_week_start.strftime('%b %d')} – {_week_end.strftime('%b %d')}) \"\n",
    "    f\"| Season: {_season}\"\n",
    ")\n",
    "print(f\"🗂  Occasion: {OCCASION!r}  → Folder: {folder_name}\")\n",
    "print(f\"✅ Report subfolder ready at: {report_subfolder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64108b9f-dceb-4070-a4f3-d9566f088a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Columns in Lines: ['Salesperson Code', 'Total Order No', 'Last Offer Sent Date', 'Turnover (€)', 'Sent Emails Today', 'Sent Emails Scheduled', 'Excluded', 'Unsubscribed', 'Contact Company Name', 'Contact Person Name', 'Country', 'City', 'Language Code', 'Contact Email', 'Contact Email CC', 'Email Sent', 'Send Email Error', 'Contact Phone No.', 'Contact Mobile Phone No.', 'Informal', 'Contact No.', 'Included as New Customer', 'First Purchase Date', 'Bought Wine with exceed price', 'Bought Wine in Parent Campaign(Recall)', 'Unsubsc. for Recall', 'Buy from Competitor', 'Cont. Offer Currency']\n",
      "📋 Columns in Stats: ['Period', 'Country Code', 'Customer Post Code', 'Customer City', 'Customer Net Worth', 'Spanish Customer No.', 'Spanish Customer Name', 'Wine Country', 'Wine Overall Type', 'Wine Region', 'Wine Sub-Region', 'Wine Type', 'Classification', 'Color Code', 'Producer No.', 'Wine No.', 'Wine Name', 'Producer Name', 'Item Sales Price from Price List', 'Item No.', 'Customer No.', 'Customer Name', 'Document No.', 'Proforma No.', 'Sales Date', 'Posting Date', 'Segment Code', 'Location Code', 'Currency Code', 'Current Salesperson', 'Item Description', 'Vintage', 'Bottle Size', 'Total Bottle Quantity', 'Total Bottle Quantity in Base Size (75cl)', 'Total Bottle Amount (LCY)', 'Uploaded Qty. (Base)', 'Customer Invoice No.', 'Margin', 'Contact First Name', 'Contact Surname', 'Customer Email', 'Customer Initial Web Sale Date', 'Online Customer', 'Spanish Customer Segment Code', 'Denomination Code', 'Item Category Code', 'Item Real Stock', 'Item Stock', 'Campaign No.', 'Document Currency Factor', 'Document Canceled', 'PA', 'PV Total', 'Sales Source', 'Sales Sub-Source', 'Salesperson']\n",
      "🧮 Filtered lines count: 50\n",
      "🔍 Sample Contact Nos: ['100145', '100174', '100277', '100339', '100412']\n",
      "✅ Final shape: (299, 57)\n",
      "📁 Internal .pkl saved to: C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA\\filtered_clients.pkl\n",
      "👀 Sample rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_no</th>\n",
       "      <th>spanish_customer_no</th>\n",
       "      <th>total_bottle_amount_lcy</th>\n",
       "      <th>Period</th>\n",
       "      <th>Country Code</th>\n",
       "      <th>Customer Post Code</th>\n",
       "      <th>Customer City</th>\n",
       "      <th>Customer Net Worth</th>\n",
       "      <th>Spanish Customer Name</th>\n",
       "      <th>Wine Country</th>\n",
       "      <th>...</th>\n",
       "      <th>Item Real Stock</th>\n",
       "      <th>Item Stock</th>\n",
       "      <th>Campaign No.</th>\n",
       "      <th>Document Currency Factor</th>\n",
       "      <th>Document Canceled</th>\n",
       "      <th>PA</th>\n",
       "      <th>PV Total</th>\n",
       "      <th>Sales Source</th>\n",
       "      <th>Sales Sub-Source</th>\n",
       "      <th>Salesperson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1570</th>\n",
       "      <td>10044</td>\n",
       "      <td>nan</td>\n",
       "      <td>411.655874</td>\n",
       "      <td>2025</td>\n",
       "      <td>LI</td>\n",
       "      <td>9490</td>\n",
       "      <td>Vaduz</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IT</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>CM-25-02316</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C.NETTER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>101332</td>\n",
       "      <td>nan</td>\n",
       "      <td>1108.912355</td>\n",
       "      <td>2025</td>\n",
       "      <td>LI</td>\n",
       "      <td>9494</td>\n",
       "      <td>Schaan</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IT</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>CM-25-01069</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N.BOLDRINI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629</th>\n",
       "      <td>101332</td>\n",
       "      <td>nan</td>\n",
       "      <td>1463.591049</td>\n",
       "      <td>2025</td>\n",
       "      <td>LI</td>\n",
       "      <td>9494</td>\n",
       "      <td>Schaan</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>CM-25-00459</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N.BOLDRINI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     customer_no spanish_customer_no  total_bottle_amount_lcy  Period  \\\n",
       "1570       10044                 nan               411.655874    2025   \n",
       "1586      101332                 nan              1108.912355    2025   \n",
       "1629      101332                 nan              1463.591049    2025   \n",
       "\n",
       "     Country Code Customer Post Code Customer City  Customer Net Worth  \\\n",
       "1570           LI               9490         Vaduz                   0   \n",
       "1586           LI               9494        Schaan                   0   \n",
       "1629           LI               9494        Schaan                   0   \n",
       "\n",
       "     Spanish Customer Name Wine Country  ... Item Real Stock Item Stock  \\\n",
       "1570                   NaN           IT  ...               0          0   \n",
       "1586                   NaN           IT  ...               0          0   \n",
       "1629                   NaN           US  ...               0          0   \n",
       "\n",
       "     Campaign No. Document Currency Factor Document Canceled PA PV Total  \\\n",
       "1570  CM-25-02316                        0             False  0        0   \n",
       "1586  CM-25-01069                        0             False  0        0   \n",
       "1629  CM-25-00459                        0             False  0        0   \n",
       "\n",
       "      Sales Source Sales Sub-Source Salesperson  \n",
       "1570           NaN              NaN    C.NETTER  \n",
       "1586           NaN              NaN  N.BOLDRINI  \n",
       "1629           NaN              NaN  N.BOLDRINI  \n",
       "\n",
       "[3 rows x 57 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- CELL 3: Load & Filter Clients for Targeting (Cloud-Safe) ---\n",
    "# Loads Lines.xlsx and Power BI stats, cleans & filters, and writes a compact\n",
    "# pickle for downstream cells. Robust to Excel float IDs (e.g., \"12345.0\").\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------- Inputs / Outputs ----------\n",
    "lines_path = SOURCE_PATH / \"Lines.xlsx\"\n",
    "stats_path = SOURCE_PATH / \"Power BI Dtld. Statistics ALL.xlsx\"\n",
    "output_temp_path = OUTPUT_PATH / \"filtered_clients.pkl\"\n",
    "\n",
    "def _ensure_file(p: Path, label: str):\n",
    "    if not p.exists() or p.stat().st_size == 0:\n",
    "        print(f\"❌ Missing or empty {label}: {p}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def _norm_id_series(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Normalize Excel-ish ID columns:\n",
    "       - cast to string, strip\n",
    "       - drop trailing '.0'\n",
    "       - collapse common null tokens to NaN\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return pd.Series(dtype=\"string\")\n",
    "    out = (\n",
    "        s.astype(str).str.strip()\n",
    "         .str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "         .replace({r\"^(nan|na|null|none|\\s*)$\", np.nan}, regex=True, inplace=False)\n",
    "    )\n",
    "    return out\n",
    "\n",
    "# ---------- Load ----------\n",
    "ok_lines = _ensure_file(lines_path, \"Lines.xlsx\")\n",
    "ok_stats = _ensure_file(stats_path, \"Power BI Stats.xlsx\")\n",
    "\n",
    "if not (ok_lines and ok_stats):\n",
    "    # Write empty output so downstream cells can still run gracefully\n",
    "    empty = pd.DataFrame(columns=[\"customer_no\",\"spanish_customer_no\",\"total_bottle_amount_lcy\"])\n",
    "    empty.to_pickle(output_temp_path)\n",
    "    clients_df = empty.copy()\n",
    "    print(\"⚠️ One or more inputs missing. Wrote empty filtered_clients.pkl and continuing.\")\n",
    "else:\n",
    "    # Read Excel (engine chosen by pandas)\n",
    "    lines_df = pd.read_excel(lines_path)\n",
    "    stats_df = pd.read_excel(stats_path)\n",
    "\n",
    "    # Clean column names\n",
    "    lines_df.columns = lines_df.columns.str.strip()\n",
    "    stats_df.columns = stats_df.columns.str.strip()\n",
    "\n",
    "    print(\"📋 Columns in Lines:\", lines_df.columns.tolist())\n",
    "    print(\"📋 Columns in Stats:\", stats_df.columns.tolist())\n",
    "\n",
    "    # ---------- Validate & Filter (Excluded) ----------\n",
    "    if \"Excluded\" not in lines_df.columns:\n",
    "        raise KeyError(\"❌ 'Excluded' column not found in Lines.xlsx.\")\n",
    "    \n",
    "    excluded_norm = lines_df[\"Excluded\"].apply(lambda v: str(v).strip().lower() if pd.notna(v) else \"\")\n",
    "    # Treat only explicit truthy values as excluded\n",
    "    truthy_exclude = {\"true\", \"1\", \"yes\", \"y\", \"t\"}\n",
    "    keep_mask = ~excluded_norm.isin(truthy_exclude)\n",
    "    filtered_lines = lines_df.loc[keep_mask].copy()\n",
    "    print(f\"🧮 Filtered lines count: {len(filtered_lines)}\")\n",
    "    if filtered_lines.empty:\n",
    "        print(\"⚠️ No lines passed the Excluded=False filter.\")\n",
    "\n",
    "    # ---------- Extract and normalize contact numbers ----------\n",
    "    contact_col = None\n",
    "    for c in [\"Contact No.\", \"Contact No\", \"Customer No.\", \"Customer No\", \"ContactNo\", \"CustomerNumber\"]:\n",
    "        if c in filtered_lines.columns:\n",
    "            contact_col = c\n",
    "            break\n",
    "    if contact_col is None:\n",
    "        raise KeyError(\"❌ 'Contact No.' column not found in Lines.xlsx.\")\n",
    "\n",
    "    contact_nos = _norm_id_series(filtered_lines[contact_col]).dropna().unique().tolist()\n",
    "    print(\"🔍 Sample Contact Nos:\", contact_nos[:5])\n",
    "\n",
    "    # ---------- Validate required stats columns ----------\n",
    "    required_cols = [\"Customer No.\", \"Spanish Customer No.\", \"Total Bottle Amount (LCY)\"]\n",
    "    missing = [c for c in required_cols if c not in stats_df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"❌ Missing columns in Power BI Stats file: {missing}\")\n",
    "\n",
    "    # Normalize IDs and amounts\n",
    "    stats_df[\"Customer No.\"] = _norm_id_series(stats_df[\"Customer No.\"])\n",
    "    stats_df[\"Spanish Customer No.\"] = _norm_id_series(stats_df[\"Spanish Customer No.\"])\n",
    "    stats_df[\"Total Bottle Amount (LCY)\"] = pd.to_numeric(\n",
    "        stats_df[\"Total Bottle Amount (LCY)\"], errors=\"coerce\"\n",
    "    ).fillna(0.0)\n",
    "\n",
    "    # ---------- Business rule for client 122636 (vectorized) ----------\n",
    "    # If Customer No. == '122636' AND Spanish Customer No. is one of contact_nos:\n",
    "    #   - overwrite Customer No. with Spanish Customer No.\n",
    "    #   - multiply Total Bottle Amount (LCY) by 1.12\n",
    "    mask_122636 = (stats_df[\"Customer No.\"] == \"122636\") & (\n",
    "        stats_df[\"Spanish Customer No.\"].isin(contact_nos)\n",
    "    )\n",
    "    if mask_122636.any():\n",
    "        stats_df.loc[mask_122636, \"Customer No.\"] = stats_df.loc[mask_122636, \"Spanish Customer No.\"]\n",
    "        stats_df.loc[mask_122636, \"Total Bottle Amount (LCY)\"] *= 1.12\n",
    "        print(f\"🔧 Applied 1.12x adjustment to {mask_122636.sum()} rows for client 122636.\")\n",
    "\n",
    "    # ---------- Final filter: keep only valid clients ----------\n",
    "    final_df = stats_df.loc[\n",
    "        stats_df[\"Customer No.\"].isin(contact_nos)\n",
    "    ].copy()\n",
    "\n",
    "    # ---------- Normalize output schema ----------\n",
    "    final_df = final_df.rename(\n",
    "        columns={\n",
    "            \"Customer No.\": \"customer_no\",\n",
    "            \"Spanish Customer No.\": \"spanish_customer_no\",\n",
    "            \"Total Bottle Amount (LCY)\": \"total_bottle_amount_lcy\",\n",
    "        }\n",
    "    )\n",
    "    # keep only the essentials + anything you know downstream needs\n",
    "    core_cols = [\"customer_no\", \"spanish_customer_no\", \"total_bottle_amount_lcy\"]\n",
    "    # If there are extra useful columns, keep them but put core first\n",
    "    extras = [c for c in final_df.columns if c not in core_cols]\n",
    "    final_df = final_df[core_cols + extras]\n",
    "\n",
    "    # Persist compact binary for downstream cells\n",
    "    final_df.to_pickle(output_temp_path)\n",
    "\n",
    "    # Expose for the rest of the notebook\n",
    "    clients_df = final_df\n",
    "\n",
    "    # ---------- Logs ----------\n",
    "    print(f\"✅ Final shape: {final_df.shape}\")\n",
    "    print(f\"📁 Internal .pkl saved to: {output_temp_path}\")\n",
    "    if final_df.empty:\n",
    "        print(\"⚠️ The output file contains only headers — no matching customers found.\")\n",
    "    else:\n",
    "        print(\"👀 Sample rows:\")\n",
    "        try:\n",
    "            from IPython.display import display  # safe if IPython present\n",
    "            display(final_df.head(3))\n",
    "        except Exception:\n",
    "            print(final_df.head(3).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6c50a82-2316-45fd-879a-4e4a959f2d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Stock columns (raw): ['ID', 'Stock', 'Minimum Order Q-ty', 'Size', 'Wine', 'Producer', 'Origin', 'Classification', 'Region', 'Country', 'Color', 'Type', 'Vintage', 'Rating', 'RP', 'WA', 'WS', 'JS', 'RG', 'VINOUS', 'JR', 'Decanter', 'NM', 'JA', 'AG', 'Falstaff', 'JD', 'DB', 'WInd', 'WCI', 'YB', 'AMA', 'JMQ', 'MDM', 'VINUM', 'AVG', 'EUR p/bt', 'CHF p/bt VAT excl.', 'CHF p/bt VAT incl.', 'OMT Last Offer Price CHF', 'OMT Last Offer Price Euro', 'OMT last offer date']\n",
      "✅ Final wine dataset enriched and saved to: C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA\\stock_df_final.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>stock</th>\n",
       "      <th>Minimum Order Q-ty</th>\n",
       "      <th>size</th>\n",
       "      <th>wine</th>\n",
       "      <th>producer</th>\n",
       "      <th>origin</th>\n",
       "      <th>classification</th>\n",
       "      <th>region</th>\n",
       "      <th>Country</th>\n",
       "      <th>...</th>\n",
       "      <th>Num_of_CM</th>\n",
       "      <th>region_group</th>\n",
       "      <th>type_class</th>\n",
       "      <th>bottle_size_ml</th>\n",
       "      <th>grape_list</th>\n",
       "      <th>body</th>\n",
       "      <th>sweetness</th>\n",
       "      <th>occasion</th>\n",
       "      <th>full_type</th>\n",
       "      <th>wine_full_match_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40261</td>\n",
       "      <td>108</td>\n",
       "      <td>12</td>\n",
       "      <td>75.0</td>\n",
       "      <td>Bolaire</td>\n",
       "      <td>Château Bolaire</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>France</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>Red</td>\n",
       "      <td>750</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Still Red</td>\n",
       "      <td>Bolaire Château Bolaire 2016 75.0 40261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54770</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>75.0</td>\n",
       "      <td>Ygrec de Ch. d'Yquem</td>\n",
       "      <td>Château d'Yquem</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>France</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>White</td>\n",
       "      <td>750</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>Still White</td>\n",
       "      <td>Ygrec de Ch. d'Yquem Château d'Yquem 2020 75.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62182</td>\n",
       "      <td>96</td>\n",
       "      <td>6</td>\n",
       "      <td>75.0</td>\n",
       "      <td>Ygrec de Ch. d'Yquem</td>\n",
       "      <td>Château d'Yquem</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>France</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>White</td>\n",
       "      <td>750</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>Still White</td>\n",
       "      <td>Ygrec de Ch. d'Yquem Château d'Yquem 2022 75.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  stock  Minimum Order Q-ty  size                  wine  \\\n",
       "0  40261    108                  12  75.0               Bolaire   \n",
       "1  54770      3                   1  75.0  Ygrec de Ch. d'Yquem   \n",
       "2  62182     96                   6  75.0  Ygrec de Ch. d'Yquem   \n",
       "\n",
       "          producer    origin classification    region Country  ... Num_of_CM  \\\n",
       "0  Château Bolaire  Bordeaux            NaN  Bordeaux  France  ...       NaN   \n",
       "1  Château d'Yquem  Bordeaux            NaN  Bordeaux  France  ...       NaN   \n",
       "2  Château d'Yquem  Bordeaux            NaN  Bordeaux  France  ...      22.0   \n",
       "\n",
       "  region_group type_class bottle_size_ml grape_list body sweetness occasion  \\\n",
       "0     Bordeaux        Red            750    Unknown    4         3   Casual   \n",
       "1     Bordeaux      White            750    Unknown    2         3   Dinner   \n",
       "2     Bordeaux      White            750    Unknown    2         3   Dinner   \n",
       "\n",
       "     full_type                             wine_full_match_string  \n",
       "0    Still Red            Bolaire Château Bolaire 2016 75.0 40261  \n",
       "1  Still White  Ygrec de Ch. d'Yquem Château d'Yquem 2020 75.0...  \n",
       "2  Still White  Ygrec de Ch. d'Yquem Château d'Yquem 2022 75.0...  \n",
       "\n",
       "[3 rows x 59 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- CELL 4: Enhance Stock Dataset (Detailed Stock + OMT Info) ---\n",
    "# Robust load, normalize, enrich with price tiers (label + stable key),\n",
    "# trait inference, and OMT campaign summary. Outputs stock_df_final.pkl.\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, re, json\n",
    "\n",
    "\n",
    "# Part 1 - Turn OFF legacy history writer; Cell 5 will own history + schedules\n",
    "ENABLE_LEGACY_HISTORY_WRITER = False\n",
    "\n",
    "# --- Bootstrap IO paths if Cell 1 wasn't run ---\n",
    "try:\n",
    "    OUTPUT_PATH\n",
    "    SOURCE_PATH\n",
    "except NameError:\n",
    "    from pathlib import Path\n",
    "    import os\n",
    "    def _resolve_path(env_key, default_path_str):\n",
    "        return Path(os.getenv(env_key, default_path_str))\n",
    "    SOURCE_PATH = _resolve_path(\"INPUT_PATH\", str(Path.home() / \"OneDrive - AVU SA\" / \"AVU CPI Campaign\" / \"Puzzle_control_Reports\" / \"SOURCE_FILES\"))\n",
    "    OUTPUT_PATH = _resolve_path(\"OUTPUT_PATH\", str(Path.home() / \"OneDrive - AVU SA\" / \"AVU CPI Campaign\" / \"Puzzle_control_Reports\" / \"IRON_DATA\"))\n",
    "    SOURCE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# (…rest of your Cell 4 code unchanged…)\n",
    "\n",
    "if ENABLE_LEGACY_HISTORY_WRITER:\n",
    "    pass  # intentionally disabled (history is built in Cell 5)\n",
    "\n",
    "# --- Safe unidecode import (fallback) ---\n",
    "try:\n",
    "    from unidecode import unidecode  # pip install Unidecode\n",
    "except Exception:\n",
    "    import unicodedata\n",
    "    def unidecode(x):\n",
    "        x = '' if x is None else str(x)\n",
    "        return ''.join(\n",
    "            ch for ch in unicodedata.normalize('NFKD', x)\n",
    "            if not unicodedata.combining(ch)\n",
    "        )\n",
    "\n",
    "stock_path = SOURCE_PATH / \"Detailed Stock List.xlsx\"\n",
    "omt_path   = SOURCE_PATH / \"OMT Main Offer List.xlsx\"\n",
    "\n",
    "def _ensure_file(p: Path, label: str) -> bool:\n",
    "    if not p.exists() or p.stat().st_size == 0:\n",
    "        print(f\"❌ Missing or empty {label}: {p}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "ok_stock = _ensure_file(stock_path, \"Detailed Stock List.xlsx\")\n",
    "ok_omt   = _ensure_file(omt_path, \"OMT Main Offer List.xlsx\")\n",
    "\n",
    "if not ok_stock:\n",
    "    # Minimal empty frame so downstream cells can still run.\n",
    "    stock_df = pd.DataFrame(columns=[\n",
    "        \"id\",\"wine\",\"producer\",\"classification\",\"region\",\"type\",\"color\",\"size\",\"vintage\",\n",
    "        \"origin\",\"stock\",\"CHF Price\",\"price_tier\",\"price_tier_key\",\"full_type\",\"region_group\",\n",
    "        \"bottle_size_ml\",\"grape_list\",\"body\",\"sweetness\",\"occasion\",\"wine_full_match_string\",\n",
    "        \"Num_of_CM\",\"most_recent_date\",\"last_eur_price\",\"last_chf_price\",\"number_of_sent_emails\"\n",
    "    ])\n",
    "    final_stock_path = OUTPUT_PATH / \"stock_df_final.pkl\"\n",
    "    stock_df.to_pickle(final_stock_path)\n",
    "    print(f\"⚠️ No stock file. Wrote empty dataset to: {final_stock_path}\")\n",
    "else:\n",
    "    # ---- Load Detailed Stock (header fallback: row 2 first, then row 0) ----\n",
    "    try:\n",
    "        detailed_df = pd.read_excel(stock_path, header=2)\n",
    "        # If the expected key columns aren't present, retry with header=0\n",
    "        if not any(k in detailed_df.columns for k in [\"ID\",\"Item No.\",\"Wine\",\"Stock\"]):\n",
    "            detailed_df = pd.read_excel(stock_path, header=0)\n",
    "    except Exception:\n",
    "        detailed_df = pd.read_excel(stock_path, header=0)\n",
    "\n",
    "    # ---- Flexible rename map (accept common variants) ----\n",
    "    print(\"📋 Stock columns (raw):\", detailed_df.columns.tolist())\n",
    "    rename_map_variants = {\n",
    "        \"ID\": \"id\",\n",
    "        \"Item No.\": \"id\",\n",
    "        \"Wine\": \"wine\",\n",
    "        \"Producer\": \"producer\",\n",
    "        \"Classification\": \"classification\",\n",
    "        \"Region\": \"region\",\n",
    "        \"Type\": \"type\",\n",
    "        \"Color\": \"color\",\n",
    "        \"Size\": \"size\",\n",
    "        \"Vintage\": \"vintage\",\n",
    "        \"Origin\": \"origin\",\n",
    "        \"Stock\": \"stock\",\n",
    "        \"Qty\": \"stock\",\n",
    "        \"CHF p/bt VAT excl.\": \"CHF Price\",\n",
    "        \"CHF p/bt (VAT excl.)\": \"CHF Price\",\n",
    "        \"Unit Price (CHF)\": \"CHF Price\",\n",
    "        \"CHF Price\": \"CHF Price\",\n",
    "        \"AVG\": \"avg_score\",\n",
    "        \"Average Score\": \"avg_score\",\n",
    "    }\n",
    "    actual_map = {c: rename_map_variants[c] for c in detailed_df.columns if c in rename_map_variants}\n",
    "    detailed_df = detailed_df.rename(columns=actual_map)\n",
    "\n",
    "    # ---- Ensure core columns exist ----\n",
    "    for c in [\"id\",\"wine\",\"producer\",\"classification\",\"region\",\"type\",\"color\",\"size\",\"vintage\",\"origin\",\"stock\",\"CHF Price\",\"avg_score\"]:\n",
    "        if c not in detailed_df.columns:\n",
    "            detailed_df[c] = np.nan\n",
    "\n",
    "    # ---- Normalize critical fields ----\n",
    "    detailed_df[\"id\"] = (\n",
    "        detailed_df[\"id\"].astype(str).str.strip()\n",
    "        .str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "        .replace({\"nan\": \"\", \"None\": \"\"})\n",
    "    )\n",
    "    # CHF Price → numeric\n",
    "    detailed_df[\"CHF Price\"] = (\n",
    "        detailed_df[\"CHF Price\"]\n",
    "        .astype(str).str.replace(r\"[^\\d\\.,-]\", \"\", regex=True)\n",
    "        .str.replace(\",\", \".\", regex=False)\n",
    "    )\n",
    "    detailed_df[\"CHF Price\"] = pd.to_numeric(detailed_df[\"CHF Price\"], errors=\"coerce\")\n",
    "    # Stock → numeric, missing → 0\n",
    "    detailed_df[\"stock\"] = pd.to_numeric(detailed_df[\"stock\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    # ---- OMT Campaign Summary (robust headers & normalization) ----\n",
    "    if not ok_omt:\n",
    "        campaign_summary = pd.DataFrame(columns=[\n",
    "            \"Item No.\",\"Num_of_CM\",\"most_recent_date\",\"last_eur_price\",\"last_chf_price\",\"number_of_sent_emails\"\n",
    "        ])\n",
    "    else:\n",
    "        omt_df = pd.read_excel(omt_path)\n",
    "\n",
    "        # Canonical headers\n",
    "        id_aliases    = [\"Item No.\",\"Item No\",\"Item\",\"ID\",\"Wine ID\",\"Sku\",\"SKU\",\"SKU Code\",\"Item Code\",\"Product ID\",\"Code\"]\n",
    "        sched_aliases = [\"Schedule DateTime\",\"Scheduled\",\"Scheduled At\",\"DateTime\",\"Date\",\"Send Date\",\"Offer Date\",\n",
    "                         \"Campaign Date\",\"Scheduled Date\",\"Scheduled On\",\"Sent At\",\"Created\",\"Execution Date\"]\n",
    "        eur_price_aliases = [\"Unit Price (EUR)\",\"EUR Unit Price\",\"EUR Price\",\"Price (EUR)\"]\n",
    "        chf_price_aliases = [\"Unit Price (CHF)\",\"CHF Unit Price\",\"CHF Price\",\"Unit Price\"]\n",
    "        sent_aliases  = [\"Number of Sent Emails\",\"Sent Emails\",\"Emails Sent\"]\n",
    "\n",
    "        def _pick(cands, df):\n",
    "            for c in cands:\n",
    "                if c in df.columns: return c\n",
    "            return None\n",
    "\n",
    "        id_col    = _pick(id_aliases, omt_df)\n",
    "        if id_col is None:\n",
    "            campaign_summary = pd.DataFrame(columns=[\n",
    "                \"Item No.\",\"Num_of_CM\",\"most_recent_date\",\"last_eur_price\",\"last_chf_price\",\"number_of_sent_emails\"\n",
    "            ])\n",
    "        else:\n",
    "            sched_col = _pick(sched_aliases, omt_df)\n",
    "            eur_col   = _pick(eur_price_aliases, omt_df)\n",
    "            chf_col   = _pick(chf_price_aliases, omt_df)\n",
    "            sent_col  = _pick(sent_aliases, omt_df)\n",
    "\n",
    "            omt_df = omt_df.rename(columns={id_col: \"Item No.\"})\n",
    "            omt_df[\"Item No.\"] = (\n",
    "                omt_df[\"Item No.\"].astype(str).str.strip()\n",
    "                .str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "            )\n",
    "\n",
    "            if sched_col:\n",
    "                omt_df[\"Schedule DateTime\"] = pd.to_datetime(omt_df[sched_col], errors=\"coerce\", dayfirst=True, utc=False)\n",
    "            else:\n",
    "                omt_df[\"Schedule DateTime\"] = pd.NaT\n",
    "\n",
    "            omt_df[\"Unit Price (EUR)\"] = pd.to_numeric(omt_df[eur_col], errors=\"coerce\") if eur_col else np.nan\n",
    "            omt_df[\"Unit Price\"]       = pd.to_numeric(omt_df[chf_col], errors=\"coerce\") if chf_col else np.nan\n",
    "            omt_df[\"Number of Sent Emails\"] = pd.to_numeric(omt_df[sent_col], errors=\"coerce\") if sent_col else np.nan\n",
    "\n",
    "            # ---- CLUSTERING across size variants on the same timestamp ----\n",
    "            desc_col = next((c for c in [\"Item Description\",\"Description\",\"Product Name\",\"Wine\",\"Title\"] if c in omt_df.columns), None)\n",
    "            def _norm_desc(s: str) -> str:\n",
    "                s = str(s or \"\").strip()\n",
    "                # strip trailing “, 75cl” / “, 1.5L” / “, 750” / bare digits\n",
    "                s = re.sub(r\"(,\\s*)?(\\d+(\\.\\d+)?\\s*(ml|cl|l|lt))\\s*$\", \"\", s, flags=re.I)\n",
    "                s = re.sub(r\"(,\\s*)?\\d{2,4}\\s*$\", \"\", s)\n",
    "                return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "            omt_df[\"__desc_base\"] = omt_df[desc_col].map(_norm_desc) if desc_col else \"\"\n",
    "            vint_col = next((c for c in [\"Vintage\",\"Year\"] if c in omt_df.columns), None)\n",
    "            if vint_col:\n",
    "                omt_df[vint_col] = omt_df[vint_col].astype(str).str.strip().replace({\"\": \"NV\", \"nan\": \"NV\"})\n",
    "            omt_df[\"__ts_min\"] = pd.to_datetime(omt_df[\"Schedule DateTime\"], errors=\"coerce\").dt.floor(\"min\")\n",
    "            omt_df[\"__name_key\"] = np.where(\n",
    "                omt_df[\"__desc_base\"].ne(\"\"),\n",
    "                omt_df[\"__desc_base\"] + \"::\" + (omt_df[vint_col] if vint_col else \"NV\"),\n",
    "                \"\"\n",
    "            )\n",
    "\n",
    "            # Cluster by (name_key, timestamp) -> sum sent emails, keep last prices\n",
    "            name_clusters = (\n",
    "                omt_df[omt_df[\"__name_key\"].ne(\"\")]\n",
    "                .groupby([\"__name_key\",\"__ts_min\"], dropna=False)\n",
    "                .agg(number_of_sent_emails=(\"Number of Sent Emails\",\"sum\"),\n",
    "                     last_eur_price=(\"Unit Price (EUR)\",\"last\"),\n",
    "                     last_chf_price=(\"Unit Price\",\"last\"))\n",
    "                .reset_index()\n",
    "            )\n",
    "            # Latest per name_key\n",
    "            name_latest = (\n",
    "                name_clusters.sort_values(\"__ts_min\")\n",
    "                .groupby(\"__name_key\", as_index=False).tail(1)\n",
    "                .rename(columns={\"__ts_min\":\"most_recent_date\"})\n",
    "            )\n",
    "\n",
    "            # ---- Classic per-ID summary (kept; we still merge by id) ----\n",
    "            omt_df = omt_df.sort_values([\"Item No.\",\"Schedule DateTime\"], ascending=[True, True])\n",
    "            counts = omt_df.groupby(\"Item No.\").size().rename(\"Num_of_CM\")\n",
    "            last_rows = omt_df.groupby(\"Item No.\").tail(1)[\n",
    "                [\"Item No.\",\"Schedule DateTime\",\"Unit Price (EUR)\",\"Unit Price\",\"Number of Sent Emails\"]\n",
    "            ]\n",
    "\n",
    "            campaign_summary = (\n",
    "                last_rows.rename(columns={\n",
    "                    \"Schedule DateTime\": \"most_recent_date\",\n",
    "                    \"Unit Price (EUR)\": \"last_eur_price\",\n",
    "                    \"Unit Price\": \"last_chf_price\",\n",
    "                    \"Number of Sent Emails\": \"number_of_sent_emails\",\n",
    "                })\n",
    "                .merge(counts, left_on=\"Item No.\", right_index=True, how=\"left\")\n",
    "            )\n",
    "\n",
    "            # Expose for later index build (Cell 5 may use this if present)\n",
    "            _OMT_NAME_LATEST = name_latest.copy()\n",
    "\n",
    "    # ---- Price tier logic (label + stable key) ----\n",
    "    def price_tier_label(price):\n",
    "        try:\n",
    "            p = float(price)\n",
    "            if p < 50:   return \"Budget\"\n",
    "            if p < 100:  return \"Mid-range\"\n",
    "            if p < 200:  return \"Premium\"\n",
    "            if p < 500:  return \"Luxury\"\n",
    "            return \"Ultra Luxury\"\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def price_tier_key_from_label(label):\n",
    "        m = (label or \"\").strip().lower()\n",
    "        return {\n",
    "            \"budget\": \"budget\",\n",
    "            \"mid-range\": \"mid\",\n",
    "            \"premium\": \"premium\",\n",
    "            \"luxury\": \"luxury\",\n",
    "            \"ultra luxury\": \"ultra\",\n",
    "        }.get(m, \"\"\n",
    "\n",
    ")\n",
    "\n",
    "# Part 2 -     \n",
    "    \n",
    "    detailed_df[\"price_tier\"] = detailed_df[\"CHF Price\"].apply(price_tier_label)\n",
    "    detailed_df[\"price_tier_key\"] = detailed_df[\"price_tier\"].apply(price_tier_key_from_label)\n",
    "\n",
    "    # ---- Merge OMT summary → stock ----\n",
    "    enhanced_df = detailed_df.merge(\n",
    "        campaign_summary, how=\"left\", left_on=\"id\", right_on=\"Item No.\"\n",
    "    )\n",
    "\n",
    "    # Region group fallback (handle empty strings)\n",
    "    enhanced_df[\"region_group\"] = (\n",
    "        enhanced_df[\"origin\"].replace(\"\", np.nan)\n",
    "        .fillna(enhanced_df[\"region\"])\n",
    "    )\n",
    "    enhanced_df[\"most_recent_date\"] = pd.to_datetime(enhanced_df[\"most_recent_date\"], errors=\"coerce\")\n",
    "\n",
    "    # ---- Helpers / inference ----\n",
    "    def infer_type_class(row):\n",
    "        t = unidecode(str(row.get(\"type\", \"\")).lower())\n",
    "        c = unidecode(str(row.get(\"color\", \"\")).lower())\n",
    "        txt = f\"{t} {c}\"\n",
    "        if \"sparkling\" in txt or \"champagne\" in txt or \"cava\" in txt or \"prosecco\" in txt:\n",
    "            return \"Sparkling\"\n",
    "        if \"dessert\" in txt or \"sweet\" in txt or \"sauternes\" in txt or \"porto\" in txt:\n",
    "            return \"Dessert\"\n",
    "        if \"rose\" in txt or \"rosé\" in txt:\n",
    "            return \"Rose\"\n",
    "        if \"white\" in txt or \"blanc\" in txt or \"bianco\" in txt:\n",
    "            return \"White\"\n",
    "        if \"red\" in txt or \"rouge\" in txt or \"rosso\" in txt:\n",
    "            return \"Red\"\n",
    "        return \"Red\" if \"red\" in t else (\"White\" if \"white\" in t else \"Red\")\n",
    "\n",
    "    def parse_bottle_size_ml(val):\n",
    "        s = unidecode(str(val)).lower().strip()\n",
    "        if not s or s == \"nan\": return np.nan\n",
    "        if \"jeroboam\" in s: return 3000\n",
    "        if \"magnum\" in s:   return 1500\n",
    "        s = s.replace(\"lt\", \"l\")\n",
    "        if \"l\" in s and \"ml\" not in s and \"cl\" not in s:\n",
    "            num = re.sub(r\"[^\\d\\.]\", \"\", s)\n",
    "            try: return int(float(num) * 1000)\n",
    "            except Exception: return np.nan\n",
    "        s = s.replace(\"cl\",\"\").replace(\"ml\",\"\").strip()\n",
    "        s = re.sub(r\"[^\\d\\.]\", \"\", s)\n",
    "        try:\n",
    "            num = float(s)\n",
    "            return int(num * 10) if num < 100 else int(num)\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "\n",
    "    def infer_grapes(row):\n",
    "        wine_name = unidecode(str(row.get('wine', '')).lower().strip())\n",
    "        producer  = unidecode(str(row.get('producer', '')).lower().strip())\n",
    "        origin    = unidecode(str(row.get('origin', '')).lower().strip())\n",
    "        combined  = f\"{wine_name} {producer} {origin}\"\n",
    "        grape_keywords = {\n",
    "            'nebbiolo':'Nebbiolo','tempranillo':'Tempranillo','cabernet sauvignon':'Cabernet Sauvignon',\n",
    "            ' cabernet ':'Cabernet Sauvignon',' cab ':'Cabernet Sauvignon','merlot':'Merlot',\n",
    "            'pinot noir':'Pinot Noir',' pinot ':'Pinot Noir','sangiovese':'Sangiovese','syrah':'Syrah',\n",
    "            'shiraz':'Syrah','grenache':'Grenache','chardonnay':'Chardonnay',' chard ':'Chardonnay',\n",
    "            'riesling':'Riesling','sauvignon blanc':'Sauvignon Blanc',' sauvignon ':'Sauvignon Blanc',\n",
    "            ' sauv ':'Sauvignon Blanc','zinfandel':'Zinfandel','primitivo':'Primitivo','malbec':'Malbec',\n",
    "            'grigio':'Pinot Grigio','garganega':'Garganega',\"nero d avola\":\"Nero d'Avola\",'barbera':'Barbera',\n",
    "            'carmenere':'Carmenère','trebbiano':'Trebbiano','vermentino':'Vermentino','teroldego':'Teroldego'\n",
    "        }\n",
    "        hits = {g for k,g in grape_keywords.items() if k in f\" {combined} \"}\n",
    "        if not hits:\n",
    "            t = unidecode(str(row.get('type',''))).lower()\n",
    "            if 'red' in t: hits.add('Red Blend')\n",
    "            elif 'white' in t: hits.add('White Blend')\n",
    "            elif 'sparkling' in t: hits.add('Sparkling Blend')\n",
    "            elif 'rose' in t or 'rosé' in t: hits.add('Rosé Blend')\n",
    "            else: hits.add('Unknown')\n",
    "        return '/'.join(sorted(hits))\n",
    "\n",
    "    def infer_body(row):\n",
    "        c = str(row.get(\"color\",\"\")).lower()\n",
    "        t = str(row.get(\"type\",\"\")).lower()\n",
    "        if \"sparkling\" in t: return 2\n",
    "        if \"red\" in c: return 4\n",
    "        if \"white\" in c or \"ros\" in c: return 2\n",
    "        return 3\n",
    "\n",
    "    def infer_sweetness(row):\n",
    "        t = str(row.get(\"type\",\"\")).lower()\n",
    "        if \"brut\" in t or \"dry\" in t: return 1\n",
    "        if \"sweet\" in t: return 5\n",
    "        if \"medium\" in t: return 3\n",
    "        return 3\n",
    "\n",
    "    def infer_occasion(row):\n",
    "        tier = row.get(\"price_tier\")\n",
    "        t = str(row.get(\"type\",\"\")).lower()\n",
    "        if tier in [\"Luxury\",\"Ultra Luxury\"]: return \"Gifting\"\n",
    "        if \"sparkling\" in t or \"ros\" in t:     return \"Celebration\"\n",
    "        if tier in [\"Mid-range\",\"Premium\"]:    return \"Dinner\"\n",
    "        return \"Casual\"\n",
    "\n",
    "    # Ensure text cols exist to avoid \"nan\" literals\n",
    "    for tcol in [\"wine\",\"producer\",\"region\",\"type\",\"color\",\"size\",\"vintage\"]:\n",
    "        enhanced_df[tcol] = enhanced_df[tcol].fillna(\"\").astype(str)\n",
    "\n",
    "    enhanced_df[\"type_class\"]     = enhanced_df.apply(infer_type_class, axis=1)\n",
    "    enhanced_df[\"bottle_size_ml\"] = enhanced_df[\"size\"].apply(parse_bottle_size_ml)\n",
    "    enhanced_df[\"grape_list\"]     = enhanced_df.apply(infer_grapes, axis=1)\n",
    "    enhanced_df[\"body\"]           = enhanced_df.apply(infer_body, axis=1)\n",
    "    enhanced_df[\"sweetness\"]      = enhanced_df.apply(infer_sweetness, axis=1)\n",
    "    enhanced_df[\"occasion\"]       = enhanced_df.apply(infer_occasion, axis=1)\n",
    "\n",
    "    # --- full_type: vectorized ---\n",
    "    t = enhanced_df[\"type\"].fillna(\"\").astype(str).str.strip()\n",
    "    c = enhanced_df[\"color\"].fillna(\"\").astype(str).str.strip()\n",
    "    enhanced_df[\"full_type\"] = (t.str.title() + (\" \" + c.str.title()).where(c.ne(\"\"), \"\")).str.strip()\n",
    "\n",
    "    # Stable dedupe key for scheduling logic if needed\n",
    "    enhanced_df[\"wine_full_match_string\"] = (\n",
    "        enhanced_df[\"wine\"].str.strip() + \" \" +\n",
    "        enhanced_df[\"producer\"].str.strip() + \" \" +\n",
    "        enhanced_df[\"vintage\"].astype(str).str.strip() + \" \" +\n",
    "        enhanced_df[\"size\"].astype(str).str.strip() + \" \" +\n",
    "        enhanced_df[\"id\"].astype(str).str.strip()\n",
    "    ).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "    # Final save\n",
    "    final_stock_path = OUTPUT_PATH / \"stock_df_final.pkl\"\n",
    "    enhanced_df.to_pickle(final_stock_path)\n",
    "\n",
    "    stock_df = enhanced_df  # expose to later cells\n",
    "\n",
    "    print(f\"✅ Final wine dataset enriched and saved to: {final_stock_path}\")\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(enhanced_df.head(3))\n",
    "    except Exception:\n",
    "        print(enhanced_df.head(3).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb4d0581-14af-4ae1-98c3-a16d8454185f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HIST] OMT rows after unify (wine-only): 25,176\n",
      "✅ history (flat) + campaign_index written.\n",
      "📅 weekly schedules & 🔒 locked calendars written (post-stock).\n",
      "✅ History keys look good (id or wine::vintage).\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 5: History (post-stock) — robust OMT parsing + de-dupe + UI artifacts ---\n",
    "# Outputs:\n",
    "#   - OUTPUT_PATH/history/wine_campaign_history.json         (FLAT map: id or wine::vintage -> {wine, vintage, last_campaign_date, emails_total})\n",
    "#   - OUTPUT_PATH/campaign_index.json                        (by_id/by_name + emails + meta_by_id for UI hydration)\n",
    "#   - OUTPUT_PATH/weekly_campaign_schedule_{YEAR}_week_{W}.json\n",
    "#   - OUTPUT_PATH/locked_weeks/locked_calendar_{YEAR}_week_{W}.json\n",
    "#   - OUTPUT_PATH/weekly_leads_{YEAR}_week_{W}.json\n",
    "#   - OUTPUT_PATH/schedule_index.json\n",
    "\n",
    "import os, io, re, time, shutil, json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------------- Bootstraps (if Cell 1 didn't run) ----------------\n",
    "try:\n",
    "    OUTPUT_PATH\n",
    "    SOURCE_PATH\n",
    "    TMP_PATH\n",
    "except NameError:\n",
    "    def _resolve_path(env_key, default_path_str): return Path(os.getenv(env_key, default_path_str))\n",
    "    SOURCE_PATH = _resolve_path(\"INPUT_PATH\",  str(Path.home() / \"OneDrive - AVU SA\" / \"AVU CPI Campaign\" / \"Puzzle_control_Reports\" / \"SOURCE_FILES\"))\n",
    "    OUTPUT_PATH = _resolve_path(\"OUTPUT_PATH\", str(Path.home() / \"OneDrive - AVU SA\" / \"AVU CPI Campaign\" / \"Puzzle_control_Reports\" / \"IRON_DATA\"))\n",
    "    TMP_PATH    = OUTPUT_PATH / \"_tmp\"\n",
    "    for p in (SOURCE_PATH, OUTPUT_PATH, TMP_PATH): p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------------- Settings ----------------\n",
    "NUM_SLOTS = 5\n",
    "DAYS_FULL = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    "DAYMAP = {1:\"Monday\",2:\"Tuesday\",3:\"Wednesday\",4:\"Thursday\",5:\"Friday\",6:\"Saturday\",7:\"Sunday\"}\n",
    "\n",
    "LOCK_HISTORY_CARDS = True         # keep history cards locked in weekly files\n",
    "INCLUDE_SCHEDULED_AT = True       # include ISO timestamp of the campaign on each card\n",
    "\n",
    "HISTORY_DIR = OUTPUT_PATH / \"history\"; HISTORY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOCKED_PATH  = OUTPUT_PATH / \"locked_weeks\"; LOCKED_PATH.mkdir(parents=True, exist_ok=True)\n",
    "HISTORY_JSON = HISTORY_DIR / \"wine_campaign_history.json\"\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def _to_py(o):\n",
    "    \"\"\"JSON-safe conversion for numpy/pandas scalars.\"\"\"\n",
    "    if o is None: return None\n",
    "    if isinstance(o, (np.integer,)):  return int(o)\n",
    "    if isinstance(o, (np.floating,)):\n",
    "        try:\n",
    "            return None if np.isnan(o) else float(o)\n",
    "        except Exception:\n",
    "            return float(o)\n",
    "    if isinstance(o, (np.bool_,)):    return bool(o)\n",
    "    if isinstance(o, (pd.Timestamp,)):return o.isoformat()\n",
    "    return o\n",
    "\n",
    "CRITIC_COLS_CANON = [\"avg_score\"]  # if Cell 4 already computed/kept it\n",
    "CRITIC_COLS_RAW = [\n",
    "    \"RP\",\"WA\",\"WS\",\"JS\",\"RG\",\"VINOUS\",\"JR\",\"Decanter\",\"NM\",\"JA\",\"AG\",\"Falstaff\",\"JD\",\"DB\",\n",
    "    \"WInd\",\"WCI\",\"YB\",\"AMA\",\"JMQ\",\"MDM\",\"VINUM\",\"Rating\"\n",
    "]\n",
    "\n",
    "def _compute_avg_score(row: pd.Series) -> float | None:\n",
    "    \"\"\"Robust critic average: use any available critic columns, ignore 0/NaN/out-of-range.\"\"\"\n",
    "    vals = []\n",
    "    for col in CRITIC_COLS_CANON + [c for c in CRITIC_COLS_RAW if c in row.index]:\n",
    "        v = pd.to_numeric(row.get(col), errors=\"coerce\")\n",
    "        if pd.notna(v) and 50 <= float(v) <= 100:\n",
    "            vals.append(float(v))\n",
    "    if not vals: \n",
    "        return None\n",
    "    return round(float(np.mean(vals)), 1)\n",
    "\n",
    "def _fallback_grapes(meta: dict) -> str:\n",
    "    \"\"\"Return a graceful fallback when grape_list is Unknown/blank.\"\"\"\n",
    "    g = (meta.get(\"grape_list\") or \"\").strip().lower()\n",
    "    if g and g != \"unknown\":\n",
    "        return meta.get(\"grape_list\")\n",
    "    tc = (meta.get(\"type_class\") or \"\").strip().lower()\n",
    "    return {\n",
    "        \"red\": \"Red Blend\",\n",
    "        \"white\": \"White Blend\",\n",
    "        \"rose\": \"Rosé Blend\",\n",
    "        \"rosé\": \"Rosé Blend\",\n",
    "        \"sparkling\": \"Sparkling Blend\",\n",
    "        \"dessert\": \"Dessert Blend\",\n",
    "    }.get(tc, \"Blend\")\n",
    "\n",
    "# ---------------- Load stock meta from Cell 4 output ----------------\n",
    "stock_df_path = OUTPUT_PATH / \"stock_df_final.pkl\"\n",
    "stock_meta = {}\n",
    "if stock_df_path.exists() and stock_df_path.stat().st_size > 0:\n",
    "    _sdf = pd.read_pickle(stock_df_path)\n",
    "    _sdf[\"id\"] = _sdf[\"id\"].astype(str).str.strip().str.replace(r\"\\.0$\",\"\",regex=True)\n",
    "\n",
    "    # we'll compute a robust avg_score per row (ignoring 0s/outliers)\n",
    "    have_cols = [c for c in (CRITIC_COLS_CANON + CRITIC_COLS_RAW) if c in _sdf.columns]\n",
    "    for _, r in _sdf.dropna(subset=[\"id\"]).drop_duplicates(\"id\").iterrows():\n",
    "        wid = (r[\"id\"] or \"\").strip()\n",
    "        if not wid: \n",
    "            continue\n",
    "\n",
    "        # prefer a valid existing avg_score; else compute\n",
    "        avg = pd.to_numeric(r.get(\"avg_score\"), errors=\"coerce\")\n",
    "        avg = float(avg) if pd.notna(avg) and 50 <= float(avg) <= 100 else None\n",
    "        if avg is None and have_cols:\n",
    "            avg = _compute_avg_score(r)\n",
    "\n",
    "        stock_meta[wid] = {\n",
    "            \"wine\":            str(r.get(\"wine\",\"\")).strip(),\n",
    "            \"vintage\":         str(r.get(\"vintage\",\"NV\")).strip() or \"NV\",\n",
    "            \"full_type\":       str(r.get(\"full_type\",\"\")).strip(),\n",
    "            \"region_group\":    str(r.get(\"region_group\",\"\")).strip(),\n",
    "            \"price_tier\":      str(r.get(\"price_tier\",\"\")).strip(),\n",
    "            \"price_tier_key\":  str(r.get(\"price_tier_key\",\"\")).strip(),\n",
    "            \"stock\":           int(pd.to_numeric(r.get(\"stock\"), errors=\"coerce\") or 0),\n",
    "\n",
    "            # extras for UI\n",
    "            \"producer\":        str(r.get(\"producer\",\"\")).strip(),\n",
    "            \"chf_price\":       (float(r[\"CHF Price\"]) if pd.notna(r.get(\"CHF Price\")) else None),\n",
    "            \"grape_list\":      str(r.get(\"grape_list\",\"\")).strip(),\n",
    "            \"bottle_size_ml\":  (int(r[\"bottle_size_ml\"]) if pd.notna(r.get(\"bottle_size_ml\")) else None),\n",
    "            \"type_class\":      str(r.get(\"type_class\",\"\")).strip(),\n",
    "            \"body\":            (_to_py(r.get(\"body\")) if \"body\" in r else None),\n",
    "            \"sweetness\":       (_to_py(r.get(\"sweetness\")) if \"sweetness\" in r else None),\n",
    "            \"occasion\":        str(r.get(\"occasion\",\"\")).strip(),\n",
    "            \"avg_score\":       avg,   # ← fixed: recomputed and only present if valid\n",
    "        }\n",
    "\n",
    "# ---------------- Discover & safe-read OMT sources ----------------\n",
    "def _find_omt_sources(base: Path) -> list[Path]:\n",
    "    pats = [\"*OMT*offer*.xlsx\",\"*OMT*offer*.csv\",\"*OMT*main*offer*.xlsx\",\"*OMT*main*offer*.csv\",\n",
    "            \"*campaign*list*.xlsx\",\"*campaign*list*.csv\",\"*offer*list*.xlsx\",\"*offer*list*.csv\"]\n",
    "    out, seen = [], set()\n",
    "    for pat in pats:\n",
    "        for p in base.rglob(pat):\n",
    "            if p.exists() and str(p) not in seen:\n",
    "                seen.add(str(p)); out.append(p)\n",
    "    return sorted(out)\n",
    "\n",
    "def _safe_read_xlsx(p: Path) -> pd.DataFrame:\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            with open(p,\"rb\") as f: buf=f.read()\n",
    "            return pd.read_excel(io.BytesIO(buf), engine=\"openpyxl\")\n",
    "        except PermissionError: time.sleep(0.6)\n",
    "        except Exception: break\n",
    "    tmp = TMP_PATH / f\"shadow_{int(time.time()*1000)}_{p.name}\"\n",
    "    try: shutil.copy2(p, tmp); return pd.read_excel(tmp, engine=\"openpyxl\")\n",
    "    except Exception: return pd.DataFrame()\n",
    "\n",
    "def _safe_read_csv(p: Path) -> pd.DataFrame:\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            with open(p,\"rb\") as f: buf=f.read()\n",
    "            return pd.read_csv(io.BytesIO(buf))\n",
    "        except PermissionError: time.sleep(0.6)\n",
    "        except Exception: break\n",
    "    tmp = TMP_PATH / f\"shadow_{int(time.time()*1000)}_{p.name}\"\n",
    "    try: shutil.copy2(p, tmp); return pd.read_csv(tmp)\n",
    "    except Exception: return pd.DataFrame()\n",
    "\n",
    "def _load_omt(p: Path) -> pd.DataFrame:\n",
    "    try:\n",
    "        df = _safe_read_xlsx(p) if p.suffix.lower() in {\".xlsx\",\".xls\"} else _safe_read_csv(p)\n",
    "        df.columns = [str(c).strip() for c in df.columns]\n",
    "        return df\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ---------------- Helpers for normalization & clustering ----------------\n",
    "_SIZE_NUMS = {187,200,250,300,375,500,620,700,720,730,750,1000,1500,2000,3000,4500,6000,9000,12000}\n",
    "_RE_SIZE_TOKEN = re.compile(r\"(?i)\\b(\\d{3,5})\\s*(ml|cl|l|lt|litre|liter)?\\b\")\n",
    "_RE_SIZE_PACK  = re.compile(r\"(?i)\\b\\d+\\s*x\\s*\\d{3,5}\\s*(ml|cl|l)\\b\")\n",
    "\n",
    "def _strip_last_size_token(s: str) -> str:\n",
    "    if not s: return \"\"\n",
    "    parts = [p.strip() for p in str(s).split(\",\")]\n",
    "    while len(parts) > 1:\n",
    "        last = parts[-1]\n",
    "        m = _RE_SIZE_TOKEN.search(last)\n",
    "        only_num = last.replace(\" \",\"\").isdigit()\n",
    "        ok_num = only_num and int(last.replace(\" \",\"\")) in _SIZE_NUMS\n",
    "        if m or ok_num or _RE_SIZE_PACK.search(last): parts.pop()\n",
    "        else: break\n",
    "    return \", \".join(parts).strip()\n",
    "\n",
    "def _norm(s): return re.sub(r\"\\s+\",\" \", str(s or \"\").strip())\n",
    "\n",
    "def _extract_vintage(v, name=None, desc=None):\n",
    "    v = _norm(v)\n",
    "    if re.fullmatch(r\"(19|20)\\d{2}\", v): return v\n",
    "    for t in (name, desc):\n",
    "        m = re.search(r\"\\b(19|20)\\d{2}\\b\", str(t or \"\"))\n",
    "        if m: return m.group(0)\n",
    "    return \"NV\"\n",
    "\n",
    "def _key_by_name(wine, vintage): \n",
    "    return f\"{_norm(wine).lower()}::{_norm(vintage or 'NV').lower()}\"\n",
    "\n",
    "def _looks_like_wine_name(s: str) -> bool:\n",
    "    s = _norm(s).lower()\n",
    "    if not s: return False\n",
    "    bad = [\"accessor\", \"glass\", \"opener\", \"gift\", \"voucher\", \"packaging\", \"ship\", \"bag\", \"box\"]\n",
    "    return not any(b in s for b in bad)\n",
    "\n",
    "def _unify_one(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty: return pd.DataFrame()\n",
    "    cols = {c: re.sub(r\"[^a-z0-9]+\",\"_\", c.strip().lower()) for c in df.columns}\n",
    "    df = df.rename(columns=cols)\n",
    "\n",
    "    def pick(*names): \n",
    "        for n in names:\n",
    "            if n in df.columns: return n\n",
    "        return None\n",
    "\n",
    "    c_id   = pick(\"wine_id\",\"id\",\"item_no\",\"item_no_\",\"sku\",\"product_id\",\"code\",\"item_code\")\n",
    "    c_name = pick(\"wine\",\"wine_name\",\"name\",\"product_name\",\"title\",\"label\",\"long_name\",\"item_description\",\"description\")\n",
    "    c_desc = pick(\"item_description\",\"description\")\n",
    "    c_vint = pick(\"vintage\",\"year\")\n",
    "    c_dt   = pick(\"schedule_datetime\",\"scheduled_datetime\",\"schedule_date_time\",\"schedule_date\",\"date\",\"sent_at\",\"created\",\"execution_date\",\"offer_date\")\n",
    "    c_sent = pick(\"number_of_sent_emails\",\"emails_sent\",\"sent_emails\",\"emails\",\"#_emails\",\"mails_sent\",\"sent\")\n",
    "\n",
    "    rows = []\n",
    "    for _, r in df.iterrows():\n",
    "        dt = pd.to_datetime(r.get(c_dt), errors=\"coerce\", dayfirst=True, utc=False) if c_dt else pd.NaT\n",
    "        if pd.isna(dt): \n",
    "            continue\n",
    "        desc = str(r.get(c_desc) or \"\")\n",
    "        raw_name = str(r.get(c_name) or \"\")\n",
    "        name = _norm(raw_name) or _strip_last_size_token(desc)\n",
    "        if not _looks_like_wine_name(name):\n",
    "            continue\n",
    "        vint = _extract_vintage(r.get(c_vint), raw_name, desc)\n",
    "        wid  = _norm(r.get(c_id) or \"\").replace(\".0\",\"\")\n",
    "        if wid and not wid.isdigit():\n",
    "            wid = \"\"  # name-key fallback for non-numeric ids\n",
    "        emails = r.get(c_sent)\n",
    "        try: emails = int(emails) if pd.notna(emails) else 0\n",
    "        except: emails = 0\n",
    "\n",
    "        rows.append({\n",
    "            \"schedule_dt\": dt,\n",
    "            \"schedule_min\": dt.floor(\"min\"),\n",
    "            \"wine_id\": wid,\n",
    "            \"wine_name\": name,\n",
    "            \"vintage\": vint,\n",
    "            \"item_core\": _strip_last_size_token(desc or raw_name),\n",
    "            \"emails_sent\": emails,\n",
    "        })\n",
    "    out = pd.DataFrame(rows)\n",
    "    if out.empty: return out\n",
    "    iso = out[\"schedule_dt\"].dt.isocalendar()\n",
    "    out[\"iso_year\"] = iso.year.astype(int)\n",
    "    out[\"iso_week\"] = iso.week.astype(int)\n",
    "    out[\"iso_wday\"] = iso.day.astype(int)\n",
    "    return out\n",
    "\n",
    "# ---------------- Load & unify all OMT sources ----------------\n",
    "files = _find_omt_sources(SOURCE_PATH)\n",
    "frames = []\n",
    "for p in files:\n",
    "    dfp = _load_omt(p)\n",
    "    if dfp is not None and not dfp.empty:\n",
    "        frames.append(_unify_one(dfp))\n",
    "raw = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "print(f\"[HIST] OMT rows after unify (wine-only): {len(raw):,}\")\n",
    "\n",
    "if raw.empty:\n",
    "    payload = {\"by_id\": {}, \"by_name\": {}, \"emails_sent_by_id\": {}, \"emails_sent_by_name\": {}, \"meta_by_id\": stock_meta}\n",
    "    (OUTPUT_PATH / \"campaign_index.json\").write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n",
    "    HISTORY_JSON.write_text(json.dumps({}, indent=2), encoding=\"utf-8\")\n",
    "    print(\"ℹ️ No OMT rows found. Wrote empty campaign_index & history.\")\n",
    "else:\n",
    "    # ---------------- Cluster ----------------\n",
    "    tmp = raw.copy()\n",
    "    tmp[\"name_key\"] = tmp.apply(lambda r: _key_by_name(r[\"wine_name\"], r[\"vintage\"]), axis=1)\n",
    "    tmp[\"wine_key\"] = tmp.apply(lambda r: r[\"wine_id\"] if r[\"wine_id\"] else r[\"name_key\"], axis=1)\n",
    "\n",
    "    gcols = [\"schedule_min\",\"wine_key\",\"item_core\",\"vintage\",\"iso_year\",\"iso_week\",\"iso_wday\"]\n",
    "    cl = (tmp.groupby(gcols, dropna=False, as_index=False)\n",
    "              .agg({\"schedule_dt\":\"max\",\"emails_sent\":\"sum\",\"wine_id\":\"first\",\"wine_name\":\"first\"}))\n",
    "    cl = (cl.sort_values([\"schedule_min\",\"wine_key\",\"emails_sent\"], ascending=[True,True,False])\n",
    "            .drop_duplicates([\"schedule_min\",\"wine_key\"], keep=\"first\"))\n",
    "\n",
    "    # ---------------- Last-campaign & emails totals ----------------\n",
    "    last_by_id, last_by_name = {}, {}\n",
    "    emails_by_id, emails_by_name = defaultdict(int), defaultdict(int)\n",
    "    for _, r in cl.iterrows():\n",
    "        wid = (r[\"wine_id\"] or \"\").strip()\n",
    "        nkey = _key_by_name(r[\"wine_name\"], r[\"vintage\"])\n",
    "        dt_iso = pd.to_datetime(r[\"schedule_dt\"]).date().isoformat()\n",
    "        if wid:\n",
    "            last_by_id[wid] = max(last_by_id.get(wid,\"\"), dt_iso) if last_by_id.get(wid) else dt_iso\n",
    "            emails_by_id[wid] += int(r.get(\"emails_sent\") or 0)\n",
    "        last_by_name[nkey] = max(last_by_name.get(nkey,\"\"), dt_iso) if last_by_name.get(nkey) else dt_iso\n",
    "        emails_by_name[nkey] += int(r.get(\"emails_sent\") or 0)\n",
    "\n",
    "    # ---------------- Flat history map ----------------\n",
    "    history_flat = {}\n",
    "    for _, r in cl.iterrows():\n",
    "        wid = (r[\"wine_id\"] or \"\").strip()\n",
    "        nkey = _key_by_name(r[\"wine_name\"], r[\"vintage\"])\n",
    "        key  = wid if wid else nkey\n",
    "        if wid and not wid.isdigit():\n",
    "            key = nkey\n",
    "        history_flat[key] = {\n",
    "            \"wine\": r[\"wine_name\"],\n",
    "            \"vintage\": r[\"vintage\"],\n",
    "            \"last_campaign_date\": (last_by_id.get(wid) if wid else last_by_name.get(nkey)) or \"\",\n",
    "            \"emails_total\": int(emails_by_id.get(wid, 0) if wid else emails_by_name.get(nkey, 0)),\n",
    "        }\n",
    "    HISTORY_JSON.write_text(json.dumps(history_flat, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    # ---------------- campaign_index (for UI hydration) ----------------\n",
    "    campaign_index = {\n",
    "        \"by_id\": last_by_id,\n",
    "        \"by_name\": last_by_name,\n",
    "        \"emails_sent_by_id\": dict(emails_by_id),\n",
    "        \"emails_sent_by_name\": dict(emails_by_name),\n",
    "        \"meta_by_id\": stock_meta\n",
    "    }\n",
    "    (OUTPUT_PATH / \"campaign_index.json\").write_text(json.dumps(campaign_index, indent=2), encoding=\"utf-8\")\n",
    "    print(\"✅ history (flat) + campaign_index written.\")\n",
    "\n",
    "    # ---------------- Weekly schedules ----------------\n",
    "    def _atomic_write(path: Path, text: str):\n",
    "        tmp = path.with_suffix(path.suffix + \".tmp\")\n",
    "        tmp.write_text(text, encoding=\"utf-8\")\n",
    "        os.replace(tmp, path)\n",
    "\n",
    "    idx_path = OUTPUT_PATH / \"schedule_index.json\"\n",
    "    try: schedule_idx = json.loads(idx_path.read_text(encoding=\"utf-8\")) if idx_path.exists() else {}\n",
    "    except Exception: schedule_idx = {}\n",
    "\n",
    "    latest_y = latest_w = None\n",
    "\n",
    "    wk = (cl.sort_values([\"iso_year\",\"iso_week\",\"wine_key\",\"schedule_dt\"])\n",
    "            .groupby([\"iso_year\",\"iso_week\",\"wine_key\"], as_index=False)\n",
    "            .agg({\"schedule_dt\":\"max\",\"emails_sent\":\"sum\",\"wine_id\":\"first\",\"wine_name\":\"first\",\"vintage\":\"first\",\"iso_wday\":\"first\"}))\n",
    "\n",
    "    for (yr, w), sub in wk.groupby([\"iso_year\",\"iso_week\"]):\n",
    "        cal = {d: [] for d in DAYS_FULL}\n",
    "        seen = set()\n",
    "        for r in sub.sort_values([\"schedule_dt\",\"emails_sent\"], ascending=[True,False]).to_dict(\"records\"):\n",
    "            dname = DAYMAP.get(int(r.get(\"iso_wday\",1)), \"Monday\")\n",
    "            key   = r[\"wine_key\"]\n",
    "            if key in seen: \n",
    "                continue\n",
    "            wid   = (r.get(\"wine_id\") or \"\").strip()\n",
    "            meta  = stock_meta.get(wid, {})\n",
    "\n",
    "            label = (r.get(\"wine_name\") or \"\").strip() or (r.get(\"item_core\") or \"\").strip() or meta.get(\"wine\") or \"Unknown\"\n",
    "            vint  = (r.get(\"vintage\") or \"\").strip() or meta.get(\"vintage\") or \"NV\"\n",
    "            sched_date = pd.to_datetime(r[\"schedule_dt\"])\n",
    "\n",
    "            # --- build extras with grape fallback & fixed avg_score ---\n",
    "            extras = {\n",
    "                \"producer\":       _to_py(meta.get(\"producer\")),\n",
    "                \"chf_price\":      _to_py(meta.get(\"chf_price\")),\n",
    "                \"grape_list\":     _fallback_grapes(meta),  # ← never 'Unknown'\n",
    "                \"bottle_size_ml\": _to_py(meta.get(\"bottle_size_ml\")),\n",
    "                \"type_class\":     _to_py(meta.get(\"type_class\")),\n",
    "                \"body\":           _to_py(meta.get(\"body\")),\n",
    "                \"sweetness\":      _to_py(meta.get(\"sweetness\")),\n",
    "                \"occasion\":       _to_py(meta.get(\"occasion\")),\n",
    "                \"avg_score\":      _to_py(meta.get(\"avg_score\")),  # ← recomputed if possible\n",
    "            }\n",
    "\n",
    "            item = {\n",
    "                \"id\": wid,\n",
    "                \"wine\": label,\n",
    "                \"vintage\": vint,\n",
    "                \"full_type\": meta.get(\"full_type\",\"\") or \"Unknown\",\n",
    "                \"region_group\": meta.get(\"region_group\",\"\") or \"Unknown\",\n",
    "                \"price_tier\": meta.get(\"price_tier\",\"\") or \"\",\n",
    "                \"stock\": int(meta.get(\"stock\", 0)),\n",
    "                \"match_quality\": \"History (Locked)\",\n",
    "                \"avg_cpi_score\": 0.0,\n",
    "                \"locked\": bool(LOCK_HISTORY_CARDS),\n",
    "                \"emails_sent\": int(r.get(\"emails_sent\") or 0),\n",
    "                \"last_campaign_date\": sched_date.date().isoformat(),\n",
    "\n",
    "                # convenience / back-compat\n",
    "                \"name\": label,\n",
    "                \"price_tier_id\": meta.get(\"price_tier_key\",\"\"),\n",
    "                \"extras\": extras\n",
    "            }\n",
    "            if INCLUDE_SCHEDULED_AT:\n",
    "                item[\"scheduled_at\"] = sched_date.isoformat()\n",
    "\n",
    "            if len(cal[dname]) < NUM_SLOTS:\n",
    "                cal[dname].append(item)\n",
    "                seen.add(key)\n",
    "\n",
    "        # year-week schedule\n",
    "        out_year = OUTPUT_PATH / f\"weekly_campaign_schedule_{yr}_week_{w}.json\"\n",
    "        _atomic_write(out_year, json.dumps(cal, indent=2))\n",
    "\n",
    "        # locked snapshot\n",
    "        out_locked = LOCKED_PATH / f\"locked_calendar_{yr}_week_{w}.json\"\n",
    "        locked_week = {day: [(cal[day][i] if i < len(cal[day]) else None) for i in range(NUM_SLOTS)] for day in DAYS_FULL}\n",
    "        _atomic_write(out_locked, json.dumps(locked_week, indent=2))\n",
    "\n",
    "        # empty leads buckets for UI\n",
    "        leads_json = OUTPUT_PATH / f\"weekly_leads_{yr}_week_{w}.json\"\n",
    "        _atomic_write(leads_json, json.dumps({\"TueWed\": [], \"ThuFri\": []}, indent=2))\n",
    "\n",
    "        # index bookkeeping\n",
    "        schedule_idx.setdefault(str(yr), {})[str(w)] = {\n",
    "            \"json\": out_year.name,\n",
    "            \"updated_at\": datetime.now().isoformat(timespec=\"seconds\")\n",
    "        }\n",
    "        if (latest_y is None) or (yr > latest_y) or (yr == latest_y and w > (latest_w or 0)):\n",
    "            latest_y, latest_w = yr, w\n",
    "\n",
    "    if latest_y is not None:\n",
    "        schedule_idx[\"_latest_year\"] = str(latest_y)\n",
    "        schedule_idx[\"_latest_week\"] = str(latest_w)\n",
    "\n",
    "    _atomic_write(idx_path, json.dumps(schedule_idx, indent=2))\n",
    "    print(\"📅 weekly schedules & 🔒 locked calendars written (post-stock).\")\n",
    "\n",
    "# ---------------- Quick sanity (optional) ----------------\n",
    "try:\n",
    "    d = json.loads(HISTORY_JSON.read_text(encoding=\"utf-8\"))\n",
    "    bad = [k for k in d.keys() if not (re.fullmatch(r\"\\d+\", k) or \"::\" in k)]\n",
    "    if bad:\n",
    "        print(\"❌ Bad keys in history:\", bad[:10])\n",
    "    else:\n",
    "        print(\"✅ History keys look good (id or wine::vintage).\")\n",
    "except Exception as e:\n",
    "    print(\"⚠️ History sanity skipped:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06c13cb8-fd29-448e-b526-2380df9c71af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🍾 Bottle size [750ml] → wines: 4578 → 2405\n",
      "⏮️ Recency filter → removed 16 (source: weekly_campaign_schedule_2024_week_38.json)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔄 Generating CPI vectors: 100%|█████████████████████████████████████████████████████| 345/345 [00:04<00:00, 75.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ CPI computation completed in 4.76 seconds.\n",
      "✅ Preferences, CPI matrix, and UI stock snapshot saved (.pkl).\n",
      "🧪 CPI Matrix shape: (3719, 347) | UI stock cols: ['id', 'wine', 'producer', 'vintage', 'price_tier', 'stock', 'full_type', 'type', 'color', 'region', 'region_group', 'bottle_size_ml', 'avg_score', 'high_score', 'avg_cpi_score']\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 6: CPI Calculation + Filter Wiring (Clean & Robust) ---\n",
    "# Honors UI filters (loyalty, wine type, bottle size, price tier, last stock, seasonality),\n",
    "# computes CPI against stock, and saves compact outputs the UI can reuse.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def get_season_from_week(week_no: int):\n",
    "    if 1 <= week_no <= 8 or 49 <= week_no <= 53: return \"Winter\"\n",
    "    if 9  <= week_no <= 22: return \"Spring\"\n",
    "    if 23 <= week_no <= 35: return \"Summer\"\n",
    "    if 36 <= week_no <= 48: return \"Autumn\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "def extract_ids_from_weekly_calendar(obj) -> set:\n",
    "    ids = set()\n",
    "    if isinstance(obj, dict):\n",
    "        for arr in obj.values():\n",
    "            if isinstance(arr, list):\n",
    "                for it in arr:\n",
    "                    if isinstance(it, dict):\n",
    "                        v = it.get(\"id\") or it.get(\"wine_id\")\n",
    "                        if v is not None and str(v).strip():\n",
    "                            ids.add(str(v))\n",
    "    elif isinstance(obj, list):\n",
    "        for it in obj:\n",
    "            if isinstance(it, dict):\n",
    "                v = it.get(\"id\") or it.get(\"wine_id\")\n",
    "                if v is not None and str(v).strip():\n",
    "                    ids.add(str(v))\n",
    "    return ids\n",
    "\n",
    "def coerce_numeric(s, default=np.nan):\n",
    "    try:\n",
    "        return pd.to_numeric(s, errors=\"coerce\")\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "# keep the calendar_year chosen earlier; do NOT reset to now().year later\n",
    "try:\n",
    "    week_number = int(week_number)\n",
    "except Exception:\n",
    "    week_number = int(os.getenv(\"WEEK_NUMBER\", datetime.now().isocalendar().week))\n",
    "try:\n",
    "    year = int(calendar_year)\n",
    "except Exception:\n",
    "    year = datetime.now().year\n",
    "\n",
    "selected_season = get_season_from_week(week_number)\n",
    "\n",
    "# text-mode tqdm (avoid widget deps)\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **k): return x\n",
    "\n",
    "# ---------- Ensure inputs are present ----------\n",
    "# If notebook was run standalone, try loading from previous cell outputs\n",
    "if 'clients_df' not in globals():\n",
    "    p = OUTPUT_PATH / \"filtered_clients.pkl\"\n",
    "    clients_df = pd.read_pickle(p) if p.exists() else pd.DataFrame()\n",
    "if 'stock_df' not in globals():\n",
    "    p = OUTPUT_PATH / \"stock_df_final.pkl\"\n",
    "    stock_df = pd.read_pickle(p) if p.exists() else pd.DataFrame()\n",
    "\n",
    "# Normalize schemas\n",
    "clients_df = clients_df.copy()\n",
    "clients_df.columns = (\n",
    "    clients_df.columns\n",
    "    .str.strip().str.lower().str.replace(\" \", \"_\").str.replace(\".\", \"\", regex=False)\n",
    ")\n",
    "if 'customer_no' in clients_df.columns:\n",
    "    clients_df['customer_no'] = clients_df['customer_no'].astype(str).str.strip()\n",
    "if 'item_no' in clients_df.columns:\n",
    "    clients_df['item_no'] = clients_df['item_no'].astype(str).str.strip()\n",
    "\n",
    "stock_df = stock_df.copy()\n",
    "if 'id' in stock_df.columns:\n",
    "    stock_df['id'] = stock_df['id'].astype(str).str.strip()\n",
    "\n",
    "# critic score column consistency\n",
    "if 'avg_score' not in stock_df.columns and 'AVG' in stock_df.columns:\n",
    "    stock_df.rename(columns={'AVG': 'avg_score'}, inplace=True)\n",
    "\n",
    "# ---------- Loyalty derivation (counts → tier) ----------\n",
    "if {'customer_no','item_no'}.issubset(clients_df.columns):\n",
    "    purchase_counts = clients_df.groupby('customer_no')['item_no'].count().reset_index(name='purchase_count')\n",
    "    purchase_counts['loyalty_level'] = purchase_counts['purchase_count'].apply(\n",
    "        lambda x: 'vip' if x >= 20 else 'gold' if x >= 10 else 'silver' if x >= 5 else 'bronze'\n",
    "    )\n",
    "    clients_df = clients_df.drop(columns=['loyalty_level'], errors='ignore') \\\n",
    "                           .merge(purchase_counts[['customer_no','loyalty_level']], on='customer_no', how='left')\n",
    "else:\n",
    "    clients_df['loyalty_level'] = 'bronze'\n",
    "clients_df['loyalty_level'] = clients_df['loyalty_level'].fillna('bronze')\n",
    "\n",
    "# ---------- Bottle size ML on clients (fallback only) ----------\n",
    "if 'bottle_size' in clients_df.columns:\n",
    "    def _to_ml(x):\n",
    "        try:\n",
    "            v = float(x)\n",
    "            return round(v*10) if v < 100 else v\n",
    "        except:\n",
    "            return np.nan\n",
    "    clients_df['bottle_size_ml'] = clients_df['bottle_size'].apply(_to_ml)\n",
    "else:\n",
    "    clients_df['bottle_size_ml'] = np.nan\n",
    "\n",
    "# Optional sales date\n",
    "sales_dates = clients_df[['customer_no','sales_date']].drop_duplicates() \\\n",
    "    if 'sales_date' in clients_df.columns else pd.DataFrame(columns=['customer_no','sales_date'])\n",
    "\n",
    "# Vintage from stock by item_no (if available)\n",
    "if 'item_no' in clients_df.columns and 'vintage' in stock_df.columns and 'id' in stock_df.columns:\n",
    "    stock_vintage_map = stock_df.set_index('id')['vintage'].astype(str).to_dict()\n",
    "    clients_df['vintage'] = clients_df['item_no'].map(stock_vintage_map)\n",
    "else:\n",
    "    clients_df['vintage'] = 'Unknown'\n",
    "\n",
    "def classify_vintage_group(vint):\n",
    "    try:\n",
    "        s = str(vint).strip().upper()\n",
    "        if s == \"NV\": return \"Non-Vintage Lover\"\n",
    "        y = int(s); cy = datetime.now().year\n",
    "        if y == cy: return \"Likes Current Vintage\"\n",
    "        if y == cy-1: return \"Likes En Primeur\"\n",
    "        if cy - y <= 3: return \"Likes Young Wines\"\n",
    "        if cy - y <= 8: return \"Likes Mature Wines\"\n",
    "        if cy - y > 15: return \"Likes Old Wines\"\n",
    "        return \"Unknown\"\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "\n",
    "clients_df['inferred_vintage'] = clients_df['vintage'].apply(classify_vintage_group)\n",
    "\n",
    "def classify_size_from_ml(size_ml):\n",
    "    try:\n",
    "        v = int(float(size_ml))\n",
    "        m = {375:\"Half\",750:\"Standard\",1500:\"Magnum\",3000:\"Jeroboam\",4500:\"Rehoboam\",\n",
    "             6000:\"Methuselah\",9000:\"Salmanazar\",12000:\"Balthazar\",15000:\"Nebuchadnezzar\",\n",
    "             18000:\"Melchior\",27000:\"Primat\"}\n",
    "        return m.get(v, \"Unknown\")\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# ---------- Build purchases_df for preference inference ----------\n",
    "needed = ['id','grape_list','type','region','sweetness','body','price_tier','avg_score','vintage','bottle_size_ml']\n",
    "stock_merge_df = stock_df.copy()\n",
    "for c in needed:\n",
    "    if c not in stock_merge_df.columns:\n",
    "        stock_merge_df[c] = np.nan\n",
    "\n",
    "if 'item_no' not in clients_df.columns:\n",
    "    purchases_df = clients_df.copy()\n",
    "    for c in ['grape_list','type','region','sweetness','body','price_tier','avg_score','vintage','bottle_size_ml']:\n",
    "        purchases_df[c] = np.nan\n",
    "else:\n",
    "    purchases_df = clients_df.merge(\n",
    "        stock_merge_df[needed], left_on='item_no', right_on='id', how='left'\n",
    "    )\n",
    "\n",
    "purchases_df['vintage'] = purchases_df.get('vintage', pd.Series([\"Unknown\"]*len(purchases_df))).fillna(\"Unknown\")\n",
    "if 'bottle_size_ml' not in purchases_df.columns:\n",
    "    purchases_df['bottle_size_ml'] = 750\n",
    "purchases_df['size_group'] = purchases_df['bottle_size_ml'].apply(classify_size_from_ml)\n",
    "\n",
    "# Occasion inference (coarse)\n",
    "def infer_occasion(row):\n",
    "    try:\n",
    "        price = str(row.get('price_tier','')).lower()\n",
    "        wine_type = str(row.get('type','')).lower()\n",
    "        size = float(row.get('bottle_size_ml',750))\n",
    "        score = float(row.get('avg_score',0))\n",
    "        sweetness = float(row.get('sweetness',0))\n",
    "        if size >= 3000: return 'Big Event'\n",
    "        if size >= 1500: return 'Celebration'\n",
    "        if 'sparkling' in wine_type or sweetness >= 4: return 'Celebration'\n",
    "        if score >= 92 or (price in ['luxury','ultra luxury','premium'] and size == 750): return 'Gift'\n",
    "        if price in ['budget','entry'] and size <= 750 and sweetness <= 2: return 'Everyday'\n",
    "        if price in ['mid-range','premium'] and 'still' in wine_type: return 'Dinner'\n",
    "        if score >= 90 and price in ['mid-range','premium']: return 'Dinner'\n",
    "        if sweetness <= 2 and score >= 88 and size <= 750: return 'Everyday'\n",
    "        if price in ['mid-range','premium','luxury'] and 'still' in wine_type: return 'Personal Consumption'\n",
    "        return 'Unknown'\n",
    "    except:\n",
    "        return 'Unknown'\n",
    "purchases_df['occasion'] = purchases_df.apply(infer_occasion, axis=1)\n",
    "\n",
    "# ---------- Preference aggregation ----------\n",
    "def split_and_flatten(series, delimiter='/'):\n",
    "    return [x.strip() for s in series.dropna() for x in str(s).split(delimiter) if x.strip()]\n",
    "\n",
    "inferred_pref = purchases_df.groupby('customer_no').agg({\n",
    "    'grape_list':  lambda x: ','.join(sorted(set(split_and_flatten(x)))),\n",
    "    'type':        lambda x: ','.join(sorted(set(x.dropna().astype(str).str.lower()))),\n",
    "    'region':      lambda x: ','.join(sorted(set(x.dropna().astype(str).str.lower()))),\n",
    "    'sweetness':   lambda x: round(coerce_numeric(x).dropna().astype(float).mean(), 2) if len(x.dropna()) else np.nan,\n",
    "    'body':        lambda x: round(coerce_numeric(x).dropna().astype(float).mean(), 2) if len(x.dropna()) else np.nan,\n",
    "    'price_tier':  lambda x: x.dropna().mode().iloc[0] if not x.dropna().mode().empty else '',\n",
    "    'avg_score':   lambda x: round(coerce_numeric(x).dropna().astype(float).mean(), 2) if len(x.dropna()) else np.nan,\n",
    "    'vintage':     lambda x: ','.join(sorted(set(x.dropna().astype(str)))),\n",
    "    'size_group':  lambda x: ','.join(sorted(set(x.dropna().astype(str)))),\n",
    "    'occasion':    lambda x: ','.join(sorted(set(x.dropna().astype(str))))\n",
    "}).reset_index().rename(columns={\n",
    "    'grape_list':'inferred_grape_preferences',\n",
    "    'type':'inferred_type',\n",
    "    'region':'inferred_region',\n",
    "    'sweetness':'inferred_sweetness',\n",
    "    'body':'inferred_body',\n",
    "    'price_tier':'inferred_budget',\n",
    "    'avg_score':'avg_critic_score',\n",
    "    'vintage':'inferred_vintage',\n",
    "    'size_group':'inferred_size',\n",
    "    'occasion':'inferred_occasion'\n",
    "})\n",
    "\n",
    "# Fallback vintage/size preferences when inference is missing\n",
    "clients_df['fallback_size_label'] = clients_df['bottle_size_ml'].apply(classify_size_from_ml)\n",
    "fallback_vintage = (\n",
    "    clients_df.groupby('customer_no')['vintage']\n",
    "    .agg(lambda x: x.dropna().mode().iloc[0] if not x.dropna().mode().empty else 'Unknown')\n",
    "    .reset_index(name='fallback_vintage')\n",
    ")\n",
    "fallback_vintage['fallback_vintage_label'] = fallback_vintage['fallback_vintage'].apply(classify_vintage_group)\n",
    "\n",
    "inferred_pref = inferred_pref.merge(\n",
    "    fallback_vintage[['customer_no','fallback_vintage_label']], on='customer_no', how='left'\n",
    ")\n",
    "inferred_pref['inferred_vintage'] = inferred_pref['fallback_vintage_label'].fillna(inferred_pref['inferred_vintage'])\n",
    "inferred_pref.drop(columns=['fallback_vintage_label'], inplace=True)\n",
    "\n",
    "inferred_pref = inferred_pref.merge(\n",
    "    clients_df[['customer_no','fallback_size_label']].drop_duplicates(),\n",
    "    on='customer_no', how='left'\n",
    ")\n",
    "inferred_pref['inferred_size'] = inferred_pref['fallback_size_label'].fillna(inferred_pref['inferred_size'])\n",
    "inferred_pref.drop(columns=['fallback_size_label'], inplace=True)\n",
    "\n",
    "# Merge traits + loyalty + sales date\n",
    "client_pref_df = (inferred_pref\n",
    "    .merge(clients_df[['customer_no']].drop_duplicates(), on='customer_no', how='right')\n",
    "    .merge(clients_df[['customer_no','loyalty_level']].drop_duplicates(), on='customer_no', how='left')\n",
    "    .merge(sales_dates, on='customer_no', how='left')\n",
    ")\n",
    "client_pref_df['loyalty_level'] = client_pref_df['loyalty_level'].fillna('bronze')\n",
    "\n",
    "# Flags\n",
    "client_pref_df['prefers_high_scores'] = (coerce_numeric(client_pref_df['avg_critic_score']).fillna(0) >= 95)\n",
    "stock_df['avg_score'] = coerce_numeric(stock_df.get('avg_score', np.nan))\n",
    "stock_df['high_score'] = stock_df['avg_score'].ge(95)\n",
    "\n",
    "# ---------- UI Filters (from Cell 1 or env) ----------\n",
    "if 'filters' not in globals():\n",
    "    try:\n",
    "        filters = json.loads(os.getenv(\"FILTER_INPUTS\", \"{}\"))\n",
    "    except Exception:\n",
    "        filters = {}\n",
    "\n",
    "# Loyalty filter (client subset)\n",
    "loy = str(filters.get('loyalty','all') or 'all').lower()\n",
    "if loy != 'all':\n",
    "    before = len(client_pref_df)\n",
    "    client_pref_df = client_pref_df.loc[client_pref_df['loyalty_level'].str.lower() == loy].copy()\n",
    "    after = len(client_pref_df)\n",
    "    print(f\"🎯 Loyalty filter [{loy}] → clients: {before} → {after}\")\n",
    "\n",
    "# Wine type filter (stock subset)\n",
    "wt = (filters.get('wine_type') or '').strip().lower()\n",
    "if wt:\n",
    "    if wt in {'red','white','rose','rosé','sparkling','dessert'}:\n",
    "        if wt in {'rose','rosé'}:\n",
    "            mask = stock_df.get('color','').astype(str).str.contains('ros', case=False, na=False)\n",
    "        elif wt == 'sparkling':\n",
    "            mask = stock_df.get('type','').astype(str).str.contains('sparkling', case=False, na=False) \\\n",
    "                   | stock_df.get('color','').astype(str).str.contains('spark', case=False, na=False)\n",
    "        elif wt == 'dessert':\n",
    "            mask = stock_df.get('type','').astype(str).str.contains('dessert', case=False, na=False)\n",
    "        else:\n",
    "            mask = stock_df.get('color','').astype(str).str.contains(wt, case=False, na=False)\n",
    "        before = len(stock_df)\n",
    "        stock_df = stock_df.loc[mask].copy()\n",
    "        after = len(stock_df)\n",
    "        print(f\"🍷 Wine type [{wt}] → wines: {before} → {after}\")\n",
    "\n",
    "# Bottle size filter\n",
    "bs = filters.get('bottle_size', None)\n",
    "try:\n",
    "    bs_ml = int(float(bs)) if bs is not None else None\n",
    "except Exception:\n",
    "    bs_ml = None\n",
    "if bs_ml:\n",
    "    before = len(stock_df)\n",
    "    stock_df = stock_df.loc[(coerce_numeric(stock_df.get('bottle_size_ml')).round(0) == bs_ml)].copy()\n",
    "    after = len(stock_df)\n",
    "    print(f\"🍾 Bottle size [{bs_ml}ml] → wines: {before} → {after}\")\n",
    "\n",
    "# Price tier bucket filter\n",
    "pt = (filters.get('price_tier_bucket') or '').strip()\n",
    "if pt:\n",
    "    before = len(stock_df)\n",
    "    stock_df = stock_df.loc[stock_df.get('price_tier','').astype(str).str.lower() == pt.lower()].copy()\n",
    "    after = len(stock_df)\n",
    "    print(f\"💰 Price tier [{pt}] → wines: {before} → {after}\")\n",
    "\n",
    "# Low stock filter\n",
    "if filters.get('last_stock', False):\n",
    "    th = int(filters.get('last_stock_threshold', 10) or 10)\n",
    "    before = len(stock_df)\n",
    "    stock_df = stock_df.loc[coerce_numeric(stock_df.get('stock')).fillna(0) < th].copy()\n",
    "    after = len(stock_df)\n",
    "    print(f\"⚠️ Last stock < {th} → wines: {before} → {after}\")\n",
    "\n",
    "# Seasonality filter (only if column exists)\n",
    "if filters.get(\"seasonality_boost\", False) and \"seasonality_boost\" in stock_df.columns:\n",
    "    stock_df[\"seasonal_match\"] = stock_df[\"seasonality_boost\"].apply(\n",
    "        lambda x: selected_season in x if isinstance(x, list) else False\n",
    "    )\n",
    "    before = len(stock_df)\n",
    "    stock_df = stock_df.loc[stock_df[\"seasonal_match\"] == True].copy()\n",
    "    after = len(stock_df)\n",
    "    print(f\"🌿 Seasonality [{selected_season}] → wines: {before} → {after}\")\n",
    "elif filters.get(\"seasonality_boost\", False):\n",
    "    print(\"⚠️ 'seasonality_boost' column not found — skipping seasonality filter\")\n",
    "\n",
    "# Recency filter (avoid using items that were already used last year in same week, or this week if already emitted)\n",
    "def _load_calendar_ids(p: Path) -> set:\n",
    "    try:\n",
    "        if p.suffix.lower() == \".pkl\":\n",
    "            obj = pd.read_pickle(p)\n",
    "            cal = obj.get(\"weekly_calendar\", obj) if isinstance(obj, dict) else obj\n",
    "        else:\n",
    "            cal = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "            cal = cal.get(\"weekly_calendar\", cal)\n",
    "        return extract_ids_from_weekly_calendar(cal)\n",
    "    except Exception:\n",
    "        return set()\n",
    "\n",
    "candidates = [\n",
    "    OUTPUT_PATH / f\"weekly_campaign_schedule_{year-1}_week_{week_number}.pkl\",\n",
    "    OUTPUT_PATH / f\"weekly_campaign_schedule_{year-1}_week_{week_number}.json\",\n",
    "    OUTPUT_PATH / f\"weekly_campaign_schedule_{year}_week_{week_number}.pkl\",\n",
    "    OUTPUT_PATH / f\"weekly_campaign_schedule_{year}_week_{week_number}.json\",\n",
    "    OUTPUT_PATH / f\"weekly_campaign_schedule_week_{week_number}.pkl\",   # legacy\n",
    "    OUTPUT_PATH / f\"weekly_campaign_schedule_week_{week_number}.json\",   # legacy\n",
    "]\n",
    "past_ids = set()\n",
    "used_path = None\n",
    "for p in candidates:\n",
    "    if p.exists():\n",
    "        ids = _load_calendar_ids(p)\n",
    "        if ids:\n",
    "            past_ids = ids\n",
    "            used_path = p\n",
    "            break\n",
    "\n",
    "if past_ids:\n",
    "    before = len(stock_df)\n",
    "    stock_df = stock_df.loc[~stock_df['id'].astype(str).isin(past_ids)].copy()\n",
    "    after = len(stock_df)\n",
    "    print(f\"⏮️ Recency filter → removed {before - after} (source: {used_path.name})\")\n",
    "else:\n",
    "    print(f\"📁 No past schedule for Week {week_number} → skipping recency filter\")\n",
    "\n",
    "# Refresh flags after filters\n",
    "stock_df['avg_score'] = coerce_numeric(stock_df.get('avg_score'))\n",
    "stock_df['high_score'] = stock_df['avg_score'].ge(95)\n",
    "\n",
    "# ---------- CPI compute ----------\n",
    "display_col = 'wine' if 'wine' in stock_df.columns else 'id'\n",
    "\n",
    "def compute_cpi_matrix(client_df, stock_df, style=\"default\", display_col='wine'):\n",
    "    stock_df = stock_df.copy()\n",
    "    stock_df['id'] = stock_df['id'].astype(str)\n",
    "\n",
    "    # Baseline weights\n",
    "    weights = {\n",
    "        'grape': 1.0, 'type': 1.0, 'region': 1.0,\n",
    "        'sweetness': 0.5, 'body': 0.5,\n",
    "        'budget': 0.75, 'prefers_high_scores': 0.75,\n",
    "        'avg_score': 0.5\n",
    "    }\n",
    "    loyalty_bonus = {'bronze': 0.0, 'silver': 0.25, 'gold': 0.5, 'vip': 0.75}\n",
    "\n",
    "    # Persona tweaks\n",
    "    st = (style or 'default').lower()\n",
    "    if st == \"cat\":\n",
    "        weights['type'] = 1.2\n",
    "        weights['region'] = 1.2\n",
    "        weights['avg_score'] = 0.4\n",
    "    elif st == \"nigo\":\n",
    "        weights['budget'] = 1.0\n",
    "        weights['avg_score'] = 0.7\n",
    "\n",
    "    total_possible = sum(weights.values()) + max(loyalty_bonus.values())\n",
    "\n",
    "    # Ensure numeric for tolerance checks\n",
    "    sd_sweet = coerce_numeric(stock_df.get('sweetness')).fillna(-999)\n",
    "    sd_body  = coerce_numeric(stock_df.get('body')).fillna(-999)\n",
    "\n",
    "    result = []\n",
    "    it = client_df.iterrows() if len(client_df) else iter([(-1, {'customer_no':'GLOBAL','inferred_grape_preferences':'','inferred_type':'','inferred_region':'','inferred_sweetness':np.nan,'inferred_body':np.nan,'inferred_budget':'','avg_critic_score':np.nan,'loyalty_level':'bronze','prefers_high_scores':False})])\n",
    "\n",
    "    for _, client in tqdm(it, total=max(len(client_df),1), desc=\"🔄 Generating CPI vectors\"):\n",
    "        score = pd.Series(0.0, index=stock_df.index)\n",
    "\n",
    "        # grapes\n",
    "        grape_prefs = set(str(client.get('inferred_grape_preferences','')).lower().split(','))\n",
    "        wine_grapes = stock_df.get('grape_list','').fillna('').astype(str).str.lower().str.split('/')\n",
    "        score += wine_grapes.apply(lambda gs: any(g.strip() in grape_prefs for g in gs if g)).astype(float) * weights['grape']\n",
    "\n",
    "        # type/region\n",
    "        score += (stock_df.get('type','').fillna('').astype(str).str.lower()\n",
    "                  .str.contains(str(client.get('inferred_type','')).lower(), na=False)) * weights['type']\n",
    "        score += (stock_df.get('region','').fillna('').astype(str).str.lower()\n",
    "                  .str.contains(str(client.get('inferred_region','')).lower(), na=False)) * weights['region']\n",
    "\n",
    "        # sweetness/body (tolerance)\n",
    "        cs = client.get('inferred_sweetness', np.nan)\n",
    "        cb = client.get('inferred_body', np.nan)\n",
    "        if pd.notna(cs):\n",
    "            score += (np.isclose(sd_sweet, float(cs), atol=0.5)).astype(float) * weights['sweetness']\n",
    "        if pd.notna(cb):\n",
    "            score += (np.isclose(sd_body,  float(cb), atol=0.5)).astype(float) * weights['body']\n",
    "\n",
    "        # budget match\n",
    "        score += (stock_df.get('price_tier','').fillna('').astype(str).str.lower()\n",
    "                  == str(client.get('inferred_budget','')).lower()) * weights['budget']\n",
    "\n",
    "        # high score preference\n",
    "        pref_hi = bool(client.get('prefers_high_scores', False))\n",
    "        score += (stock_df['high_score'] & pref_hi).astype(float) * weights['prefers_high_scores']\n",
    "\n",
    "        # general avg score quality\n",
    "        score += (coerce_numeric(stock_df['avg_score']).fillna(0) >= 90).astype(float) * weights['avg_score']\n",
    "\n",
    "        # loyalty\n",
    "        score += loyalty_bonus.get(str(client.get('loyalty_level','bronze')).lower(), 0)\n",
    "\n",
    "        score /= total_possible\n",
    "        result.append(score.round(4).rename(f\"pref_cpi_for_{client.get('customer_no','GLOBAL')}\"))\n",
    "\n",
    "    matrix = pd.concat([stock_df[['id', display_col]].reset_index(drop=True)] + result, axis=1)\n",
    "    return matrix\n",
    "\n",
    "style = (filters.get('style') or 'default').lower()\n",
    "t0 = perf_counter()\n",
    "cpi_matrix = compute_cpi_matrix(client_pref_df, stock_df, style=style, display_col=display_col)\n",
    "print(\"⏱️ CPI computation completed in\", round(perf_counter() - t0, 2), \"seconds.\")\n",
    "\n",
    "# ---------- Attach average CPI per wine BEFORE saving UI snapshot ----------\n",
    "cpi_cols = [c for c in cpi_matrix.columns if c.startswith(\"pref_cpi_for_\")]\n",
    "if cpi_cols:\n",
    "    cpi_avg_df = pd.DataFrame({\n",
    "        \"id\": cpi_matrix[\"id\"].astype(str),\n",
    "        \"avg_cpi_score\": cpi_matrix[cpi_cols].mean(axis=1).round(4)\n",
    "    })\n",
    "    stock_df = stock_df.merge(cpi_avg_df, on=\"id\", how=\"left\")\n",
    "else:\n",
    "    stock_df[\"avg_cpi_score\"] = np.nan\n",
    "\n",
    "# ---------- Save outputs ----------\n",
    "client_pref_df.to_pickle(OUTPUT_PATH / \"client_pref_df_latest.pkl\")\n",
    "cpi_matrix.to_pickle(OUTPUT_PATH / \"cpi_matrix_latest.pkl\")\n",
    "\n",
    "# a compact stock file the webapp can use to render cards\n",
    "ui_cols = [\n",
    "    'id','wine','producer','vintage','price_tier','stock','full_type','type','color',\n",
    "    'region','region_group','bottle_size_ml','avg_score','high_score','avg_cpi_score'\n",
    "]\n",
    "present = [c for c in ui_cols if c in stock_df.columns]\n",
    "stock_df[present].to_pickle(OUTPUT_PATH / \"stock_for_ui_latest.pkl\")\n",
    "\n",
    "print(\"✅ Preferences, CPI matrix, and UI stock snapshot saved (.pkl).\")\n",
    "print(\"🧪 CPI Matrix shape:\", cpi_matrix.shape, \"| UI stock cols:\", present)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e403bf5-384f-4b9d-a4ad-6def25b55281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧮 Fallback pool after filters: 2389 rows\n",
      "💾 Saved fallback_pool → C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA\\fallback_pool.pkl (top 5 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>wine</th>\n",
       "      <th>vintage</th>\n",
       "      <th>stock</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>price_tier</th>\n",
       "      <th>full_type</th>\n",
       "      <th>region_group</th>\n",
       "      <th>bottle_size_ml</th>\n",
       "      <th>avg_cpi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53995</td>\n",
       "      <td>Trotanoy</td>\n",
       "      <td>2020</td>\n",
       "      <td>6</td>\n",
       "      <td>98.300000</td>\n",
       "      <td>Luxury</td>\n",
       "      <td>Still Red</td>\n",
       "      <td>Pomerol</td>\n",
       "      <td>750</td>\n",
       "      <td>0.561615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59622</td>\n",
       "      <td>Lodovico</td>\n",
       "      <td>2020</td>\n",
       "      <td>102</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>Luxury</td>\n",
       "      <td>Still Red</td>\n",
       "      <td>Bolgheri</td>\n",
       "      <td>750</td>\n",
       "      <td>0.561615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51070</td>\n",
       "      <td>St. Eden</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>Ultra Luxury</td>\n",
       "      <td>Still Red</td>\n",
       "      <td>Napa Valley</td>\n",
       "      <td>750</td>\n",
       "      <td>0.561615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53882</td>\n",
       "      <td>Lafite Rothschild</td>\n",
       "      <td>2020</td>\n",
       "      <td>1716</td>\n",
       "      <td>97.923077</td>\n",
       "      <td>Ultra Luxury</td>\n",
       "      <td>Still Red</td>\n",
       "      <td>Pauillac</td>\n",
       "      <td>750</td>\n",
       "      <td>0.561615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62256</td>\n",
       "      <td>Cabernet Sauvignon Tychson Hill</td>\n",
       "      <td>2019</td>\n",
       "      <td>21</td>\n",
       "      <td>97.600000</td>\n",
       "      <td>Ultra Luxury</td>\n",
       "      <td>Still Red</td>\n",
       "      <td>Napa Valley</td>\n",
       "      <td>750</td>\n",
       "      <td>0.561615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                             wine vintage  stock  avg_score  \\\n",
       "0  53995                         Trotanoy    2020      6  98.300000   \n",
       "1  59622                         Lodovico    2020    102  98.000000   \n",
       "2  51070                         St. Eden    2016      3  98.000000   \n",
       "3  53882                Lafite Rothschild    2020   1716  97.923077   \n",
       "4  62256  Cabernet Sauvignon Tychson Hill    2019     21  97.600000   \n",
       "\n",
       "     price_tier  full_type region_group  bottle_size_ml   avg_cpi  \n",
       "0        Luxury  Still Red      Pomerol             750  0.561615  \n",
       "1        Luxury  Still Red     Bolgheri             750  0.561615  \n",
       "2  Ultra Luxury  Still Red  Napa Valley             750  0.561615  \n",
       "3  Ultra Luxury  Still Red     Pauillac             750  0.561615  \n",
       "4  Ultra Luxury  Still Red  Napa Valley             750  0.561615  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- CELL 7: Build/refresh fallback_pool safely ---\n",
    "# Goal: give the scheduler a good \"pool\" to draw from when client prefs can't fill the week.\n",
    "\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "# ---------- Fallbacks if prior cells didn't run ----------\n",
    "try:\n",
    "    OUTPUT_PATH\n",
    "except NameError:\n",
    "    _default_output = Path.home() / \"OneDrive - AVU SA\" / \"AVU CPI Campaign\" / \"Puzzle_control_Reports\" / \"IRON_DATA\"\n",
    "    OUTPUT_PATH = Path(os.getenv(\"OUTPUT_PATH\", _default_output))\n",
    "    OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if 'stock_df' not in globals():\n",
    "    _stock_pkl = OUTPUT_PATH / \"stock_df_final.pkl\"\n",
    "    stock_df = pd.read_pickle(_stock_pkl) if _stock_pkl.exists() else pd.DataFrame()\n",
    "\n",
    "if 'id' not in stock_df.columns:\n",
    "    stock_df['id'] = \"\"  # ensure merge key exists\n",
    "\n",
    "# ---------- Guards: ensure filters + UI vars exist even if Cell 5/6 didn't run ----------\n",
    "# 1) Ensure `filters` is a dict (load from file/env if missing)\n",
    "if 'filters' not in globals() or not isinstance(filters, dict):\n",
    "    filters = {}\n",
    "    _filters_path = os.path.join(\"notebooks\", \"filters.json\")\n",
    "    if os.path.exists(_filters_path):\n",
    "        try:\n",
    "            with open(_filters_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                filters = json.load(f) or {}\n",
    "        except Exception as _e:\n",
    "            print(f\"⚠️ Could not read filters.json: {_e}\")\n",
    "    else:\n",
    "        try:\n",
    "            env_f = os.getenv(\"FILTER_INPUTS\", \"\")\n",
    "            if env_f:\n",
    "                filters = json.loads(env_f)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# 2) Define UI variables with robust defaults\n",
    "if 'selected_type' not in globals():\n",
    "    selected_type = filters.get(\"wine_type\", None)\n",
    "\n",
    "if 'selected_size' not in globals():\n",
    "    _sz = filters.get(\"bottle_size\", None)\n",
    "    try:\n",
    "        selected_size = int(_sz) if _sz is not None and str(_sz).strip().lower() != \"bigger\" else None\n",
    "    except Exception:\n",
    "        selected_size = None\n",
    "\n",
    "if 'last_stock' not in globals():\n",
    "    _ls = filters.get(\"last_stock\", False)\n",
    "    last_stock = (str(_ls).strip().lower() in (\"1\", \"true\", \"yes\", \"y\"))\n",
    "\n",
    "if 'seasonality_boost' not in globals():\n",
    "    _sb = filters.get(\"seasonality_boost\", False)\n",
    "    seasonality_boost = (str(_sb).strip().lower() in (\"1\", \"true\", \"yes\", \"y\"))\n",
    "\n",
    "def _parse_size_to_ml(val):\n",
    "    try:\n",
    "        s = str(val).strip().lower().replace(\"ml\",\"\").replace(\"cl\",\"\").replace(\"l\",\"\")\n",
    "        if not s: return np.nan\n",
    "        v = float(s)\n",
    "        if v <= 100:  # cl\n",
    "            return int(round(v * 10))\n",
    "        if v < 20:    # liters\n",
    "            return int(round(v * 1000))\n",
    "        return int(round(v))  # already ml\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# ---------- Try CPI-augmented pool; else fall back to stock only ----------\n",
    "try:\n",
    "    if \"merged_cpi_df\" in globals():\n",
    "        _cpi_src = merged_cpi_df.copy()\n",
    "        cpi_cols = [c for c in _cpi_src.columns if c.startswith(\"pref_cpi_for_\")]\n",
    "        _avg = _cpi_src[cpi_cols].mean(axis=1) if cpi_cols else pd.Series(np.nan, index=_cpi_src.index)\n",
    "        cpi_avg_df = _cpi_src[['id']].copy()\n",
    "        cpi_avg_df['avg_cpi'] = _avg\n",
    "    else:\n",
    "        _cpi_path = OUTPUT_PATH / \"cpi_matrix_latest.pkl\"\n",
    "        _cpi = pd.read_pickle(_cpi_path) if _cpi_path.exists() else pd.DataFrame()\n",
    "        if _cpi.empty or 'id' not in _cpi.columns:\n",
    "            raise FileNotFoundError(\"cpi_matrix_latest.pkl missing or malformed\")\n",
    "        cpi_cols = [c for c in _cpi.columns if c.startswith(\"pref_cpi_for_\")]\n",
    "        _avg = _cpi[cpi_cols].mean(axis=1) if cpi_cols else pd.Series(np.nan, index=_cpi.index)\n",
    "        cpi_avg_df = pd.DataFrame({\"id\": _cpi['id'].astype(str), \"avg_cpi\": _avg.values})\n",
    "\n",
    "    # Assemble base pool from stock with useful columns\n",
    "    _stock_cols = ['id','wine','vintage','stock','avg_score','price_tier','full_type','region_group']\n",
    "    have_cols = [c for c in _stock_cols if c in stock_df.columns]\n",
    "    base = stock_df[have_cols].copy()\n",
    "    base['id'] = base['id'].astype(str)\n",
    "\n",
    "    # Bottle size to ML (prefer existing column)\n",
    "    if 'bottle_size_ml' in stock_df.columns:\n",
    "        base = base.merge(stock_df[['id','bottle_size_ml']], on='id', how='left')\n",
    "    else:\n",
    "        size_sources = [c for c in stock_df.columns if c.lower() in ('size','size_cl','bottle_size','bottle size')]\n",
    "        if size_sources:\n",
    "            _sz = stock_df[['id', size_sources[0]]].rename(columns={size_sources[0]: 'size_raw'})\n",
    "            _sz['bottle_size_ml'] = _sz['size_raw'].apply(_parse_size_to_ml)\n",
    "            base = base.merge(_sz[['id','bottle_size_ml']], on='id', how='left')\n",
    "        else:\n",
    "            base['bottle_size_ml'] = np.nan\n",
    "\n",
    "    # Attach CPI average\n",
    "    fallback_pool = base.merge(cpi_avg_df, on='id', how='left')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not build CPI-based fallback pool: {e}\")\n",
    "    base_cols = ['id','wine','vintage','stock','avg_score','price_tier','full_type','region_group']\n",
    "    have_cols = [c for c in base_cols if c in stock_df.columns]\n",
    "    if 'id' not in have_cols:\n",
    "        have_cols = ['id'] + have_cols\n",
    "    fallback_pool = stock_df[have_cols].copy()\n",
    "    if 'bottle_size_ml' in stock_df.columns and 'bottle_size_ml' not in fallback_pool.columns:\n",
    "        fallback_pool = fallback_pool.merge(stock_df[['id','bottle_size_ml']], on='id', how='left')\n",
    "    fallback_pool['avg_cpi'] = np.nan  # ensure column exists\n",
    "\n",
    "# ---------- Ensure dtypes ----------\n",
    "fallback_pool['stock']     = pd.to_numeric(fallback_pool.get('stock', 0), errors='coerce').fillna(0).astype(int)\n",
    "fallback_pool['avg_cpi']   = pd.to_numeric(fallback_pool.get('avg_cpi', np.nan), errors='coerce')\n",
    "fallback_pool['avg_score'] = pd.to_numeric(fallback_pool.get('avg_score', np.nan), errors='coerce')\n",
    "\n",
    "# If CPI avg came from Cell 6 as avg_cpi_score, use it\n",
    "if ('avg_cpi' not in fallback_pool.columns) or fallback_pool['avg_cpi'].isna().all():\n",
    "    if 'avg_cpi_score' in stock_df.columns:\n",
    "        try:\n",
    "            fallback_pool = fallback_pool.merge(\n",
    "                stock_df[['id','avg_cpi_score']].rename(columns={'avg_cpi_score':'avg_cpi'}),\n",
    "                on='id', how='left'\n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# ---------- Apply UI filters ----------\n",
    "_selected_type = (selected_type or \"\").strip()\n",
    "_selected_size = selected_size\n",
    "_last_stock    = bool(last_stock)\n",
    "_season_boost  = bool(seasonality_boost)\n",
    "\n",
    "# wine type\n",
    "if _selected_type and _selected_type.lower() != \"all\":\n",
    "    if 'full_type' in fallback_pool.columns:\n",
    "        mask = fallback_pool['full_type'].astype(str).str.contains(_selected_type, case=False, na=False)\n",
    "        fallback_pool = fallback_pool[mask]\n",
    "    else:\n",
    "        mask = False\n",
    "        for c in [c for c in fallback_pool.columns if c.lower() in ('type','color','full_type')]:\n",
    "            mask = mask | fallback_pool[c].astype(str).str.contains(_selected_type, case=False, na=False)\n",
    "        fallback_pool = fallback_pool[mask]\n",
    "\n",
    "# bottle size (ml)\n",
    "if _selected_size:\n",
    "    try:\n",
    "        sel_ml = int(_selected_size)\n",
    "        fallback_pool = fallback_pool[ fallback_pool['bottle_size_ml'].fillna(-1).astype(int) == sel_ml ]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# last stock (<10)\n",
    "if _last_stock:\n",
    "    fallback_pool = fallback_pool[fallback_pool['stock'] < 10]\n",
    "\n",
    "# seasonality boost (only if column exists and is truthy)\n",
    "if _season_boost and 'seasonality_boost' in stock_df.columns:\n",
    "    season_ids = set(\n",
    "        stock_df.loc[\n",
    "            stock_df['seasonality_boost'].apply(lambda x: bool(x) and str(x).strip() not in ('[]','False','false','0')),\n",
    "            'id'\n",
    "        ].astype(str)\n",
    "    )\n",
    "    fallback_pool = fallback_pool[fallback_pool['id'].astype(str).isin(season_ids)]\n",
    "\n",
    "print(f\"🧮 Fallback pool after filters: {len(fallback_pool)} rows\")\n",
    "\n",
    "# ---------- Rank (prefer higher CPI, then score, then stock) ----------\n",
    "if 'avg_cpi' not in fallback_pool.columns:\n",
    "    fallback_pool['avg_cpi'] = np.nan\n",
    "fallback_pool['avg_cpi']   = pd.to_numeric(fallback_pool['avg_cpi'], errors='coerce')\n",
    "fallback_pool['avg_score'] = pd.to_numeric(fallback_pool['avg_score'], errors='coerce')\n",
    "\n",
    "fallback_pool = fallback_pool.sort_values(\n",
    "    ['avg_cpi','avg_score','stock'], ascending=[False, False, False], na_position='last'\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Persist for the webapp/scheduler\n",
    "fallback_path = OUTPUT_PATH / \"fallback_pool.pkl\"\n",
    "fallback_pool.to_pickle(fallback_path)\n",
    "print(f\"💾 Saved fallback_pool → {fallback_path} (top {min(5, len(fallback_pool))} rows):\")\n",
    "display(fallback_pool.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9322eef4-687c-4ef9-b542-ff899c465f38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cadence cards saved → C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA\\calendar\\2025_week_38\\cadence_cards.json and alias C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA\\calendar\\latest\\cadence_cards.json\n",
      "• Tue (Value): Item 64103 → CM-25-02208 | item: 64103\n",
      "• Wed (Luxury): Angélus Hommage à Elisabeth Bouchet → CM-25-02273 | item: 63044\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 8 — Cadence Cards (Value Tue, Luxury Wed) [FIXED robust stock lookup] ---\n",
    "import os, re, json\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------\n",
    "# 0) Paths & week\n",
    "# ------------------------------\n",
    "CALENDAR_PATH = OUTPUT_PATH / \"calendar\"\n",
    "CALENDAR_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    year = int(calendar_year)\n",
    "    week = int(week_number)\n",
    "except Exception:\n",
    "    iso = datetime.now().isocalendar()\n",
    "    year, week = iso.year, iso.week\n",
    "\n",
    "week_root = CALENDAR_PATH / f\"{year}_week_{week:02d}\"\n",
    "week_root.mkdir(parents=True, exist_ok=True)\n",
    "latest_root = CALENDAR_PATH / \"latest\"\n",
    "latest_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _artifact_paths(name: str, ext: str = \"json\"):\n",
    "    fname = f\"{name}.{ext.lstrip('.')}\"\n",
    "    return (week_root / fname, latest_root / fname)\n",
    "\n",
    "def _write_json(name: str, obj):\n",
    "    wp, lp = _artifact_paths(name, \"json\")\n",
    "    wp.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    lp.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    return wp, lp\n",
    "\n",
    "def _tier(price):\n",
    "    try:\n",
    "        p = float(price)\n",
    "    except Exception:\n",
    "        return \"Unknown\"\n",
    "    if p < 50:   return \"Budget\"\n",
    "    if p < 100:  return \"Mid-range\"\n",
    "    if p < 200:  return \"Premium\"\n",
    "    if p < 500:  return \"Luxury\"\n",
    "    return \"Ultra Luxury\"\n",
    "\n",
    "# ------------------------------\n",
    "# 1) Load inputs (single-read; reuse prior artifacts)\n",
    "# ------------------------------\n",
    "succ7_p  = OUTPUT_PATH / \"campaign_success_7d.pkl\"\n",
    "succ14_p = OUTPUT_PATH / \"campaign_success_14d.pkl\"\n",
    "succ30_p = OUTPUT_PATH / \"campaign_success_30d.pkl\"\n",
    "\n",
    "succ7  = pd.read_pickle(succ7_p)  if succ7_p.exists()  else None\n",
    "succ14 = pd.read_pickle(succ14_p) if succ14_p.exists() else None\n",
    "succ30 = pd.read_pickle(succ30_p) if succ30_p.exists() else None\n",
    "\n",
    "# Stock for enrichment\n",
    "if 'stock_df' in globals() and isinstance(stock_df, pd.DataFrame):\n",
    "    stock = stock_df.copy()\n",
    "else:\n",
    "    _sp = OUTPUT_PATH / \"stock_df_final.pkl\"\n",
    "    stock = pd.read_pickle(_sp) if _sp.exists() else pd.DataFrame()\n",
    "if not stock.empty and 'id' in stock.columns:\n",
    "    stock['id'] = stock['id'].astype(str).str.strip()\n",
    "\n",
    "# Clients / loyalty (derived from number of orders in Cell 6)\n",
    "if 'client_pref_df' in globals() and isinstance(client_pref_df, pd.DataFrame):\n",
    "    cpdf = client_pref_df.copy()\n",
    "else:\n",
    "    _cp = OUTPUT_PATH / \"client_pref_df_latest.pkl\"\n",
    "    cpdf = pd.read_pickle(_cp) if _cp.exists() else pd.DataFrame()\n",
    "\n",
    "group_counts = {}\n",
    "if not cpdf.empty and 'loyalty_level' in cpdf.columns:\n",
    "    group_counts = (cpdf['loyalty_level'].astype(str).str.lower()\n",
    "                    .value_counts(dropna=False).to_dict())\n",
    "\n",
    "# Power BI stats (for mapping campaign -> item list)\n",
    "def _get_cached(name):\n",
    "    try:\n",
    "        return DATA_BUS.get(\"frames\", {}).get(name)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "stats = _get_cached(\"powerbi_stats\")\n",
    "if stats is None:\n",
    "    stats_path = SOURCE_PATH / \"Power BI Dtld. Statistics ALL.xlsx\"\n",
    "    stats = pd.read_excel(stats_path) if stats_path.exists() else pd.DataFrame()\n",
    "\n",
    "def _pick(df, *names, regex=None):\n",
    "    if df is None or df.empty: return None\n",
    "    cols = list(df.columns)\n",
    "    if regex:\n",
    "        m = re.compile(regex, re.I)\n",
    "        for c in cols:\n",
    "            if m.search(str(c)): return c\n",
    "    for n in names:\n",
    "        if n in cols: return n\n",
    "    return None\n",
    "\n",
    "col_camp = _pick(stats, \"Campaign No.\", \"Campaign No\", \"Campaign Code\", \"CampaignNo\", regex=r\"\\bcampaign\\b\")\n",
    "col_item = _pick(stats, \"Item No.\", \"Item No\", \"Item\", \"Item Number\", regex=r\"^item(\\s|_)?(no|number)?$\")\n",
    "col_amt  = _pick(stats, \"Total Bottle Amount (LCY)\", regex=r\"total.*bottle.*amount.*lcy\")\n",
    "\n",
    "if not stats.empty:\n",
    "    stats['_Camp_No'] = stats[col_camp].astype(str).str.strip() if col_camp else \"\"\n",
    "    stats['_Item_No'] = (stats[col_item].astype(str).str.strip().str.replace(r\"\\.0$\", \"\", regex=True)) if col_item else \"\"\n",
    "    stats['_Amt']     = pd.to_numeric(stats[col_amt], errors=\"coerce\").fillna(0.0) if col_amt else 0.0\n",
    "else:\n",
    "    stats = pd.DataFrame(columns=['_Camp_No','_Item_No','_Amt'])\n",
    "\n",
    "# ------------------------------\n",
    "# 2) Candidate winners from precomputed success pkl (or approximate fallback)\n",
    "# ------------------------------\n",
    "def _prep_succ(df):\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame(columns=[\"Campaign_No\",\"Weighted_Score\",\"Main_Sales\",\"Conversion\",\"Avg_Item_Price\",\"Tier\",\"campaign_date\"])\n",
    "    rename = {}\n",
    "    for a, b in [(\"Campaign_No\",\"Campaign_No\"),(\"Campaign No\",\"Campaign_No\"),\n",
    "                 (\"Weighted_Score\",\"Weighted_Score\"),(\"Main_Sales\",\"Main_Sales\"),\n",
    "                 (\"Conversion\",\"Conversion\"),(\"Avg_Item_Price\",\"Avg_Item_Price\"),\n",
    "                 (\"campaign_date\",\"campaign_date\")]:\n",
    "        if a in df.columns: rename[a] = b\n",
    "    d = df.rename(columns=rename).copy()\n",
    "    if \"Avg_Item_Price\" not in d.columns: d[\"Avg_Item_Price\"] = np.nan\n",
    "    d[\"Tier\"] = d[\"Avg_Item_Price\"].apply(_tier)\n",
    "    return d[[\"Campaign_No\",\"Weighted_Score\",\"Main_Sales\",\"Conversion\",\"Avg_Item_Price\",\"Tier\",\"campaign_date\"]]\n",
    "\n",
    "cand = pd.concat([_prep_succ(succ30), _prep_succ(succ14), _prep_succ(succ7)], ignore_index=True)\n",
    "cand = cand.dropna(subset=[\"Campaign_No\"]).copy()\n",
    "\n",
    "if cand.empty:\n",
    "    # Fallback: approximate score from sales only (no re-reading OMT)\n",
    "    if stats.empty or (stats['_Camp_No'] == \"\").all():\n",
    "        cand = pd.DataFrame(columns=[\"Campaign_No\",\"Weighted_Score\",\"Main_Sales\",\"Conversion\",\"Avg_Item_Price\",\"Tier\",\"campaign_date\"])\n",
    "    else:\n",
    "        grp = stats.groupby('_Camp_No', as_index=False).agg(Main_Sales=('_Amt','sum'))\n",
    "        grp[\"Conversion\"] = np.nan\n",
    "        # try to find a price column as proxy\n",
    "        col_price = _pick(stats, \"Item Sales Price from Price List\", \"Item Sales Price\", regex=r\"(item\\s*)?sales\\s*price.*list\")\n",
    "        if col_price:\n",
    "            price_df = stats.groupby('_Camp_No', as_index=False)[col_price].median().rename(columns={col_price:\"Avg_Item_Price\"})\n",
    "        else:\n",
    "            price_df = pd.DataFrame({'_Camp_No': grp['_Camp_No'], 'Avg_Item_Price': np.nan})\n",
    "        cand = grp.merge(price_df, on='_Camp_No', how='left').rename(columns={'_Camp_No':'Campaign_No'})\n",
    "        m = float(cand[\"Main_Sales\"].max() or 0.0)\n",
    "        cand[\"Weighted_Score\"] = (cand[\"Main_Sales\"] / m) if m > 0 else 0.0\n",
    "        cand[\"campaign_date\"] = pd.NaT\n",
    "        cand[\"Tier\"] = cand[\"Avg_Item_Price\"].apply(_tier)\n",
    "\n",
    "if not cand.empty:\n",
    "    cand = (cand.sort_values([\"Weighted_Score\",\"Main_Sales\"], ascending=[False, False])\n",
    "                 .drop_duplicates([\"Campaign_No\"], keep=\"first\"))\n",
    "\n",
    "# ------------------------------\n",
    "# 3) Pick Value (Tue) & Luxury (Wed)\n",
    "# ------------------------------\n",
    "def _pick_by_tiers(df, tiers):\n",
    "    if df is None or df.empty: return None\n",
    "    sub = df[df[\"Tier\"].isin(tiers)].copy()\n",
    "    if sub.empty: return None\n",
    "    sub = sub.sort_values([\"Weighted_Score\",\"Main_Sales\",\"Conversion\"], ascending=[False, False, False])\n",
    "    return sub.iloc[0].to_dict()\n",
    "\n",
    "value_tiers = {\"Budget\",\"Mid-range\",\"Premium\"}\n",
    "lux_tiers   = {\"Luxury\",\"Ultra Luxury\"}\n",
    "\n",
    "value_win = _pick_by_tiers(cand, value_tiers)\n",
    "lux_win   = _pick_by_tiers(cand, lux_tiers)\n",
    "\n",
    "# ------------------------------\n",
    "# 4) Map campaign → one or more item candidates (sorted by sales)\n",
    "# ------------------------------\n",
    "def _items_for_campaign(camp_no: str) -> list[str]:\n",
    "    if not camp_no or stats.empty: return []\n",
    "    sub = stats.loc[stats['_Camp_No'].astype(str) == str(camp_no), ['_Item_No','_Amt']].copy()\n",
    "    sub = sub[sub['_Item_No'].astype(str).str.strip() != \"\"]\n",
    "    if sub.empty: return []\n",
    "    sub = sub.groupby('_Item_No', as_index=False).agg(Main_Sales=('_Amt','sum'))\n",
    "    sub = sub.sort_values(\"Main_Sales\", ascending=False)\n",
    "    return [str(x) for x in sub['_Item_No'].tolist()]\n",
    "\n",
    "value_items = _items_for_campaign(value_win[\"Campaign_No\"]) if value_win else []\n",
    "lux_items   = _items_for_campaign(lux_win[\"Campaign_No\"])   if lux_win   else []\n",
    "\n",
    "# ------------------------------\n",
    "# 5) Safe stock meta lookup + card builder (robust to missing stock rows)\n",
    "# ------------------------------\n",
    "def _get_stock_meta(item_id: str) -> dict:\n",
    "    if not item_id or stock.empty or 'id' not in stock.columns: \n",
    "        return {}\n",
    "    try:\n",
    "        recs = stock.loc[stock['id'].astype(str) == str(item_id)].head(1).to_dict(\"records\")\n",
    "        return recs[0] if recs else {}\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def _mk_card_from_candidates(item_ids: list[str], tier_label: str, camp_row: dict, day: str, tag: str):\n",
    "    # try each item until one exists in stock; else fall back to the first id with minimal meta\n",
    "    chosen_id = None\n",
    "    meta = {}\n",
    "    for iid in item_ids:\n",
    "        meta = _get_stock_meta(iid)\n",
    "        if meta:\n",
    "            chosen_id = iid\n",
    "            break\n",
    "    if chosen_id is None and item_ids:\n",
    "        chosen_id = item_ids[0]  # minimal fallback (might not be in stock dataset)\n",
    "        meta = {}\n",
    "\n",
    "    if not chosen_id:\n",
    "        return None  # nothing to emit\n",
    "\n",
    "    label = meta.get(\"wine\") or meta.get(\"Wine\") or f\"Item {chosen_id}\"\n",
    "    vint  = str(meta.get(\"vintage\",\"NV\") or \"NV\")\n",
    "    price_tier = str(meta.get(\"price_tier\") or tier_label or \"\")\n",
    "    region_group = str(meta.get(\"region_group\") or meta.get(\"region\") or \"\")\n",
    "    full_type    = str(meta.get(\"full_type\") or \"\")\n",
    "    chf_price    = (float(meta[\"CHF Price\"]) if (\"CHF Price\" in meta and pd.notna(meta[\"CHF Price\"])) else None)\n",
    "    avg_cpi      = (float(meta[\"avg_cpi_score\"]) if (\"avg_cpi_score\" in meta and pd.notna(meta[\"avg_cpi_score\"])) else 0.0)\n",
    "    avg_score    = (float(meta[\"avg_score\"]) if (\"avg_score\" in meta and pd.notna(meta[\"avg_score\"])) else None)\n",
    "    stock_qty    = int(pd.to_numeric(meta.get(\"stock\", 0), errors=\"coerce\") or 0)\n",
    "\n",
    "    card = {\n",
    "        \"id\": str(chosen_id),\n",
    "        \"wine\": label,\n",
    "        \"vintage\": vint,\n",
    "        \"full_type\": full_type or \"Unknown\",\n",
    "        \"region_group\": region_group or \"Unknown\",\n",
    "        \"price_tier\": price_tier,\n",
    "        \"stock\": stock_qty,\n",
    "        \"match_quality\": f\"Cadence ({tag})\",\n",
    "        \"avg_cpi_score\": avg_cpi,\n",
    "        \"locked\": False,\n",
    "        \"extras\": {\n",
    "            \"producer\": meta.get(\"producer\"),\n",
    "            \"chf_price\": chf_price,\n",
    "            \"avg_score\": avg_score,\n",
    "        },\n",
    "        \"source_campaign_no\": camp_row.get(\"Campaign_No\") if camp_row else None,\n",
    "        \"source_weighted_score\": float(camp_row.get(\"Weighted_Score\", 0.0)) if camp_row else 0.0,\n",
    "        \"source_main_sales\": float(camp_row.get(\"Main_Sales\", 0.0)) if camp_row else 0.0,\n",
    "        \"source_tier\": tier_label,\n",
    "        \"scheduled_day_hint\": day,\n",
    "    }\n",
    "    if camp_row and camp_row.get(\"campaign_date\"):\n",
    "        try:\n",
    "            card[\"last_campaign_date\"] = pd.to_datetime(camp_row[\"campaign_date\"]).date().isoformat()\n",
    "        except Exception:\n",
    "            pass\n",
    "    return card\n",
    "\n",
    "tue_card = _mk_card_from_candidates(value_items, value_win.get(\"Tier\") if value_win else None, value_win, \"Tuesday\", \"Value Winner\") if value_win else None\n",
    "wed_card = _mk_card_from_candidates(lux_items,   lux_win.get(\"Tier\")   if lux_win   else None, lux_win,   \"Wednesday\", \"Luxury Winner\") if lux_win else None\n",
    "\n",
    "# ------------------------------\n",
    "# 6) Persist cadence cards (+ meta)\n",
    "# ------------------------------\n",
    "cadence_payload = {\n",
    "    \"week\": {\"year\": year, \"week_number\": week},\n",
    "    \"cards\": {\n",
    "        \"Tuesday\":  [tue_card] if tue_card else [],\n",
    "        \"Wednesday\":[wed_card] if wed_card else [],\n",
    "    },\n",
    "    \"meta\": {\n",
    "        \"computed_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"group_counts\": group_counts,     # Bronze/Silver/Gold/VIP distribution\n",
    "        \"sources\": {\n",
    "            \"campaign_success_7d\":  str(succ7_p)  if succ7_p.exists() else None,\n",
    "            \"campaign_success_14d\": str(succ14_p) if succ14_p.exists() else None,\n",
    "            \"campaign_success_30d\": str(succ30_p) if succ30_p.exists() else None,\n",
    "            \"stats_file\":           str(SOURCE_PATH / \"Power BI Dtld. Statistics ALL.xlsx\"),\n",
    "            \"stock_pkl\":            str(OUTPUT_PATH / \"stock_df_final.pkl\"),\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "_wp, _lp = _write_json(\"cadence_cards\", cadence_payload)\n",
    "print(f\"✅ Cadence cards saved → { _wp } and alias { _lp }\")\n",
    "if tue_card: print(\"• Tue (Value):\", tue_card[\"wine\"], \"→\", tue_card[\"source_campaign_no\"], \"| item:\", tue_card[\"id\"])\n",
    "if wed_card: print(\"• Wed (Luxury):\", wed_card[\"wine\"], \"→\", wed_card[\"source_campaign_no\"], \"| item:\", wed_card[\"id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3730e2a3-0a32-4955-97d3-a7cf4273ede4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 Winner cards saved → C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA\\calendar\\2025_week_38\\winner_cards.json and alias C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA\\calendar\\latest\\winner_cards.json\n",
      "↩️  Legacy winners alias written → C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA\\winner_cards_current_week.json\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 9 — Resolve winners → stock ids, build winner cards (no re-imports) ---\n",
    "import json, re\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------\n",
    "# 0) Paths & week context\n",
    "# ------------------------------\n",
    "try:\n",
    "    year = int(calendar_year)\n",
    "    week = int(week_number)\n",
    "except Exception:\n",
    "    _iso = datetime.now().isocalendar()\n",
    "    year, week = _iso.year, _iso.week\n",
    "\n",
    "CALENDAR_PATH = OUTPUT_PATH / \"calendar\"\n",
    "WEEK_ROOT     = CALENDAR_PATH / f\"{year}_week_{week:02d}\"\n",
    "LATEST_ROOT   = CALENDAR_PATH / \"latest\"\n",
    "for p in (CALENDAR_PATH, WEEK_ROOT, LATEST_ROOT): p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _artifact_paths(name: str, ext: str = \"json\"):\n",
    "    fname = f\"{name}.{ext.lstrip('.')}\"\n",
    "    return (WEEK_ROOT / fname, LATEST_ROOT / fname)\n",
    "\n",
    "def _write_json(name: str, obj):\n",
    "    wp, lp = _artifact_paths(name, \"json\")\n",
    "    txt = json.dumps(obj, ensure_ascii=False, indent=2)\n",
    "    wp.write_text(txt, encoding=\"utf-8\")\n",
    "    lp.write_text(txt, encoding=\"utf-8\")\n",
    "    return wp, lp\n",
    "\n",
    "# ------------------------------\n",
    "# 1) Reuse cached frames / single-read policy\n",
    "# ------------------------------\n",
    "def _get_cached(name):\n",
    "    try:\n",
    "        return DATA_BUS.get(\"frames\", {}).get(name)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Stock snapshot (prefer in-memory)\n",
    "if 'stock_df' in globals() and isinstance(stock_df, pd.DataFrame):\n",
    "    _stock = stock_df.copy()\n",
    "else:\n",
    "    _sp = OUTPUT_PATH / \"stock_df_final.pkl\"\n",
    "    _stock = pd.read_pickle(_sp) if _sp.exists() else pd.DataFrame()\n",
    "if not _stock.empty and 'id' in _stock.columns:\n",
    "    _stock['id'] = _stock['id'].astype(str).str.strip().str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "\n",
    "# Client prefs (for meta group counts)\n",
    "if 'client_pref_df' in globals() and isinstance(client_pref_df, pd.DataFrame):\n",
    "    _cpdf = client_pref_df.copy()\n",
    "else:\n",
    "    _cp = OUTPUT_PATH / \"client_pref_df_latest.pkl\"\n",
    "    _cpdf = pd.read_pickle(_cp) if _cp.exists() else pd.DataFrame()\n",
    "group_counts = {}\n",
    "if not _cpdf.empty and 'loyalty_level' in _cpdf.columns:\n",
    "    group_counts = (_cpdf['loyalty_level'].astype(str).str.lower()\n",
    "                    .value_counts(dropna=False).to_dict())\n",
    "\n",
    "# Power BI stats for mapping Campaign -> Item list (prefer cache)\n",
    "_stats = _get_cached(\"powerbi_stats\")\n",
    "if _stats is None:\n",
    "    _stats_path = SOURCE_PATH / \"Power BI Dtld. Statistics ALL.xlsx\"\n",
    "    _stats = pd.read_excel(_stats_path) if _stats_path.exists() else pd.DataFrame()\n",
    "\n",
    "def _pick(df, *names, regex=None):\n",
    "    if df is None or df.empty: return None\n",
    "    cols = list(df.columns)\n",
    "    if regex:\n",
    "        rx = re.compile(regex, re.I)\n",
    "        for c in cols:\n",
    "            if rx.search(str(c)): return c\n",
    "    for n in names:\n",
    "        if n in cols: return n\n",
    "    return None\n",
    "\n",
    "_col_camp = _pick(_stats, \"Campaign No.\", \"Campaign Code\", \"CampaignNo\", regex=r\"\\bcampaign\\b\")\n",
    "_col_item = _pick(_stats, \"Item No.\", \"Item\", \"Item Number\", regex=r\"^item(\\s|_)?(no|number)?$\")\n",
    "_col_amt  = _pick(_stats, \"Total Bottle Amount (LCY)\", regex=r\"total.*bottle.*amount.*lcy\")\n",
    "_stats = _stats.copy()\n",
    "if not _stats.empty:\n",
    "    _stats[\"__camp\"]   = _stats.get(_col_camp, pd.Series(\"\", index=_stats.index)).astype(str).str.strip()\n",
    "    _stats[\"__itemno\"] = _stats.get(_col_item, pd.Series(\"\", index=_stats.index)).astype(str).str.strip().str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    _stats[\"__amt\"]    = pd.to_numeric(_stats.get(_col_amt, 0), errors=\"coerce\").fillna(0.0)\n",
    "else:\n",
    "    _stats = pd.DataFrame(columns=[\"__camp\",\"__itemno\",\"__amt\"])\n",
    "\n",
    "# ------------------------------\n",
    "# 2) Load precomputed winners (7/14/30d) from Cell 8’s pipeline\n",
    "# ------------------------------\n",
    "def _load_best(days: int):\n",
    "    p = OUTPUT_PATH / f\"campaign_success_{days}d.pkl\"\n",
    "    if not p.exists() or p.stat().st_size == 0: return None\n",
    "    df = pd.read_pickle(p)\n",
    "    if df is None or df.empty: return None\n",
    "    # normalize expected columns\n",
    "    ren = {}\n",
    "    for a,b in [(\"Campaign_No\",\"Campaign_No\"),(\"Campaign No\",\"Campaign_No\"),\n",
    "                (\"Weighted_Score\",\"Weighted_Score\"),(\"Main_Sales\",\"Main_Sales\"),\n",
    "                (\"Conversion\",\"Conversion\"),(\"Avg_Item_Price\",\"Avg_Item_Price\"),(\"campaign_date\",\"campaign_date\")]:\n",
    "        if a in df.columns: ren[a] = b\n",
    "    d = df.rename(columns=ren).copy()\n",
    "    d = d.sort_values([\"Weighted_Score\",\"Main_Sales\",\"Conversion\"], ascending=[False, False, False])\n",
    "    return d.iloc[0].to_dict()\n",
    "\n",
    "best7  = _load_best(7)\n",
    "best14 = _load_best(14)\n",
    "best30 = _load_best(30)\n",
    "\n",
    "# ------------------------------\n",
    "# 3) Map each winning campaign → candidate items (sorted by sales)\n",
    "# ------------------------------\n",
    "def _items_for_campaign(camp_no: str) -> list[str]:\n",
    "    if not camp_no or _stats.empty: return []\n",
    "    sub = _stats.loc[_stats[\"__camp\"].astype(str) == str(camp_no), [\"__itemno\",\"__amt\"]].copy()\n",
    "    sub = sub[sub[\"__itemno\"].astype(str).str.strip() != \"\"]\n",
    "    if sub.empty: return []\n",
    "    g = (sub.groupby(\"__itemno\", as_index=False)[\"__amt\"].sum()\n",
    "             .sort_values(\"__amt\", ascending=False))\n",
    "    return [str(x) for x in g[\"__itemno\"].tolist()]\n",
    "\n",
    "cand7  = _items_for_campaign(best7.get(\"Campaign_No\"))  if best7  else []\n",
    "cand14 = _items_for_campaign(best14.get(\"Campaign_No\")) if best14 else []\n",
    "cand30 = _items_for_campaign(best30.get(\"Campaign_No\")) if best30 else []\n",
    "\n",
    "# ------------------------------\n",
    "# 4) Resolve to a stock-backed card (robust to missing stock rows)\n",
    "# ------------------------------\n",
    "def _tier(price):\n",
    "    try:\n",
    "        p = float(price)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    if p < 50: return \"Budget\"\n",
    "    if p < 100: return \"Mid-range\"\n",
    "    if p < 200: return \"Premium\"\n",
    "    if p < 500: return \"Luxury\"\n",
    "    return \"Ultra Luxury\"\n",
    "\n",
    "def _choose_stock(item_ids: list[str]) -> tuple[str|None, dict]:\n",
    "    if not item_ids: return None, {}\n",
    "    for iid in item_ids:\n",
    "        if _stock.empty or 'id' not in _stock.columns: break\n",
    "        recs = _stock.loc[_stock['id'].astype(str) == str(iid)].head(1).to_dict(\"records\")\n",
    "        if recs: return str(iid), recs[0]\n",
    "    # fallback to first id without meta\n",
    "    return (str(item_ids[0]), {}) if item_ids else (None, {})\n",
    "\n",
    "def _mk_winner_card(tag: str, best_row: dict, item_ids: list[str]):\n",
    "    if not best_row: return None\n",
    "    chosen_id, meta = _choose_stock(item_ids)\n",
    "    if not chosen_id: return None\n",
    "    tier_name = _tier(best_row.get(\"Avg_Item_Price\"))\n",
    "    card = {\n",
    "        \"id\": chosen_id,\n",
    "        \"wine\": meta.get(\"wine\") or f\"Campaign {best_row.get('Campaign_No')}\",\n",
    "        \"name\": meta.get(\"wine\") or f\"Campaign {best_row.get('Campaign_No')}\",\n",
    "        \"vintage\": str(meta.get(\"vintage\") or \"NV\"),\n",
    "        \"full_type\": meta.get(\"full_type\") or \"Unknown\",\n",
    "        \"region_group\": meta.get(\"region_group\") or \"Unknown\",\n",
    "        \"stock\": int(pd.to_numeric(meta.get(\"stock\", 0), errors=\"coerce\") or 0),\n",
    "        \"price_tier\": meta.get(\"price_tier\") or tier_name or \"\",\n",
    "        \"match_quality\": f\"Winner {tag}\",\n",
    "        \"avg_cpi_score\": float(meta.get(\"avg_cpi_score\")) if pd.notna(meta.get(\"avg_cpi_score\", np.nan)) else 0.0,\n",
    "        \"locked\": True,\n",
    "        \"reason\": f\"Top campaign ({tag}) — score {round(float(best_row.get('Weighted_Score',0)),3)}; \"\n",
    "                  f\"conv {round(float(best_row.get('Conversion',0))*100,2)}%; \"\n",
    "                  f\"sales {round(float(best_row.get('Main_Sales',0)),2)}\",\n",
    "        \"source_campaign_no\": best_row.get(\"Campaign_No\"),\n",
    "    }\n",
    "    if best_row.get(\"campaign_date\"):\n",
    "        try:\n",
    "            card[\"last_campaign_date\"] = pd.to_datetime(best_row[\"campaign_date\"]).date().isoformat()\n",
    "        except Exception:\n",
    "            pass\n",
    "    return card\n",
    "\n",
    "# Place winners Mon/Thu/Fri (leave Wed free for Luxury cadence card from Cell 8)\n",
    "winners_cards = {\n",
    "    \"Monday\":   [_mk_winner_card(\"7d\",  best7,  cand7)]  if best7  else [],\n",
    "    \"Thursday\": [_mk_winner_card(\"14d\", best14, cand14)] if best14 else [],\n",
    "    \"Friday\":   [_mk_winner_card(\"30d\", best30, cand30)] if best30 else [],\n",
    "}\n",
    "# prune None\n",
    "for d in list(winners_cards.keys()):\n",
    "    winners_cards[d] = [x for x in winners_cards[d] if x]\n",
    "\n",
    "# ------------------------------\n",
    "# 5) Persist winners (calendar week + latest + legacy alias)\n",
    "# ------------------------------\n",
    "payload = {\n",
    "    \"week\": {\"year\": year, \"week_number\": week},\n",
    "    \"cards\": winners_cards,\n",
    "    \"meta\": {\n",
    "        \"computed_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"group_counts\": group_counts,\n",
    "        \"sources\": {\n",
    "            \"campaign_success_7d\":  str((OUTPUT_PATH / \"campaign_success_7d.pkl\")),\n",
    "            \"campaign_success_14d\": str((OUTPUT_PATH / \"campaign_success_14d.pkl\")),\n",
    "            \"campaign_success_30d\": str((OUTPUT_PATH / \"campaign_success_30d.pkl\")),\n",
    "            \"stats_file\":           str(SOURCE_PATH / \"Power BI Dtld. Statistics ALL.xlsx\"),\n",
    "            \"stock_pkl\":            str(OUTPUT_PATH / \"stock_df_final.pkl\"),\n",
    "        }\n",
    "    }\n",
    "}\n",
    "wp, lp = _write_json(\"winner_cards\", payload)\n",
    "print(f\"🏆 Winner cards saved → {wp} and alias {lp}\")\n",
    "\n",
    "# Legacy flat file (for older injector code, if any)\n",
    "legacy_path = OUTPUT_PATH / \"winner_cards_current_week.json\"\n",
    "legacy_flat = {day: cards for day, cards in winners_cards.items()}\n",
    "legacy_path.write_text(json.dumps(legacy_flat, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(f\"↩️  Legacy winners alias written → {legacy_path}\")\n",
    "\n",
    "# NOTE: We do NOT inject into a 'locked_json' here; Cell 11 should merge\n",
    "#       both 'winner_cards.json' and 'cadence_cards.json' into the calendar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3408b7c-c036-42f7-986a-5432eb06787f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Cadence cards updated with groups → C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA\\calendar\\2025_week_38\\cadence_cards.json\n",
      "• Tuesday: Item 64103 | tier=Mid-range | groups=['month', 'one', 'twice']\n",
      "• Wednesday: Angélus Hommage à Elisabeth Bouchet | tier=Ultra Luxury | groups=['tris', 'bis', 'twice']\n",
      "↩️  Legacy alias written → C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA\\cadence_cards_current_week.json\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 10 — Cadence winners (Tue/Wed) → attach frequency groups (order-count based) ---\n",
    "import json, re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------\n",
    "# 0) Week + paths (reuse from Cell 1)\n",
    "# ------------------------------\n",
    "try:\n",
    "    year = int(calendar_year)\n",
    "    week = int(week_number)\n",
    "except Exception:\n",
    "    _iso = datetime.now().isocalendar()\n",
    "    year, week = _iso.year, _iso.week\n",
    "\n",
    "CALENDAR_PATH = OUTPUT_PATH / \"calendar\"\n",
    "WEEK_ROOT     = CALENDAR_PATH / f\"{year}_week_{week:02d}\"\n",
    "LATEST_ROOT   = CALENDAR_PATH / \"latest\"\n",
    "for p in (CALENDAR_PATH, WEEK_ROOT, LATEST_ROOT): p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CADENCE_WEEK   = WEEK_ROOT / \"cadence_cards.json\"\n",
    "CADENCE_LATEST = LATEST_ROOT / \"cadence_cards.json\"\n",
    "LEGACY_ALIAS   = OUTPUT_PATH / \"cadence_cards_current_week.json\"\n",
    "\n",
    "def _write_jsons(obj: dict):\n",
    "    txt = json.dumps(obj, ensure_ascii=False, indent=2)\n",
    "    CADENCE_WEEK.write_text(txt, encoding=\"utf-8\")\n",
    "    CADENCE_LATEST.write_text(txt, encoding=\"utf-8\")\n",
    "\n",
    "# ------------------------------\n",
    "# 1) Load cadence cards from Cell 8 (required)\n",
    "# ------------------------------\n",
    "if CADENCE_WEEK.exists() and CADENCE_WEEK.stat().st_size > 0:\n",
    "    cadence_payload = json.loads(CADENCE_WEEK.read_text(encoding=\"utf-8\"))\n",
    "elif CADENCE_LATEST.exists() and CADENCE_LATEST.stat().st_size > 0:\n",
    "    cadence_payload = json.loads(CADENCE_LATEST.read_text(encoding=\"utf-8\"))\n",
    "else:\n",
    "    # Minimal fallback (empty skeleton); later cells can still run\n",
    "    cadence_payload = {\"week\":{\"year\":year,\"week_number\":week},\"cards\":{\"Tuesday\":[],\"Wednesday\":[]},\"meta\":{}}\n",
    "\n",
    "cards = cadence_payload.setdefault(\"cards\", {})\n",
    "tue_card = (cards.get(\"Tuesday\") or [None])[0]\n",
    "wed_card = (cards.get(\"Wednesday\") or [None])[0]\n",
    "\n",
    "# ------------------------------\n",
    "# 2) Compute client frequency groups from order counts\n",
    "#     (reuse cached stats if present; else single read)\n",
    "# ------------------------------\n",
    "def _get_cached(name):\n",
    "    try:\n",
    "        return DATA_BUS.get(\"frames\", {}).get(name)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "_stats = _get_cached(\"powerbi_stats\")\n",
    "if _stats is None:\n",
    "    stats_path = SOURCE_PATH / \"Power BI Dtld. Statistics ALL.xlsx\"\n",
    "    _stats = pd.read_excel(stats_path) if stats_path.exists() else pd.DataFrame()\n",
    "\n",
    "def _pick(df, *names, regex=None):\n",
    "    if df is None or df.empty: return None\n",
    "    cols = list(df.columns)\n",
    "    if regex:\n",
    "        rx = re.compile(regex, re.I)\n",
    "        for c in cols:\n",
    "            if rx.search(str(c)): return c\n",
    "    for n in names:\n",
    "        if n in cols: return n\n",
    "    return None\n",
    "\n",
    "col_cust = _pick(_stats, \"Customer No.\", \"Customer No\", \"CustomerNumber\", regex=r\"customer.*no\")\n",
    "col_doc  = _pick(_stats, \"Document No.\", \"Document No\", \"Order No.\", \"Order No\", regex=r\"(document|order).*(no|number|id)\")\n",
    "col_date = _pick(_stats, \"Posting Date\", \"Document Date\", \"Sales Date\", \"Date\", regex=r\"(posting|document|sales).*(date)\")\n",
    "\n",
    "if not _stats.empty and col_cust:\n",
    "    stats2 = _stats[[col_cust]].copy()\n",
    "    stats2[col_cust] = stats2[col_cust].astype(str).str.strip().str.replace(r\"\\.0$\",\"\", regex=True)\n",
    "    if col_doc:\n",
    "        # Count DISTINCT orders per customer (preferred)\n",
    "        tmp = _stats[[col_cust, col_doc]].copy()\n",
    "        tmp[col_doc] = tmp[col_doc].astype(str).str.strip()\n",
    "        order_counts = (tmp.dropna()\n",
    "                          .drop_duplicates([col_cust, col_doc])\n",
    "                          .groupby(col_cust).size()\n",
    "                          .rename(\"order_count\")\n",
    "                          .reset_index())\n",
    "    else:\n",
    "        # Fallback: count rows per customer\n",
    "        order_counts = (stats2.groupby(col_cust).size()\n",
    "                             .rename(\"order_count\")\n",
    "                             .reset_index())\n",
    "else:\n",
    "    order_counts = pd.DataFrame(columns=[col_cust or \"customer_no\",\"order_count\"])\n",
    "\n",
    "# Map order_count → cadence group\n",
    "def _cadence_group(n: float|int) -> str:\n",
    "    try:\n",
    "        n = int(n)\n",
    "    except Exception:\n",
    "        return \"month\"  # safest, least frequent\n",
    "    if n >= 20: return \"tris\"     # 3x/week candidates\n",
    "    if n >= 10: return \"bis\"      # 2x/week (alias)\n",
    "    if n >= 5:  return \"twice\"    # 2x/week (canonical)\n",
    "    if n >= 2:  return \"one\"      # 1x/week\n",
    "    return \"month\"                # monthly\n",
    "\n",
    "if not order_counts.empty:\n",
    "    order_counts[\"cadence_group\"] = order_counts[\"order_count\"].apply(_cadence_group)\n",
    "    group_counts = order_counts[\"cadence_group\"].value_counts(dropna=False).to_dict()\n",
    "else:\n",
    "    group_counts = {}\n",
    "\n",
    "# Expose in meta (along with mapping rules)\n",
    "cadence_payload.setdefault(\"meta\", {})\n",
    "cadence_payload[\"meta\"][\"group_counts\"] = group_counts\n",
    "cadence_payload[\"meta\"][\"group_rules\"] = {\n",
    "    \"tris\":  \"≥20 orders\",\n",
    "    \"bis\":   \"10–19 orders (2x/week alias)\",\n",
    "    \"twice\": \"5–9 orders (2x/week)\",\n",
    "    \"one\":   \"2–4 orders (1x/week)\",\n",
    "    \"month\": \"0–1 orders (monthly)\"\n",
    "}\n",
    "\n",
    "# ------------------------------\n",
    "# 3) Assign cadence_groups to Tue/Wed cards based on bucket\n",
    "# ------------------------------\n",
    "def _bucket_from_price_tier(tier: str|None, price_hint=None) -> str:\n",
    "    t = (tier or \"\").strip().lower()\n",
    "    if t in {\"budget\",\"mid-range\",\"mid range\",\"premium\"}: return \"value\"\n",
    "    if t in {\"luxury\",\"ultra luxury\",\"ultraluxury\"}: return \"lux\"\n",
    "    if price_hint is not None:\n",
    "        try:\n",
    "            p = float(price_hint);  return \"value\" if p < 200 else \"lux\"\n",
    "        except Exception:\n",
    "            pass\n",
    "    return \"value\"\n",
    "\n",
    "def _recommended_groups(bucket: str, counts: dict) -> list[str]:\n",
    "    pref = [\"tris\",\"bis\",\"twice\"] if bucket == \"lux\" else [\"month\",\"one\",\"twice\"]\n",
    "    out = [g for g in pref if counts.get(g, 0) > 0]\n",
    "    return out or pref[:2]\n",
    "\n",
    "def _attach_groups(card: dict|None) -> dict|None:\n",
    "    if not card: return None\n",
    "    card = dict(card)  # copy\n",
    "    # Backfill missing fields to avoid KeyErrors downstream\n",
    "    card[\"name\"] = card.get(\"name\") or card.get(\"wine\") or f\"Item {card.get('id','')}\".strip()\n",
    "    card[\"price_tier\"] = card.get(\"price_tier\") or \"\"\n",
    "    bucket = _bucket_from_price_tier(card.get(\"price_tier\"))\n",
    "    card[\"campaign_tag\"] = \"cadence\"\n",
    "    card[\"locked\"] = True\n",
    "    card[\"cadence_groups\"] = _recommended_groups(bucket, group_counts)\n",
    "    return card\n",
    "\n",
    "if tue_card:\n",
    "    tue_card = _attach_groups(tue_card)\n",
    "if wed_card:\n",
    "    wed_card = _attach_groups(wed_card)\n",
    "\n",
    "# Put them back (keep structure stable even if missing)\n",
    "cadence_payload[\"cards\"][\"Tuesday\"]   = [tue_card] if tue_card else []\n",
    "cadence_payload[\"cards\"][\"Wednesday\"] = [wed_card] if wed_card else []\n",
    "\n",
    "# ------------------------------\n",
    "# 4) Persist (week + latest + legacy alias)\n",
    "# ------------------------------\n",
    "_write_jsons(cadence_payload)\n",
    "\n",
    "legacy_flat = {\"Tuesday\": cadence_payload[\"cards\"][\"Tuesday\"],\n",
    "               \"Wednesday\": cadence_payload[\"cards\"][\"Wednesday\"]}\n",
    "LEGACY_ALIAS.write_text(json.dumps(legacy_flat, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"📦 Cadence cards updated with groups → {CADENCE_WEEK}\")\n",
    "for d in (\"Tuesday\",\"Wednesday\"):\n",
    "    arr = cadence_payload[\"cards\"].get(d, [])\n",
    "    if arr:\n",
    "        c = arr[0]\n",
    "        nm = c.get(\"name\") or c.get(\"wine\") or str(c.get(\"id\",\"\"))\n",
    "        pt = c.get(\"price_tier\",\"\")\n",
    "        gr = c.get(\"cadence_groups\", [])\n",
    "        print(f\"• {d}: {nm} | tier={pt} | groups={gr}\")\n",
    "    else:\n",
    "        print(f\"• {d}: (none)\")\n",
    "print(f\"↩️  Legacy alias written → {LEGACY_ALIAS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e7ce5b5-643d-441a-889a-6b0f41036891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧩 Calendar composed:\n",
      "  • Monday: 1 main, 0 overflow\n",
      "  • Tuesday: 1 main, 0 overflow\n",
      "  • Wednesday: 1 main, 3 overflow\n",
      "  • Thursday: 1 main, 0 overflow\n",
      "  • Friday: 1 main, 0 overflow\n",
      "  • Saturday: 0 main, 0 overflow\n",
      "  • Sunday: 0 main, 0 overflow\n",
      "✅ Wrote composed → C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA\\calendar\\2025_week_38\\calendar_composed.json and alias C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA\\calendar\\latest\\calendar_composed.json\n",
      "🔒 Locked snapshot → C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA\\locked_weeks\\locked_calendar_2025_week_38.json\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 11 — Compose weekly calendar; add special Wednesday overflow clones ---\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Week + paths\n",
    "try:\n",
    "    year = int(calendar_year)\n",
    "    week = int(week_number)\n",
    "except Exception:\n",
    "    iso = datetime.now().isocalendar()\n",
    "    year, week = iso.year, iso.week\n",
    "\n",
    "CALENDAR_PATH = OUTPUT_PATH / \"calendar\"\n",
    "WEEK_ROOT     = CALENDAR_PATH / f\"{year}_week_{week:02d}\"\n",
    "LATEST_ROOT   = CALENDAR_PATH / \"latest\"\n",
    "LOCKED_PATH   = OUTPUT_PATH / \"locked_weeks\"\n",
    "for p in (CALENDAR_PATH, WEEK_ROOT, LATEST_ROOT, LOCKED_PATH): p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "NUM_SLOTS_SAFE = int(globals().get(\"NUM_SLOTS\", 5))\n",
    "DAYS_FULL_SAFE = list(globals().get(\"DAYS_FULL\", [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]))\n",
    "\n",
    "def _load_json(path: Path):\n",
    "    try:\n",
    "        if path.exists() and path.stat().st_size > 0:\n",
    "            return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# Load parts produced by Cells 8–10\n",
    "winner_week   = _load_json(WEEK_ROOT / \"winner_cards.json\")   or _load_json(LATEST_ROOT / \"winner_cards.json\")   or {}\n",
    "cadence_week  = _load_json(WEEK_ROOT / \"cadence_cards.json\")  or _load_json(LATEST_ROOT / \"cadence_cards.json\")  or {}\n",
    "\n",
    "w_cards = (winner_week.get(\"cards\") or {}) if isinstance(winner_week, dict) else {}\n",
    "c_cards = (cadence_week.get(\"cards\") or {}) if isinstance(cadence_week, dict) else {}\n",
    "\n",
    "# Merge order: winners first (usually locked), then cadence (also locked)\n",
    "merged = {d: [] for d in DAYS_FULL_SAFE}\n",
    "\n",
    "def _sig_val(x): \n",
    "    return \"\" if x is None else str(x).strip()\n",
    "\n",
    "def _signature(card):\n",
    "    if not isinstance(card, dict): return None\n",
    "    return (\n",
    "        _sig_val(card.get(\"id\")),\n",
    "        _sig_val(card.get(\"name\") or card.get(\"wine\")),\n",
    "        _sig_val(card.get(\"source_campaign_no\") or card.get(\"campaign_no\")),\n",
    "        _sig_val(card.get(\"campaign_tag\")),\n",
    "        _sig_val(card.get(\"audience_group\")),    # ← include group so clones are distinct\n",
    "        _sig_val(card.get(\"clone_key\")),         # ← extra safeguard\n",
    "    )\n",
    "\n",
    "def _dedup_append(target_list, items):\n",
    "    seen = { _signature(x) for x in target_list if isinstance(x, dict) }\n",
    "    for it in (items or []):\n",
    "        sig = _signature(it)\n",
    "        if sig and sig not in seen:\n",
    "            target_list.append(it); seen.add(sig)\n",
    "\n",
    "for day in DAYS_FULL_SAFE:\n",
    "    _dedup_append(merged[day], w_cards.get(day, []))\n",
    "    _dedup_append(merged[day], c_cards.get(day, []))\n",
    "\n",
    "# --- Special Wednesday overflow: clone the Luxury cadence card per audience group ---\n",
    "SPECIAL_WEDNESDAY_OVERFLOW = True\n",
    "\n",
    "def _is_luxury_card(card: dict) -> bool:\n",
    "    t = _sig_val(card.get(\"price_tier\")).lower()\n",
    "    return t in {\"luxury\",\"ultra luxury\"}\n",
    "\n",
    "def _clone_for_groups(card: dict, groups: list[str]) -> list[dict]:\n",
    "    clones = []\n",
    "    base_id = _sig_val(card.get(\"id\"))\n",
    "    base_name = card.get(\"name\") or card.get(\"wine\") or f\"Item {base_id}\"\n",
    "    for g in (groups or []):\n",
    "        clone = dict(card)\n",
    "        clone[\"audience_group\"] = g\n",
    "        clone[\"match_quality\"] = (card.get(\"match_quality\") or \"cadence\") + f\" | group:{g}\"\n",
    "        clone[\"clone_key\"] = f\"{base_id}::WED::{g}\"\n",
    "        clone[\"locked\"] = True\n",
    "        # keep same id/wine/etc., only audience changes\n",
    "        clones.append(clone)\n",
    "    return clones\n",
    "\n",
    "if SPECIAL_WEDNESDAY_OVERFLOW:\n",
    "    wed_items = merged.get(\"Wednesday\", [])\n",
    "    # find the luxury cadence card (we expect just one from Cell 10)\n",
    "    lux_card = None\n",
    "    for it in wed_items:\n",
    "        if isinstance(it, dict) and _is_luxury_card(it):\n",
    "            lux_card = it\n",
    "            break\n",
    "    if lux_card:\n",
    "        groups = lux_card.get(\"cadence_groups\") or []   # e.g., ['tris','bis','twice']\n",
    "        # create one overflow clone per audience group\n",
    "        clones = _clone_for_groups(lux_card, groups)\n",
    "        # ensure they don't end up in main; we'll append later (after main fill)\n",
    "        # stash to apply after main/overflow split\n",
    "    else:\n",
    "        clones = []\n",
    "else:\n",
    "    clones = []\n",
    "\n",
    "# Build main/overflow respecting NUM_SLOTS\n",
    "calendar_out = {d: {\"main\": [], \"overflow\": []} for d in DAYS_FULL_SAFE}\n",
    "\n",
    "for day in DAYS_FULL_SAFE:\n",
    "    items = merged.get(day, [])\n",
    "    main = items[:NUM_SLOTS_SAFE]\n",
    "    overflow = items[NUM_SLOTS_SAFE:]\n",
    "    # Inject special Wednesday clones into overflow only\n",
    "    if day == \"Wednesday\" and clones:\n",
    "        # avoid duplicating the exact same object if already present\n",
    "        _dedup_append(overflow, clones)\n",
    "    calendar_out[day][\"main\"] = main\n",
    "    calendar_out[day][\"overflow\"] = overflow\n",
    "\n",
    "# Persist composed week + alias + a compact “weekly_calendar” view for consumers\n",
    "compose_payload = {\n",
    "    \"week\": {\"year\": year, \"week_number\": week},\n",
    "    \"updated_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"weekly_calendar_struct\": calendar_out,\n",
    "    \"weekly_calendar\": { d: calendar_out[d][\"main\"] for d in DAYS_FULL_SAFE },\n",
    "    \"sources\": {\n",
    "        \"winner_cards\":  (WEEK_ROOT / \"winner_cards.json\").name if (WEEK_ROOT / \"winner_cards.json\").exists() else None,\n",
    "        \"cadence_cards\": (WEEK_ROOT / \"cadence_cards.json\").name if (WEEK_ROOT / \"cadence_cards.json\").exists() else None,\n",
    "        \"num_slots\": NUM_SLOTS_SAFE,\n",
    "    }\n",
    "}\n",
    "\n",
    "def _write(name, obj):\n",
    "    txt = json.dumps(obj, ensure_ascii=False, indent=2)\n",
    "    (WEEK_ROOT / f\"{name}.json\").write_text(txt, encoding=\"utf-8\")\n",
    "    (LATEST_ROOT / f\"{name}.json\").write_text(txt, encoding=\"utf-8\")\n",
    "\n",
    "_write(\"calendar_composed\", compose_payload)\n",
    "\n",
    "# Locked snapshot stays main-only (UI expects fixed NUM_SLOTS)\n",
    "locked_week = { day: [ (calendar_out[day][\"main\"][i] if i < len(calendar_out[day][\"main\"]) else None)\n",
    "                       for i in range(NUM_SLOTS_SAFE) ]\n",
    "               for day in DAYS_FULL_SAFE }\n",
    "locked_path = LOCKED_PATH / f\"locked_calendar_{year}_week_{week}.json\"\n",
    "locked_path.write_text(json.dumps(locked_week, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"🧩 Calendar composed:\")\n",
    "for d in DAYS_FULL_SAFE:\n",
    "    m = len(calendar_out[d][\"main\"]); o = len(calendar_out[d][\"overflow\"])\n",
    "    print(f\"  • {d}: {m} main, {o} overflow\")\n",
    "\n",
    "print(f\"✅ Wrote composed → {WEEK_ROOT / 'calendar_composed.json'} and alias {LATEST_ROOT / 'calendar_composed.json'}\")\n",
    "print(f\"🔒 Locked snapshot → {locked_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "839af05a-c816-4f39-8c01-489ed6c11c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ top3_by_type is empty — using 'Unknown' defaults for region/type.\n",
      "🔍 Effective UI filters:\n",
      " {\n",
      "  \"last_stock\": false,\n",
      "  \"seasonality_boost\": false,\n",
      "  \"wine_type\": null,\n",
      "  \"bottle_size\": null,\n",
      "  \"price_tier_bucket\": \"\",\n",
      "  \"price_tiers\": [],\n",
      "  \"loyalty_levels\": []\n",
      "}\n",
      "✅ Saved year+week UI files for 2025-W38 in C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA\n"
     ]
    }
   ],
   "source": [
    "# ---  CELL 12: Save UI-Ready JSON & PKL for Flask ---\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "start_time = time()\n",
    "\n",
    "# ---------- Fallbacks if earlier cells didn't run ----------\n",
    "try:\n",
    "    OUTPUT_PATH\n",
    "except NameError:\n",
    "    _default_output = Path.home() / \"OneDrive - AVU SA\" / \"AVU CPI Campaign\" / \"Puzzle_control_Reports\" / \"IRON_DATA\"\n",
    "    OUTPUT_PATH = Path(os.getenv(\"OUTPUT_PATH\", _default_output))\n",
    "    OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    NUM_SLOTS\n",
    "except NameError:\n",
    "    NUM_SLOTS = 5\n",
    "\n",
    "# Load stock if not in memory\n",
    "if 'stock_df' not in globals():\n",
    "    _stock_pkl = OUTPUT_PATH / \"stock_df_final.pkl\"\n",
    "    stock_df = pd.read_pickle(_stock_pkl) if _stock_pkl.exists() else pd.DataFrame()\n",
    "\n",
    "# Load client pref if not in memory\n",
    "if 'client_pref_df' not in globals():\n",
    "    _pref_pkl = OUTPUT_PATH / \"client_pref_df_latest.pkl\"\n",
    "    client_pref_df = pd.read_pickle(_pref_pkl) if _pref_pkl.exists() else pd.DataFrame(columns=['customer_no'])\n",
    "\n",
    "# ---------- Locked snapshot helpers ----------\n",
    "DAYS = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    "\n",
    "def _load_json(path: Path):\n",
    "    try:\n",
    "        if path.exists() and path.stat().st_size > 0:\n",
    "            return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def _locked_to_df(locked_json: dict, stock_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for day in DAYS:\n",
    "        for it in (locked_json.get(day) or []):\n",
    "            if not it: \n",
    "                continue\n",
    "            rows.append({\"day\": day, **it})\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    df[\"id\"] = df.get(\"id\",\"\").astype(str)\n",
    "    enrich_cols = [\"id\",\"wine\",\"vintage\",\"full_type\",\"region_group\",\"stock\",\"price_tier\",\"avg_cpi_score\"]\n",
    "    have = [c for c in enrich_cols if c in stock_df.columns]\n",
    "    if have:\n",
    "        df = df.merge(stock_df[have].drop_duplicates(\"id\"), on=\"id\", how=\"left\")\n",
    "    df[\"locked\"] = True\n",
    "    # Title-case price_tier if present; otherwise create an empty column\n",
    "    if \"price_tier\" in df.columns:\n",
    "        df[\"price_tier\"] = df[\"price_tier\"].apply(\n",
    "            lambda x: x.title().strip() if isinstance(x, str) and x else \"\"\n",
    "        )\n",
    "    else:\n",
    "        df[\"price_tier\"] = \"\"\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---------- Determine selected year/week and load locked snapshot ----------\n",
    "try:\n",
    "    sel_year = int(calendar_year)\n",
    "except Exception:\n",
    "    sel_year = datetime.now().year\n",
    "try:\n",
    "    sel_week = int(week_number)\n",
    "except Exception:\n",
    "    sel_week = datetime.now().isocalendar().week\n",
    "\n",
    "LOCKED_PATH = OUTPUT_PATH / \"locked_weeks\"\n",
    "LOCKED_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "persisted = _load_json(LOCKED_PATH / f\"locked_calendar_{sel_year}_week_{sel_week}.json\")\n",
    "\n",
    "# Prefer in-memory structured snapshot from Cell 1; else persisted; else empty\n",
    "if \"locked_calendar_snapshot\" in globals() and locked_calendar_snapshot:\n",
    "    # Convert structured snapshot {day:{main,overflow}} → flat lists per day\n",
    "    locked_json = {d: (locked_calendar_snapshot.get(d, {}).get(\"main\") or []) for d in DAYS}\n",
    "elif persisted:\n",
    "    locked_json = persisted\n",
    "else:\n",
    "    locked_json = {d: [] for d in DAYS}\n",
    "\n",
    "# Now that we have locked_json and stock_df, build locked_df\n",
    "locked_df = _locked_to_df(locked_json, stock_df)\n",
    "\n",
    "# --- Load winner boxes built in Cell 15 (optional) ---\n",
    "WINNERS_PATH = OUTPUT_PATH / \"winners_boxes.json\"\n",
    "try:\n",
    "    winner_boxes = json.loads(WINNERS_PATH.read_text(encoding=\"utf-8\")) if WINNERS_PATH.exists() else {}\n",
    "except Exception as _e:\n",
    "    print(f\"⚠️ Could not read winners_boxes.json: {_e}\")\n",
    "    winner_boxes = {}\n",
    "\n",
    "\n",
    "# ---------- Tier helpers ----------\n",
    "TIER_MAP_NAME = {\n",
    "    \"budget\": \"Budget\",\n",
    "    \"entry\": \"Budget\",\n",
    "    \"mid\": \"Mid-range\",\n",
    "    \"mid_range\": \"Mid-range\",\n",
    "    \"mid-range\": \"Mid-range\",\n",
    "    \"premium\": \"Premium\",\n",
    "    \"luxury\": \"Luxury\",\n",
    "    \"ultra\": \"Ultra Luxury\",\n",
    "    \"ultra-luxury\": \"Ultra Luxury\",\n",
    "    \"ultra_luxury\": \"Ultra Luxury\",\n",
    "    \"ultra luxury\": \"Ultra Luxury\",\n",
    "}\n",
    "TIER_MAP_ID = {\n",
    "    \"Budget\": \"budget\",\n",
    "    \"Mid-range\": \"mid\",\n",
    "    \"Premium\": \"premium\",\n",
    "    \"Luxury\": \"luxury\",\n",
    "    \"Ultra Luxury\": \"ultra\",\n",
    "}\n",
    "\n",
    "def canon_tier_name(x: str) -> str:\n",
    "    s = str(x or \"\").strip()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    key = s.lower().replace(\" \", \"_\").replace(\"__\", \"_\").replace(\"-\", \"_\")\n",
    "    return TIER_MAP_NAME.get(key, s)\n",
    "\n",
    "def tier_id_from_name(name: str) -> str:\n",
    "    return TIER_MAP_ID.get(canon_tier_name(name), \"\")\n",
    "\n",
    "def to_ml(size_val):\n",
    "    \"\"\"Accepts '750', '75cl', '1.5L', 750 etc. -> returns int ml or np.nan\"\"\"\n",
    "    if size_val is None:\n",
    "        return np.nan\n",
    "    s = str(size_val).strip().lower()\n",
    "    try:\n",
    "        if s.endswith(\"ml\"):\n",
    "            return int(float(s[:-2]))\n",
    "        if s.endswith(\"cl\"):\n",
    "            return int(float(s[:-2]) * 10)\n",
    "        if s.endswith(\"l\"):\n",
    "            return int(float(s[:-1]) * 1000)\n",
    "        # plain number: if < 100 -> assume cl, else ml\n",
    "        v = float(s)\n",
    "        return int(v * 10) if v < 100 else int(v)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def ensure_cols(df: pd.DataFrame, cols: dict):\n",
    "    \"\"\"Ensure columns exist with default; cols = {name: default}\"\"\"\n",
    "    for c, default in cols.items():\n",
    "        if c not in df.columns:\n",
    "            df[c] = default\n",
    "    return df\n",
    "\n",
    "# ---------- Default region/type from any top picks (optional) ----------\n",
    "default_region = \"Unknown\"\n",
    "default_type   = \"Unknown\"\n",
    "\n",
    "if 'top3_by_type' in globals() and isinstance(top3_by_type, pd.DataFrame) and not top3_by_type.empty:\n",
    "    top_rec = top3_by_type.iloc[0]\n",
    "    wine_id = str(top_rec.get('id', \"\")).strip()\n",
    "    default_type = str(top_rec.get('full_type', 'Unknown')) or 'Unknown'\n",
    "    try:\n",
    "        stock_df_ids = stock_df.copy()\n",
    "        stock_df_ids['id'] = stock_df_ids['id'].astype(str)\n",
    "        match_row = stock_df_ids.loc[stock_df_ids['id'] == wine_id, 'region_group']\n",
    "        if not match_row.empty:\n",
    "            default_region = str(match_row.iloc[0]) or \"Unknown\"\n",
    "    except Exception:\n",
    "        pass\n",
    "else:\n",
    "    print(\"⚠️ top3_by_type is empty — using 'Unknown' defaults for region/type.\")\n",
    "\n",
    "# Build seg_df filling missing values with defaults (but not overwriting existing)\n",
    "seg_df = client_pref_df.copy()\n",
    "seg_df['region_group'] = seg_df.get('region_group', pd.Series(index=seg_df.index, dtype='object')).fillna(default_region)\n",
    "seg_df['full_type']    = seg_df.get('full_type',    pd.Series(index=seg_df.index, dtype='object')).fillna(default_type)\n",
    "\n",
    "# ---------- Load UI filters (file -> env fallback -> defaults) ----------\n",
    "filters_path = Path(\"notebooks\") / \"filters.json\"\n",
    "filters = {}\n",
    "try:\n",
    "    if filters_path.exists():\n",
    "        filters = json.loads(filters_path.read_text(encoding=\"utf-8\")) or {}\n",
    "    elif os.getenv(\"FILTER_INPUTS\"):\n",
    "        filters = json.loads(os.getenv(\"FILTER_INPUTS\"))\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Failed to read filters.json/env: {e}\")\n",
    "finally:\n",
    "    if not filters:\n",
    "        filters = {\n",
    "            \"last_stock\": False,\n",
    "            \"seasonality_boost\": False,\n",
    "            \"wine_type\": None,\n",
    "            \"bottle_size\": None,\n",
    "            \"price_tier_bucket\": \"\",\n",
    "            \"price_tiers\": [],\n",
    "            \"loyalty_levels\": []\n",
    "        }\n",
    "\n",
    "print(\"🔍 Effective UI filters:\\n\", json.dumps(filters, indent=2))\n",
    "\n",
    "# ---------- Normalize stock needed columns ----------\n",
    "stock_df = stock_df.copy()\n",
    "stock_df['id']    = stock_df.get('id', \"\").astype(str)\n",
    "stock_df['stock'] = pd.to_numeric(stock_df.get('stock', 0), errors='coerce').fillna(0).astype(int)\n",
    "stock_df['price_tier'] = stock_df.get('price_tier', \"\").map(canon_tier_name)\n",
    "stock_df['price_tier_id'] = stock_df['price_tier'].map(tier_id_from_name)\n",
    "\n",
    "if 'bottle_size_ml' not in stock_df.columns:\n",
    "    if 'size' in stock_df.columns:\n",
    "        stock_df['bottle_size_ml'] = stock_df['size'].apply(to_ml)\n",
    "    elif 'size_cl' in stock_df.columns:\n",
    "        stock_df['bottle_size_ml'] = pd.to_numeric(stock_df['size_cl'], errors='coerce') * 10\n",
    "    else:\n",
    "        stock_df['bottle_size_ml'] = np.nan\n",
    "\n",
    "stock_df = ensure_cols(stock_df, {'occasion': 'Unknown'})\n",
    "\n",
    "# ---------- Apply global UI filters ----------\n",
    "_selected_tiers = set()\n",
    "if filters.get(\"price_tiers\"):\n",
    "    _selected_tiers.update([canon_tier_name(t) for t in filters[\"price_tiers\"] if t])\n",
    "if filters.get(\"price_tier_bucket\"):\n",
    "    _selected_tiers.add(canon_tier_name(filters[\"price_tier_bucket\"]))\n",
    "\n",
    "if _selected_tiers:\n",
    "    stock_df = stock_df[stock_df['price_tier'].isin(_selected_tiers)].copy()\n",
    "\n",
    "_selected_loyalties = {str(x).strip().lower() for x in (filters.get(\"loyalty_levels\") or []) if str(x).strip()}\n",
    "if _selected_loyalties and 'loyalty_level' in seg_df.columns:\n",
    "    seg_df = seg_df[seg_df['loyalty_level'].str.lower().isin(_selected_loyalties)].copy()\n",
    "\n",
    "last_stock_flag    = bool(filters.get(\"last_stock\", False))\n",
    "seasonality_flag   = bool(filters.get(\"seasonality_boost\", False))\n",
    "selected_type      = filters.get(\"wine_type\", None)\n",
    "selected_size_raw  = filters.get(\"bottle_size\", None)\n",
    "try:\n",
    "    selected_size_ml = int(selected_size_raw) if (selected_size_raw is not None and str(selected_size_raw).isdigit()) else None\n",
    "except Exception:\n",
    "    selected_size_ml = None\n",
    "\n",
    "# ---------- Weekday policies ----------\n",
    "WEEKDAY_OCCASION = {\n",
    "    \"Monday\": \"Casual\", \"Tuesday\": \"Casual\", \"Wednesday\": \"Dinner\",\n",
    "    \"Thursday\": \"Dinner\", \"Friday\": \"Party\", \"Saturday\": \"Gifting\", \"Sunday\": \"Dinner\"\n",
    "}\n",
    "WEEKDAY_TIERS = {\n",
    "    \"Monday\":   [\"Budget\", \"Mid-range\", \"Premium\"],\n",
    "    \"Tuesday\":  [\"Mid-range\", \"Premium\"],\n",
    "    \"Wednesday\":[\"Premium\", \"Luxury\"],\n",
    "    \"Thursday\": [\"Premium\", \"Luxury\"],\n",
    "    \"Friday\":   [\"Luxury\", \"Ultra Luxury\"],\n",
    "    \"Saturday\": [\"Luxury\", \"Ultra Luxury\"],\n",
    "    \"Sunday\":   [\"Budget\", \"Premium\", \"Luxury\"],\n",
    "}\n",
    "\n",
    "# ---------- Selector ----------\n",
    "def get_seasonal_wines_modular(stock_df, seg_df, last_stock_flag=False, seasonality=False,\n",
    "                               selected_type=None, selected_size_ml=None, num_per_day=5):\n",
    "    df = stock_df.copy()\n",
    "\n",
    "    # Type filter\n",
    "    if selected_type and str(selected_type).strip().lower() not in (\"all\", \"\"):\n",
    "        df = df[df['full_type'].astype(str).str.casefold() == str(selected_type).casefold()]\n",
    "\n",
    "    # Size filter (ml)\n",
    "    if selected_size_ml is not None and not np.isnan(selected_size_ml):\n",
    "        df = df[np.isclose(pd.to_numeric(df['bottle_size_ml'], errors='coerce'), selected_size_ml, equal_nan=False)]\n",
    "\n",
    "    # Seasonality: within ~last year → next week window (based on OMT last offer date)\n",
    "    if seasonality:\n",
    "        last_year = datetime.today() - timedelta(days=365)\n",
    "        next_week = last_year + timedelta(days=7)\n",
    "        df['OMT last offer date'] = pd.to_datetime(df.get('OMT last offer date', pd.NaT), errors='coerce')\n",
    "        df = df[df['OMT last offer date'].between(last_year, next_week)]\n",
    "\n",
    "    # Stock filters\n",
    "    if last_stock_flag:\n",
    "        df = df[df['stock'] < 10]\n",
    "    df = df[df['stock'] >= 3]\n",
    "\n",
    "    # Segment match score (region+type)\n",
    "    seg = seg_df[['region_group','full_type']].dropna()\n",
    "    def match_score(row):\n",
    "        return int(((seg['region_group'] == row['region_group']) & (seg['full_type'] == row['full_type'])).sum()) if not seg.empty else 0\n",
    "    df['segment_score'] = df.apply(match_score, axis=1) if not df.empty else 0\n",
    "\n",
    "    ultra_cap = 2\n",
    "    ultra_used = 0\n",
    "\n",
    "    calendar = {}\n",
    "\n",
    "    for day in DAYS:\n",
    "        allowed_tiers = [canon_tier_name(t) for t in WEEKDAY_TIERS.get(day, [])]\n",
    "        occasion = WEEKDAY_OCCASION.get(day, \"Casual\")\n",
    "\n",
    "        pool = df.copy()\n",
    "\n",
    "        # Step 1: match occasion + tier\n",
    "        cand = pool[(pool['occasion'].astype(str).str.casefold() == occasion.casefold()) &\n",
    "                    (pool['price_tier'].isin(allowed_tiers))]\n",
    "\n",
    "        # Ultra cap per week\n",
    "        if ultra_used >= ultra_cap:\n",
    "            cand = cand[cand['price_tier'] != \"Ultra Luxury\"]\n",
    "\n",
    "        sel = cand.sort_values(['segment_score','stock'], ascending=[False, False]).head(num_per_day)\n",
    "\n",
    "        # Step 2: relax occasion\n",
    "        if sel.empty:\n",
    "            cand2 = pool[pool['price_tier'].isin(allowed_tiers)]\n",
    "            if ultra_used >= ultra_cap:\n",
    "                cand2 = cand2[cand2['price_tier'] != \"Ultra Luxury\"]\n",
    "            sel = cand2.sort_values(['segment_score','stock'], ascending=[False, False]).head(num_per_day)\n",
    "\n",
    "        # Step 3: seasonal fallback\n",
    "        if sel.empty:\n",
    "            seasonal_only = pool.copy()\n",
    "            seasonal_only['OMT last offer date'] = pd.to_datetime(seasonal_only.get('OMT last offer date', pd.NaT), errors='coerce')\n",
    "            last_year = datetime.today() - timedelta(days=365)\n",
    "            next_week = last_year + timedelta(days=7)\n",
    "            fallback = seasonal_only[seasonal_only['OMT last offer date'].between(last_year, next_week)]\n",
    "            sel = fallback.sort_values('stock', ascending=False).head(num_per_day)\n",
    "\n",
    "        # Step 4: top stocked fallback\n",
    "        if sel.empty:\n",
    "            sel = pool.sort_values('stock', ascending=False).head(num_per_day)\n",
    "\n",
    "        # track ultra usage\n",
    "        if not sel.empty and \"Ultra Luxury\" in sel['price_tier'].values:\n",
    "            ultra_used = min(ultra_cap, ultra_used + (sel['price_tier'].eq(\"Ultra Luxury\").sum()))\n",
    "\n",
    "        calendar[day] = sel\n",
    "\n",
    "    return calendar\n",
    "\n",
    "# ---------- UI mapping ----------\n",
    "def to_ui_item(row: pd.Series) -> dict:\n",
    "    return {\n",
    "        'id': str(row.get('id', '')),\n",
    "        'wine': row.get('wine', 'Unknown'),\n",
    "        'name': row.get('wine', 'Unknown'),\n",
    "        'vintage': str(row.get('vintage', 'NV')) if pd.notna(row.get('vintage', '')) else 'NV',\n",
    "        'full_type': row.get('full_type', 'Unknown'),\n",
    "        'region_group': row.get('region_group', 'Unknown'),\n",
    "        'stock': int(pd.to_numeric(row.get('stock', 0), errors='coerce') if row.get('stock', 0) is not None else 0),\n",
    "        'price_tier': canon_tier_name(row.get('price_tier', '')),\n",
    "        'price_tier_id': tier_id_from_name(row.get('price_tier', '')),\n",
    "        'match_quality': row.get('match_quality', 'Auto'),\n",
    "        'avg_cpi_score': float(pd.to_numeric(row.get('avg_cpi_score', 0), errors='coerce')) if pd.notna(row.get('avg_cpi_score', np.nan)) else 0.0,\n",
    "        'locked': bool(row.get('locked', False))\n",
    "    }\n",
    "\n",
    "def get_ui_data(top_n_recs: pd.DataFrame, weekly_wines: dict, top_wine_counts: pd.DataFrame,\n",
    "                client_prefs: pd.DataFrame, locked_df: pd.DataFrame | None = None,\n",
    "                winner_boxes: dict | None = None):\n",
    "    calendar_data = {d: [] for d in DAYS}\n",
    "\n",
    "    # 1) Locked first\n",
    "    if isinstance(locked_df, pd.DataFrame) and not locked_df.empty:\n",
    "        for day in DAYS:\n",
    "            for _, r in locked_df.loc[locked_df['day']==day].iterrows():\n",
    "                calendar_data[day].append(to_ui_item(r))\n",
    "\n",
    "    # 1.5) Winners next (dedicated 3 boxes). Skip duplicates, respect NUM_SLOTS\n",
    "    if isinstance(winner_boxes, dict) and winner_boxes:\n",
    "        for day in DAYS:\n",
    "            have = {str(it.get(\"id\",\"\")) for it in calendar_data[day]}\n",
    "            for it in (winner_boxes.get(day) or []):\n",
    "                if len(calendar_data[day]) >= NUM_SLOTS:\n",
    "                    break\n",
    "                rid = str(it.get(\"id\",\"\")).strip()\n",
    "                if rid and rid not in have:\n",
    "                    # ensure basic schema; Cell 10 will normalize tier/ids again\n",
    "                    it.setdefault(\"match_quality\", \"Winner\")\n",
    "                    it.setdefault(\"locked\", True)\n",
    "                    it.setdefault(\"avg_cpi_score\", 0.0)\n",
    "                    calendar_data[day].append(it)\n",
    "                    have.add(rid)\n",
    "\n",
    "    # 2) Auto picks (skip duplicates)\n",
    "    for day, df_day in (weekly_wines or {}).items():\n",
    "        if isinstance(df_day, pd.DataFrame) and not df_day.empty:\n",
    "            have = {str(it.get(\"id\",\"\")) for it in calendar_data[day]}\n",
    "            for _, r in df_day.iterrows():\n",
    "                rid = str(r.get(\"id\",\"\"))\n",
    "                if rid and rid not in have:\n",
    "                    calendar_data[day].append(to_ui_item(r))\n",
    "                    have.add(rid)\n",
    "\n",
    "    # CPI preview (avg of attached)\n",
    "    cpi_score = 0.0\n",
    "    try:\n",
    "        flat = [it.get(\"avg_cpi_score\", 0) for arr in calendar_data.values() for it in arr]\n",
    "        if flat:\n",
    "            cpi_score = float(pd.to_numeric(pd.Series(flat), errors=\"coerce\").fillna(0).mean())\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return {\n",
    "        'weekly_calendar': calendar_data,\n",
    "        'cpi_score': round(cpi_score, 2),\n",
    "        'client_prefs': client_prefs.to_dict('records'),\n",
    "        'top_recommendations': []\n",
    "    }\n",
    "\n",
    "# --- Ensure at least N budget offers across the week (non-destructive to locked/winners) ---\n",
    "def _inject_budget_offers(calendar_data: dict, stock_df: pd.DataFrame, min_budget: int = 3) -> dict:\n",
    "    cal = {d: list(calendar_data.get(d, [])) for d in DAYS}\n",
    "\n",
    "    def _canon_tier(x):\n",
    "        return canon_tier_name(x) if isinstance(x, str) else \"\"\n",
    "\n",
    "    # Current IDs + budget count\n",
    "    current_ids = {str(it.get(\"id\", \"\")).strip()\n",
    "                   for day in DAYS for it in cal.get(day, []) if isinstance(it, dict)}\n",
    "    def _is_budget(it): return _canon_tier(it.get(\"price_tier\", \"\")) == \"Budget\"\n",
    "    have_budget = sum(1 for day in DAYS for it in cal.get(day, []) if _is_budget(it))\n",
    "    need = max(0, min_budget - have_budget)\n",
    "    if need == 0:\n",
    "        return cal\n",
    "\n",
    "    # Candidate pool: Budget, in stock, not used yet\n",
    "    s = stock_df.copy()\n",
    "    s[\"id\"] = s.get(\"id\", \"\").astype(str).str.strip()\n",
    "    s[\"price_tier\"] = s.get(\"price_tier\", \"\").map(canon_tier_name)\n",
    "    s[\"stock\"] = pd.to_numeric(s.get(\"stock\", 0), errors=\"coerce\").fillna(0).astype(int)\n",
    "    s[\"avg_cpi_score\"] = pd.to_numeric(s.get(\"avg_cpi_score\", 0), errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    cand = (s[(s[\"price_tier\"] == \"Budget\") & (s[\"stock\"] > 0) & (~s[\"id\"].isin(current_ids))]\n",
    "            .sort_values([\"avg_cpi_score\", \"stock\"], ascending=[False, False])\n",
    "            .head(need)\n",
    "            .to_dict(\"records\"))\n",
    "\n",
    "    # Preferred placement order: Mon → Tue → Sun → Thu → Fri → Sat → Wed\n",
    "    target_days = [\"Monday\", \"Tuesday\", \"Sunday\", \"Thursday\", \"Friday\", \"Saturday\", \"Wednesday\"]\n",
    "\n",
    "    def _is_protected(it: dict) -> bool:\n",
    "        if not isinstance(it, dict): return False\n",
    "        if bool(it.get(\"locked\", False)): return True\n",
    "        tag = str(it.get(\"match_quality\", \"\")).strip().lower()\n",
    "        return tag in {\"winner\", \"cadence\"}\n",
    "\n",
    "    for r in cand:\n",
    "        card = to_ui_item(pd.Series(r))  # shape the UI fields consistently\n",
    "        placed = False\n",
    "\n",
    "        # 1) Try to append where there is room\n",
    "        for day in target_days:\n",
    "            day_list = cal.setdefault(day, [])\n",
    "            if len(day_list) < NUM_SLOTS:\n",
    "                day_list.append(card)\n",
    "                placed = True\n",
    "                break\n",
    "\n",
    "        # 2) If no room, replace first non-locked, non-winner/cadence item\n",
    "        if not placed:\n",
    "            for day in target_days:\n",
    "                day_list = cal.setdefault(day, [])\n",
    "                replace_idx = next((i for i, it in enumerate(day_list) if not _is_protected(it)), None)\n",
    "                if replace_idx is not None:\n",
    "                    day_list[replace_idx] = card\n",
    "                    placed = True\n",
    "                    break\n",
    "        # If still not placed, we silently skip (calendar is fully protected)\n",
    "\n",
    "    return cal\n",
    "\n",
    "# ---------- Build weekly selection ----------\n",
    "IRON_DATA_PATH = Path.home() / \"OneDrive - AVU SA\" / \"AVU CPI Campaign\" / \"Puzzle_control_Reports\" / \"IRON_DATA\"\n",
    "IRON_DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "weekly_wines = get_seasonal_wines_modular(\n",
    "    stock_df, seg_df,\n",
    "    last_stock_flag=last_stock_flag,\n",
    "    seasonality=seasonality_flag,\n",
    "    selected_type=selected_type,\n",
    "    selected_size_ml=selected_size_ml,\n",
    "    num_per_day=NUM_SLOTS\n",
    ")\n",
    "\n",
    "ui_output_data = get_ui_data(\n",
    "    top_n_recs=pd.DataFrame(),\n",
    "    weekly_wines=weekly_wines,\n",
    "    top_wine_counts=pd.DataFrame(),\n",
    "    client_prefs=client_pref_df,\n",
    "    locked_df=locked_df,\n",
    "    winner_boxes=winner_boxes\n",
    ")\n",
    "\n",
    "# NEW: ensure at least 3 Budget offers without clashing with locked/winner/cadence\n",
    "ui_output_data['weekly_calendar'] = _inject_budget_offers(\n",
    "    ui_output_data['weekly_calendar'], stock_df, min_budget=3\n",
    ")\n",
    "\n",
    "# Normalize & cap to NUM_SLOTS\n",
    "calendar_norm = {d: (ui_output_data['weekly_calendar'].get(d, []) or [])[:NUM_SLOTS] for d in DAYS}\n",
    "\n",
    "# ---------- Write artifacts ----------\n",
    "(IRON_DATA_PATH / \"weekly_campaign_schedule.json\").write_text(\n",
    "    json.dumps(calendar_norm, indent=4), encoding=\"utf-8\"\n",
    ")\n",
    "(IRON_DATA_PATH / f\"weekly_campaign_schedule_{sel_year}_week_{sel_week}.json\").write_text(\n",
    "    json.dumps(calendar_norm, indent=4), encoding=\"utf-8\"\n",
    ")\n",
    "# Legacy (week only)\n",
    "(IRON_DATA_PATH / f\"weekly_campaign_schedule_week_{sel_week}.json\").write_text(\n",
    "    json.dumps(calendar_norm, indent=4), encoding=\"utf-8\"\n",
    ")\n",
    "# UI-preferred artifact\n",
    "(IRON_DATA_PATH / f\"weekly_campaign_schedule_{sel_year}_week_{sel_week}.ui.json\").write_text(\n",
    "    json.dumps({\"weekly_calendar\": calendar_norm}, indent=2), encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "# PKLs\n",
    "pd.to_pickle(calendar_norm, IRON_DATA_PATH / \"weekly_campaign_schedule.pkl\")\n",
    "pd.to_pickle(calendar_norm, IRON_DATA_PATH / f\"weekly_campaign_schedule_{sel_year}_week_{sel_week}.pkl\")\n",
    "pd.to_pickle(calendar_norm, IRON_DATA_PATH / f\"weekly_campaign_schedule_week_{sel_week}.pkl\")\n",
    "\n",
    "# Leads (present if previous logic produced it; otherwise sane default)\n",
    "leads_payload = ui_output_data.get(\"leads_campaigns\", {\"TueWed\": [], \"ThuFri\": []})\n",
    "(IRON_DATA_PATH / \"leads_campaigns.json\").write_text(json.dumps(leads_payload, indent=2), encoding=\"utf-8\")\n",
    "(IRON_DATA_PATH / f\"leads_campaigns_{sel_year}_week_{sel_week}.json\").write_text(json.dumps(leads_payload, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# Index (nested by year→week)\n",
    "index_path = IRON_DATA_PATH / \"schedule_index.json\"\n",
    "try:\n",
    "    idx = json.loads(index_path.read_text(encoding=\"utf-8\")) if index_path.exists() else {}\n",
    "except Exception:\n",
    "    idx = {}\n",
    "idx.setdefault(str(sel_year), {})[str(sel_week)] = {\n",
    "    \"json\": f\"weekly_campaign_schedule_{sel_year}_week_{sel_week}.json\",\n",
    "    \"json_ui\": f\"weekly_campaign_schedule_{sel_year}_week_{sel_week}.ui.json\",\n",
    "    \"pkl\": f\"weekly_campaign_schedule_{sel_year}_week_{sel_week}.pkl\",\n",
    "    \"leads_json\": f\"leads_campaigns_{sel_year}_week_{sel_week}.json\",\n",
    "    \"updated_at\": datetime.now().isoformat(timespec=\"seconds\")\n",
    "}\n",
    "idx[\"_latest_year\"] = str(sel_year)\n",
    "idx[\"_latest_week\"] = str(sel_week)\n",
    "(IRON_DATA_PATH / \"schedule_index.json\").write_text(json.dumps(idx, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ Saved year+week UI files for {sel_year}-W{sel_week} in {IRON_DATA_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d05c215e-37f1-42f5-a12b-1f7d8d954186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📣 Leads saved for 2025-W38\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 13: Build weekly \"Leads\" campaigns (Tue–Wed & Thu–Fri) ---\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Ensure IRON_DATA_PATH exists (fallback to OUTPUT_PATH or default OneDrive path)\n",
    "if 'IRON_DATA_PATH' not in globals():\n",
    "    if 'OUTPUT_PATH' in globals():\n",
    "        IRON_DATA_PATH = OUTPUT_PATH\n",
    "    else:\n",
    "        IRON_DATA_PATH = Path.home() / \"OneDrive - AVU SA\" / \"AVU CPI Campaign\" / \"Puzzle_control_Reports\" / \"IRON_DATA\"\n",
    "IRON_DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _canon_tier_name(x: str) -> str:\n",
    "    s = str(x or \"\").strip().lower().replace(\"-\", \" \").replace(\"_\", \" \")\n",
    "    if not s: return \"\"\n",
    "    if \"ultra\" in s: return \"Ultra Luxury\"\n",
    "    if \"luxury\" in s: return \"Luxury\"\n",
    "    if \"premium\" in s: return \"Premium\"\n",
    "    if \"mid\" in s: return \"Mid-range\"\n",
    "    if \"budget\" in s or \"<\" in s or \"cheap\" in s: return \"Budget\"\n",
    "    return s.title()\n",
    "\n",
    "def _load_inventory(iron_path: Path) -> pd.DataFrame:\n",
    "    # Prefer the most enriched inventory if present\n",
    "    candidates = [\n",
    "        iron_path / \"stock_df_with_seasonality.pkl\",\n",
    "        iron_path / \"stock_df_final.pkl\",\n",
    "    ]\n",
    "    src = next((p for p in candidates if p.exists()), None)\n",
    "    if not src:\n",
    "        return pd.DataFrame(columns=[\"id\",\"wine\",\"vintage\",\"full_type\",\"region_group\",\"stock\",\"price_tier\",\"avg_cpi_score\"])\n",
    "    df = pd.read_pickle(src)\n",
    "\n",
    "    # Normalize columns we rely on\n",
    "    for c in (\"id\",\"wine\",\"vintage\",\"full_type\",\"region_group\",\"stock\",\"price_tier\",\"price\",\"avg_cpi_score\"):\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "\n",
    "    df[\"id\"] = df[\"id\"].astype(\"string\").fillna(\"\").str.replace(r\"\\.0$\", \"\", regex=True).str.strip()\n",
    "    df[\"wine\"] = df[\"wine\"].astype(\"string\").fillna(\"\").str.strip()\n",
    "    df[\"vintage\"] = df[\"vintage\"].astype(\"string\").fillna(\"\").str.strip().replace({\"\": \"NV\",\"nan\":\"NV\"})\n",
    "    df[\"full_type\"] = df[\"full_type\"].astype(\"string\").fillna(\"\").str.strip()\n",
    "    df[\"region_group\"] = df[\"region_group\"].astype(\"string\").fillna(\"\").str.strip()\n",
    "    df[\"stock\"] = pd.to_numeric(df[\"stock\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    # Price tier: prefer provided; else derive from price if present\n",
    "    df[\"price_tier\"] = df[\"price_tier\"].astype(\"string\").fillna(\"\").map(_canon_tier_name)\n",
    "    if df[\"price_tier\"].eq(\"\").any() and \"price\" in df.columns:\n",
    "        price_num = pd.to_numeric(df[\"price\"], errors=\"coerce\")\n",
    "        def _bucket(p):\n",
    "            if pd.isna(p): return \"\"\n",
    "            if p < 50: return \"Budget\"\n",
    "            if p < 100: return \"Mid-range\"\n",
    "            if p < 200: return \"Premium\"\n",
    "            if p < 500: return \"Luxury\"\n",
    "            return \"Ultra Luxury\"\n",
    "        mask = df[\"price_tier\"].eq(\"\")\n",
    "        df.loc[mask, \"price_tier\"] = price_num.map(_bucket)\n",
    "\n",
    "    # keep only rows with something to offer\n",
    "    df = df[df[\"stock\"] > 0].copy()\n",
    "    return df\n",
    "\n",
    "def _attach_last_campaign(inv: pd.DataFrame, history: pd.DataFrame | None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Attach last_campaign_date to inventory rows. Uses history_df when provided;\n",
    "    otherwise falls back to OUTPUT/IRON campaign_index.json if available.\n",
    "    \"\"\"\n",
    "    inv = inv.copy()\n",
    "    inv[\"last_campaign_date\"] = pd.NaT\n",
    "\n",
    "    # 1) If a rich history_df is available\n",
    "    if history is not None and not history.empty and {\"id\",\"wine\",\"vintage\",\"schedule_dt\"}.issubset(history.columns):\n",
    "        hist = history.copy()\n",
    "        hist[\"id\"] = hist[\"id\"].astype(\"string\").fillna(\"\").str.replace(r\"\\.0$\",\"\",regex=True).str.strip()\n",
    "        hist[\"wine\"] = hist[\"wine\"].astype(\"string\").fillna(\"\").str.strip()\n",
    "        hist[\"vintage\"] = hist[\"vintage\"].astype(\"string\").fillna(\"\").str.strip().replace({\"\": \"NV\",\"nan\":\"NV\"})\n",
    "        hist[\"schedule_dt\"] = pd.to_datetime(hist[\"schedule_dt\"], errors=\"coerce\")\n",
    "\n",
    "        last_seen = (\n",
    "            hist.dropna(subset=[\"schedule_dt\"])\n",
    "                .groupby([\"id\",\"wine\",\"vintage\"], dropna=False)[\"schedule_dt\"]\n",
    "                .max()\n",
    "                .reset_index()\n",
    "                .rename(columns={\"schedule_dt\":\"last_campaign_date\"})\n",
    "        )\n",
    "        return inv.merge(last_seen, how=\"left\", on=[\"id\",\"wine\",\"vintage\"])\n",
    "\n",
    "    # 2) Fallback: campaign_index.json (fast, id-based)\n",
    "    idx_path = IRON_DATA_PATH / \"campaign_index.json\"\n",
    "    if not idx_path.exists() and 'OUTPUT_PATH' in globals():\n",
    "        alt = OUTPUT_PATH / \"campaign_index.json\"\n",
    "        if alt.exists():\n",
    "            idx_path = alt\n",
    "\n",
    "    if idx_path.exists():\n",
    "        try:\n",
    "            ci = json.loads(idx_path.read_text(encoding=\"utf-8\"))\n",
    "            by_id = ci.get(\"by_id\", {})\n",
    "            id_map = {str(k): v.get(\"last_campaign_date\") for k, v in by_id.items()}\n",
    "            inv[\"last_campaign_date\"] = inv[\"id\"].map(id_map)\n",
    "            inv[\"last_campaign_date\"] = pd.to_datetime(inv[\"last_campaign_date\"], errors=\"coerce\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return inv\n",
    "\n",
    "def _pick_one(df, sort_cols, asc):\n",
    "    if df.empty: return pd.DataFrame(columns=df.columns)\n",
    "    return df.sort_values(sort_cols, ascending=asc, na_position=\"last\").head(1)\n",
    "\n",
    "def _pick_varied(df, used_ids, k=2):\n",
    "    pool = df[~df[\"id\"].isin(used_ids)].copy()\n",
    "    picks = []\n",
    "\n",
    "    # prefer different full_type across picks if possible\n",
    "    type_order = [\"Sparkling\",\"White\",\"Rosé\",\"Rose\",\"Red\",\"Dessert\"]\n",
    "    for t in type_order:\n",
    "        cand = pool[pool[\"full_type\"].str.contains(t, case=False, na=False)]\n",
    "        got = _pick_one(cand, [\"stock\",\"last_campaign_date\"], [False, True])\n",
    "        if not got.empty:\n",
    "            picks.append(got)\n",
    "            pool = pool[~pool[\"id\"].isin(got[\"id\"])]\n",
    "        if len(picks) >= k:\n",
    "            break\n",
    "\n",
    "    # fallback if we didn’t get enough\n",
    "    if len(picks) < k and not pool.empty:\n",
    "        extra = pool.sort_values([\"stock\",\"last_campaign_date\"], ascending=[False, True], na_position=\"last\").head(k - len(picks))\n",
    "        if not extra.empty:\n",
    "            picks.append(extra)\n",
    "\n",
    "    return pd.concat(picks, ignore_index=True) if picks else pd.DataFrame(columns=df.columns)\n",
    "\n",
    "def build_weekly_leads(iron_path: Path, history_df: pd.DataFrame | None) -> dict:\n",
    "    inv = _load_inventory(iron_path)\n",
    "    inv = _attach_last_campaign(inv, history_df)\n",
    "\n",
    "    # 1) Budget (high stock)\n",
    "    budget = inv[inv[\"price_tier\"].eq(\"Budget\")]\n",
    "    lead_budget = _pick_one(budget, [\"stock\",\"last_campaign_date\"], [False, True])\n",
    "\n",
    "    # 2) Ultra Luxury (low stock)\n",
    "    ultra = inv[inv[\"price_tier\"].eq(\"Ultra Luxury\")]\n",
    "    lead_ultra = _pick_one(ultra, [\"stock\",\"last_campaign_date\"], [True, True])\n",
    "\n",
    "    # 3–4) Two varied items (not already chosen)\n",
    "    used_ids = pd.concat([lead_budget, lead_ultra])[\"id\"].astype(str).tolist() if not pd.concat([lead_budget, lead_ultra]).empty else []\n",
    "    varied = _pick_varied(inv, used_ids, k=2)\n",
    "\n",
    "    leads_df = pd.concat([lead_budget, lead_ultra, varied], ignore_index=True)\n",
    "    if leads_df.empty:\n",
    "        return {\"TueWed\": [], \"ThuFri\": []}\n",
    "\n",
    "    def _row_to_item(r):\n",
    "        return {\n",
    "            \"id\": str(r.get(\"id\",\"\")),\n",
    "            \"wine\": r.get(\"wine\") or \"Unknown\",\n",
    "            \"name\": r.get(\"wine\") or \"Unknown\",\n",
    "            \"vintage\": str(r.get(\"vintage\",\"NV\")) if str(r.get(\"vintage\",\"\")).strip() else \"NV\",\n",
    "            \"full_type\": r.get(\"full_type\") or \"Unknown\",\n",
    "            \"region_group\": r.get(\"region_group\") or \"Unknown\",\n",
    "            \"stock\": int(pd.to_numeric(r.get(\"stock\", 0), errors=\"coerce\") or 0),\n",
    "            \"price_tier\": r.get(\"price_tier\") or \"\",\n",
    "            \"match_quality\": \"Lead\",\n",
    "            \"avg_cpi_score\": float(pd.to_numeric(r.get(\"avg_cpi_score\", 0), errors=\"coerce\") or 0),\n",
    "            \"last_campaign_date\": (\n",
    "                r.get(\"last_campaign_date\").isoformat()\n",
    "                if pd.notna(r.get(\"last_campaign_date\")) else None\n",
    "            ),\n",
    "            # UI helpers:\n",
    "            \"locked\": False,\n",
    "            \"span_days\": 2,             # 2-day “Leads” box\n",
    "            \"campaign_tag\": \"leads\"\n",
    "        }\n",
    "\n",
    "    items = [ _row_to_item(r) for _, r in leads_df.iterrows() ]\n",
    "    # Same set in both boxes (simple + consistent)\n",
    "    return {\"TueWed\": items, \"ThuFri\": items}\n",
    "\n",
    "# Get history_df if available; otherwise pass None (we’ll fall back to campaign_index.json)\n",
    "_hist = history_df if 'history_df' in globals() else None\n",
    "try:\n",
    "    leads_campaigns = build_weekly_leads(IRON_DATA_PATH, _hist)\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ build_weekly_leads failed: {e}\")\n",
    "    leads_campaigns = {\"TueWed\": [], \"ThuFri\": []}\n",
    "\n",
    "# Persist year+week leads alongside generic (for fast API fetches)\n",
    "try:\n",
    "    y, w = int(calendar_year), int(week_number)\n",
    "except Exception:\n",
    "    y, w = datetime.now().year, datetime.now().isocalendar().week\n",
    "\n",
    "lp = IRON_DATA_PATH\n",
    "lp.mkdir(parents=True, exist_ok=True)\n",
    "(lp / \"leads_campaigns.json\").write_text(json.dumps(leads_campaigns, indent=2), encoding=\"utf-8\")\n",
    "(lp / f\"leads_campaigns_{y}_week_{w}.json\").write_text(json.dumps(leads_campaigns, indent=2), encoding=\"utf-8\")\n",
    "print(f\"📣 Leads saved for {y}-W{w}\")\n",
    "\n",
    "# Attach to in-memory UI bundle if present\n",
    "if \"ui_output_data\" not in locals():\n",
    "    ui_output_data = {}\n",
    "ui_output_data[\"leads_campaigns\"] = leads_campaigns\n",
    "\n",
    "# (Removed undefined atomic_write_* calls and the incorrect 'leads_payload' reference.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc670ad4-47b7-4ad8-b3dc-b644a7e0b9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ update_status unavailable: No module named 'notebook_status'\n",
      "✅ UI JSON and PKL saved atomically for week 38\n",
      "🧩 Calendar composed (main vs overflow):\n",
      "  • Monday: 5 main, 0 overflow\n",
      "  • Tuesday: 5 main, 0 overflow\n",
      "  • Wednesday: 5 main, 1 overflow\n",
      "  • Thursday: 5 main, 0 overflow\n",
      "  • Friday: 5 main, 0 overflow\n",
      "  • Saturday: 5 main, 0 overflow\n",
      "  • Sunday: 5 main, 0 overflow\n",
      "🎯 CPI score: 0.18\n",
      "📣 Leads (counts): {'TueWed': 0, 'ThuFri': 0}\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 14: Flask handoff & week outputs (atomic + robust, preserves overflow) ---\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === Safety Check ===\n",
    "if \"ui_output_data\" not in locals():\n",
    "    raise RuntimeError(\"❌ ui_output_data not found. Make sure the previous cell populated it.\")\n",
    "\n",
    "# === Status bridge (prefer package import; fallback to none) ===\n",
    "try:\n",
    "    from utils.notebook_status import update_status  # when run by Flask\n",
    "except Exception:\n",
    "    try:\n",
    "        from notebook_status import update_status     # when run locally\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ update_status unavailable: {e}\")\n",
    "        update_status = None\n",
    "\n",
    "# === Paths / Week ===\n",
    "IRON_DATA_PATH = Path(\n",
    "    globals().get(\"OUTPUT_PATH\")\n",
    "    or globals().get(\"output_path\")\n",
    "    or os.getenv(\"OUTPUT_PATH\")\n",
    "    or os.getenv(\"IRON_DATA\")\n",
    "    or (Path.home() / \"OneDrive - AVU SA\" / \"AVU CPI Campaign\" / \"Puzzle_control_Reports\" / \"IRON_DATA\")\n",
    ")\n",
    "IRON_DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    week_number = int(globals().get(\"week_number\", os.getenv(\"WEEK_NUMBER\", datetime.now().isocalendar().week)))\n",
    "except Exception:\n",
    "    week_number = datetime.now().isocalendar().week\n",
    "\n",
    "# === Helpers ===\n",
    "DAYS = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    "\n",
    "TIER_MAP_NAME = {\n",
    "    \"budget\":\"Budget\",\"entry\":\"Budget\",\n",
    "    \"mid\":\"Mid-range\",\"mid_range\":\"Mid-range\",\"mid-range\":\"Mid-range\",\n",
    "    \"premium\":\"Premium\",\n",
    "    \"luxury\":\"Luxury\",\n",
    "    \"ultra\":\"Ultra Luxury\",\"ultra_luxury\":\"Ultra Luxury\",\"ultra-luxury\":\"Ultra Luxury\",\"ultra luxury\":\"Ultra Luxury\",\n",
    "}\n",
    "TIER_MAP_ID = {\"Budget\":\"budget\",\"Mid-range\":\"mid\",\"Premium\":\"premium\",\"Luxury\":\"luxury\",\"Ultra Luxury\":\"ultra\"}\n",
    "\n",
    "def canon_tier_name(x: str) -> str:\n",
    "    s = str(x or \"\").strip()\n",
    "    if not s: return \"\"\n",
    "    key = s.lower().replace(\"-\", \"_\").replace(\" \", \"_\")\n",
    "    return TIER_MAP_NAME.get(key, s)\n",
    "\n",
    "def tier_id_from_name(name: str) -> str:\n",
    "    return TIER_MAP_ID.get(canon_tier_name(name), \"\")\n",
    "\n",
    "def _to_int(x, default=0):\n",
    "    try:\n",
    "        return int(float(x))\n",
    "    except Exception:\n",
    "        try:\n",
    "            return int(pd.to_numeric(x, errors=\"coerce\").fillna(default))\n",
    "        except Exception:\n",
    "            return default\n",
    "\n",
    "def _now_iso():\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def atomic_write_text(path: Path, text: str):\n",
    "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
    "    tmp.write_text(text, encoding=\"utf-8\")\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "def atomic_write_pkl(path: Path, obj):\n",
    "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
    "    pd.to_pickle(obj, tmp)\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "def _norm_item(it: dict) -> dict:\n",
    "    it = it or {}\n",
    "    price_tier = canon_tier_name(it.get(\"price_tier\", it.get(\"price_tier_bucket\", \"\")))\n",
    "    return {\n",
    "        \"id\":              str(it.get(\"id\", \"\")),\n",
    "        \"wine\":            it.get(\"wine\") or it.get(\"name\") or \"Unknown\",\n",
    "        \"name\":            it.get(\"name\") or it.get(\"wine\") or \"Unknown\",\n",
    "        \"vintage\":         str(it.get(\"vintage\", \"NV\")) if str(it.get(\"vintage\",\"\")).strip() else \"NV\",\n",
    "        \"full_type\":       it.get(\"full_type\", \"Unknown\"),\n",
    "        \"region_group\":    it.get(\"region_group\", \"Unknown\"),\n",
    "        \"stock\":           _to_int(it.get(\"stock\", it.get(\"stock_count\", 0))),\n",
    "        \"price_tier\":      price_tier,\n",
    "        \"price_tier_id\":   tier_id_from_name(price_tier),\n",
    "        \"match_quality\":   it.get(\"match_quality\", \"Auto\"),\n",
    "        \"avg_cpi_score\":   float(pd.to_numeric(it.get(\"avg_cpi_score\", it.get(\"cpi_score\", 0)), errors=\"coerce\") or 0),\n",
    "        \"locked\":          bool(it.get(\"locked\", False)),\n",
    "    }\n",
    "\n",
    "def normalize_calendar_with_overflow(calendar_raw: dict, num_slots: int = 5,\n",
    "                                     allow_overflow_days: set = {\"Wednesday\"}):\n",
    "    \"\"\"\n",
    "    Accepts either:\n",
    "      - flat: {day: [items]}\n",
    "      - structured: {day: {\"main\":[...], \"overflow\":[...]}}\n",
    "    Returns (calendar_main, calendar_overflow) both normalized.\n",
    "    \"\"\"\n",
    "    cal_main = {d: [] for d in DAYS}\n",
    "    cal_over = {d: [] for d in DAYS}\n",
    "    if not isinstance(calendar_raw, dict):\n",
    "        return cal_main, cal_over\n",
    "\n",
    "    for day in DAYS:\n",
    "        node = calendar_raw.get(day) or []\n",
    "        # support structured input\n",
    "        if isinstance(node, dict) and (\"main\" in node or \"overflow\" in node):\n",
    "            main = node.get(\"main\") or []\n",
    "            over = node.get(\"overflow\") or []\n",
    "            main_norm = [_norm_item(x) for x in main]\n",
    "            over_norm = [_norm_item(x) for x in over]\n",
    "            # enforce cap only on main; preserve provided overflow\n",
    "            cal_main[day] = main_norm[:num_slots]\n",
    "            cal_over[day] = over_norm\n",
    "        else:\n",
    "            items = [_norm_item(x) for x in (node if isinstance(node, list) else [])]\n",
    "            if day in (allow_overflow_days or set()):\n",
    "                cal_main[day] = items[:num_slots]\n",
    "                cal_over[day] = items[num_slots:]  # preserve extras as overflow\n",
    "            else:\n",
    "                cal_main[day] = items[:num_slots]\n",
    "                cal_over[day] = []  # drop extras for non-overflow days\n",
    "    return cal_main, cal_over\n",
    "\n",
    "# === Normalize payload from the previous cell(s) ===\n",
    "calendar_raw = ui_output_data.get(\"weekly_calendar\", {}) or {}\n",
    "cal_main, cal_over = normalize_calendar_with_overflow(calendar_raw, num_slots=5, allow_overflow_days={\"Wednesday\"})\n",
    "\n",
    "# Leads payload produced earlier\n",
    "leads_payload = ui_output_data.get(\"leads_campaigns\", {\"TueWed\": [], \"ThuFri\": []})\n",
    "\n",
    "# Optional summary for status callback\n",
    "summary = {\n",
    "    \"week_number\": week_number,\n",
    "    \"updated_at\": _now_iso(),\n",
    "    \"days\": {d: len(cal_main.get(d, [])) for d in DAYS},\n",
    "    \"overflow\": {d: len(cal_over.get(d, [])) for d in DAYS},\n",
    "    \"cpi_score\": ui_output_data.get(\"cpi_score\", None),\n",
    "    \"top_recommendations\": ui_output_data.get(\"top_recommendations\", []),\n",
    "    \"leads_counts\": {k: len(v or []) for k, v in (leads_payload or {}).items()}\n",
    "}\n",
    "\n",
    "# === Save (atomic) ===\n",
    "json_generic = IRON_DATA_PATH / \"weekly_campaign_schedule.json\"                     # flat/main only (compat)\n",
    "pkl_generic  = IRON_DATA_PATH / \"weekly_campaign_schedule.pkl\"\n",
    "json_week    = IRON_DATA_PATH / f\"weekly_campaign_schedule_week_{week_number}.json\"\n",
    "pkl_week     = IRON_DATA_PATH / f\"weekly_campaign_schedule_week_{week_number}.pkl\"\n",
    "\n",
    "# Structured artifact that preserves overflow\n",
    "json_struct  = IRON_DATA_PATH / \"weekly_campaign_schedule_structured.ui.json\"\n",
    "json_struct_w= IRON_DATA_PATH / f\"weekly_campaign_schedule_structured_week_{week_number}.ui.json\"\n",
    "\n",
    "# Dedicated leads files\n",
    "leads_json_generic = IRON_DATA_PATH / \"leads_campaigns.json\"\n",
    "leads_json_week    = IRON_DATA_PATH / f\"leads_campaigns_week_{week_number}.json\"\n",
    "\n",
    "index_path   = IRON_DATA_PATH / \"schedule_index.json\"\n",
    "\n",
    "try:\n",
    "    # Write calendar artifacts (main-only for legacy/UI)\n",
    "    atomic_write_text(json_generic, json.dumps(cal_main, indent=4))\n",
    "    atomic_write_text(json_week,    json.dumps(cal_main, indent=4))\n",
    "    atomic_write_pkl(pkl_generic, cal_main)\n",
    "    atomic_write_pkl(pkl_week,    cal_main)\n",
    "\n",
    "    # Write structured artifacts (main + overflow preserved)\n",
    "    atomic_write_text(json_struct,  json.dumps({\"weekly_calendar\": cal_main, \"overflow\": cal_over}, indent=2))\n",
    "    atomic_write_text(json_struct_w, json.dumps({\"weekly_calendar\": cal_main, \"overflow\": cal_over}, indent=2))\n",
    "\n",
    "    # Write leads artifacts\n",
    "    atomic_write_text(leads_json_generic, json.dumps(leads_payload, indent=2))\n",
    "    atomic_write_text(leads_json_week,    json.dumps(leads_payload, indent=2))\n",
    "\n",
    "    # Maintain a simple index for the server (keeps legacy; adds structured)\n",
    "    try:\n",
    "        idx = json.loads(index_path.read_text(encoding=\"utf-8\")) if index_path.exists() else {}\n",
    "    except Exception:\n",
    "        idx = {}\n",
    "    idx[str(week_number)] = {\n",
    "        \"json\": json_week.name,\n",
    "        \"pkl\":  pkl_week.name,\n",
    "        \"leads_json\": leads_json_week.name,\n",
    "        \"structured\": json_struct_w.name,\n",
    "        \"updated_at\": summary[\"updated_at\"]\n",
    "    }\n",
    "    idx[\"_latest\"] = str(week_number)\n",
    "    atomic_write_text(index_path, json.dumps(idx, indent=2))\n",
    "\n",
    "    # Console summary\n",
    "    main_counts = {d: len(cal_main[d]) for d in DAYS}\n",
    "    over_counts = {d: len(cal_over[d]) for d in DAYS}\n",
    "    print(f\"✅ UI JSON and PKL saved atomically for week {week_number}\")\n",
    "    print(\"🧩 Calendar composed (main vs overflow):\")\n",
    "    for d in DAYS:\n",
    "        print(f\"  • {d}: {main_counts[d]} main, {over_counts[d]} overflow\")\n",
    "    print(\"🎯 CPI score:\", ui_output_data.get('cpi_score', 'N/A'))\n",
    "    print(\"📣 Leads (counts):\", summary[\"leads_counts\"])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred while saving UI files: {e}\")\n",
    "\n",
    "# === Report back to Flask UI (if available) ===\n",
    "if callable(update_status):\n",
    "    try:\n",
    "        _nb_name = globals().get(\"NOTEBOOK_NAME\") or os.getenv(\"NOTEBOOK_NAME\") or \"AVU_schedule_only.ipynb\"\n",
    "        update_status(\n",
    "            progress=100,\n",
    "            message=\"✅ Notebook finished.\",\n",
    "            state=\"completed\",\n",
    "            done=True,\n",
    "            notebook=_nb_name,\n",
    "            meta=summary\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ update_status failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5810577a-4a46-410c-b2c8-868f5df22a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Power BI layout files saved:\n",
      "   • C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA\\powerbi_wine_arrow_layout.xlsx\n",
      "   • C:\\Users\\Marco.Africani\\OneDrive - AVU SA\\AVU CPI Campaign\\Puzzle_control_Reports\\IRON_DATA\\powerbi_wine_arrow_layout.csv\n",
      "🧭 Rows: 2389 | Columns: 15\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 15: Generate a file readable by Power BI (from normalized stock)\n",
    "# --- then PowerBI visualization will be on the UI\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Get the normalized stock frame\n",
    "try:\n",
    "    df = stock_df.copy()  # from previous cells\n",
    "except NameError:\n",
    "    df_path = OUTPUT_PATH / \"stock_df_final.pkl\"\n",
    "    if not df_path.exists():\n",
    "        raise FileNotFoundError(f\"❌ Could not find {df_path}. Run earlier cells first.\")\n",
    "    df = pd.read_pickle(df_path)\n",
    "\n",
    "# 2) Ensure required columns exist\n",
    "for col in [\"price_tier\", \"CHF Price\", \"vintage\", \"id\", \"wine\", \"region_group\", \"full_type\", \"stock\"]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = np.nan\n",
    "\n",
    "# Normalize IDs and numeric CHF price\n",
    "df[\"id\"] = df[\"id\"].astype(str).str.strip().str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "df[\"CHF Price\"] = pd.to_numeric(df[\"CHF Price\"], errors=\"coerce\")\n",
    "df[\"stock\"] = pd.to_numeric(df[\"stock\"], errors=\"coerce\")\n",
    "\n",
    "# 3) Canonicalize price tiers + add stable ID\n",
    "def price_tier_from_price(price):\n",
    "    try:\n",
    "        p = float(price)\n",
    "    except Exception:\n",
    "        return None\n",
    "    if p < 50:   return \"Budget\"\n",
    "    if p < 100:  return \"Mid-range\"\n",
    "    if p < 200:  return \"Premium\"\n",
    "    if p < 500:  return \"Luxury\"\n",
    "    return \"Ultra Luxury\"\n",
    "\n",
    "def canon_tier_name(x):\n",
    "    s = str(x or \"\").strip().lower()\n",
    "    if not s: return \"\"\n",
    "    if \"ultra\" in s: return \"Ultra Luxury\"\n",
    "    if \"luxury\" in s: return \"Luxury\"\n",
    "    if \"premium\" in s: return \"Premium\"\n",
    "    if \"mid\" in s: return \"Mid-range\"\n",
    "    if \"budget\" in s: return \"Budget\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "def tier_id(name):\n",
    "    m = {\n",
    "        \"Budget\":\"budget\",\n",
    "        \"Mid-range\":\"mid\",\n",
    "        \"Premium\":\"premium\",\n",
    "        \"Luxury\":\"luxury\",\n",
    "        \"Ultra Luxury\":\"ultra\",\n",
    "        \"Unknown\":\"\"\n",
    "    }\n",
    "    return m.get(name, \"\")\n",
    "\n",
    "# Fill missing/blank tiers from price, then canonicalize\n",
    "df[\"price_tier\"] = df[\"price_tier\"].where(\n",
    "    df[\"price_tier\"].notna() & (df[\"price_tier\"].astype(str).str.strip() != \"\")\n",
    ")\n",
    "df.loc[df[\"price_tier\"].isna() | (df[\"price_tier\"].astype(str).str.strip()==\"\"),\n",
    "       \"price_tier\"] = df[\"CHF Price\"].apply(price_tier_from_price)\n",
    "\n",
    "df[\"Cleaned_Price_Tier\"] = df[\"price_tier\"].apply(canon_tier_name)\n",
    "df[\"price_tier_id\"] = df[\"Cleaned_Price_Tier\"].apply(tier_id)\n",
    "\n",
    "# 4) Vintage grouping (Power BI-friendly labels) — prefer selected calendar year\n",
    "try:\n",
    "    _year_base = int(calendar_year)\n",
    "except Exception:\n",
    "    _year_base = datetime.now().year\n",
    "\n",
    "def classify_vintage_group(vint):\n",
    "    try:\n",
    "        s = str(vint).strip().upper()\n",
    "        if s == \"NV\": return \"Non-Vintage\"\n",
    "        y = int(s)\n",
    "        cy = _year_base\n",
    "        if y == cy:       return \"Current Vintage\"\n",
    "        if y == cy - 1:   return \"En Primeur\"\n",
    "        if cy - y <= 3:   return \"Young Wines\"\n",
    "        if cy - y <= 8:   return \"Mature Wines\"\n",
    "        if cy - y > 15:   return \"Old Wines\"\n",
    "        return \"Unknown\"\n",
    "    except Exception:\n",
    "        return \"Unknown\"\n",
    "\n",
    "df[\"Vintage_Group\"] = df[\"vintage\"].apply(classify_vintage_group)\n",
    "\n",
    "# 5) Order within each tier for triangular/scatter layout\n",
    "#    Sort by tier (alphabetical with canonical names), then by CHF price desc, then by stock desc, then wine asc\n",
    "df = df.sort_values(\n",
    "    [\"Cleaned_Price_Tier\", \"CHF Price\", \"stock\", \"wine\"],\n",
    "    ascending=[True, False, False, True],\n",
    "    na_position=\"last\"\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Tier-local index\n",
    "df[\"Tier_Index\"] = df.groupby(\"Cleaned_Price_Tier\").cumcount() + 1\n",
    "\n",
    "# Correct triangular row + centered X within row\n",
    "# Row r is the smallest r with T_r >= k where T_r = r(r+1)/2\n",
    "def compute_row(idx):\n",
    "    # idx is 1-based; use ceil to avoid under-assigning rows\n",
    "    return int(np.ceil((np.sqrt(8*idx + 1) - 1) / 2.0))\n",
    "\n",
    "df[\"Row_Number\"] = df[\"Tier_Index\"].apply(compute_row)\n",
    "\n",
    "# Position within the row: k = idx - T_{r-1}, then center with k - (r+1)/2\n",
    "def tri_prev(r):  # T_{r-1}\n",
    "    return r*(r-1)//2\n",
    "\n",
    "df[\"k_in_row\"] = df[\"Tier_Index\"] - df[\"Row_Number\"].apply(tri_prev)\n",
    "df[\"Arrow_Y\"] = -df[\"Row_Number\"]\n",
    "df[\"Arrow_X_raw\"] = df[\"k_in_row\"] - (df[\"Row_Number\"] + 1)/2.0\n",
    "\n",
    "# Horizontal offsets per tier (keeps tiers separated left→right)\n",
    "tier_offsets = {\n",
    "    \"Budget\": 0,\n",
    "    \"Mid-range\": 10,\n",
    "    \"Premium\": 20,\n",
    "    \"Luxury\": 30,\n",
    "    \"Ultra Luxury\": 40\n",
    "}\n",
    "df[\"Tier_X_Offset\"] = df[\"Cleaned_Price_Tier\"].map(tier_offsets).fillna(0)\n",
    "df[\"Arrow_X\"] = df[\"Arrow_X_raw\"] + df[\"Tier_X_Offset\"]\n",
    "\n",
    "# 6) Select nice columns for Power BI (keep original too)\n",
    "cols = [\n",
    "    \"id\",\"wine\",\"vintage\",\"Vintage_Group\",\n",
    "    \"CHF Price\",\"stock\",\"region_group\",\"full_type\",\n",
    "    \"price_tier\",\"Cleaned_Price_Tier\",\"price_tier_id\",\n",
    "    \"Tier_Index\",\"Row_Number\",\"Arrow_X\",\"Arrow_Y\"\n",
    "]\n",
    "present_cols = [c for c in cols if c in df.columns]\n",
    "out_df = df[present_cols].copy()\n",
    "\n",
    "# 7) Save to IRON_DATA/OUTPUT_PATH so Power BI can pick it up\n",
    "out_xlsx = OUTPUT_PATH / \"powerbi_wine_arrow_layout.xlsx\"\n",
    "out_csv  = OUTPUT_PATH / \"powerbi_wine_arrow_layout.csv\"\n",
    "\n",
    "out_df.to_excel(out_xlsx, index=False)\n",
    "out_df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"✅ Power BI layout files saved:\")\n",
    "print(\"   •\", out_xlsx)\n",
    "print(\"   •\", out_csv)\n",
    "print(\"🧭 Rows:\", len(out_df), \"| Columns:\", len(out_df.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5fb271e-c79a-45a5-908b-77346e8b4b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 Using: weekly_campaign_schedule_2025_week_38.json  | Exists: True\n",
      "🧩 Cards total: 35\n",
      "✅ Hydration: updated fields\n",
      "✅ Required keys present on ALL cards: emails_sent, last_campaign_date\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 16: Hydrate weekly cards with history fields (force keys) ---\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json, re\n",
    "\n",
    "ENABLE_HYDRATION = True\n",
    "ENABLE_WEEKLY_DEDUP = True\n",
    "\n",
    "base = Path(OUTPUT_PATH)\n",
    "yr = int(globals().get(\"calendar_year\", datetime.now().year))\n",
    "wk = int(globals().get(\"week_number\", datetime.now().isocalendar().week))\n",
    "\n",
    "# Pick weekly file (year-aware first, legacy fallback)\n",
    "candidates = [\n",
    "    base / f\"weekly_campaign_schedule_{yr}_week_{wk}.json\",\n",
    "    base / f\"weekly_campaign_schedule_week_{wk}.json\",\n",
    "]\n",
    "weekly = next((p for p in candidates if p.exists()), None)\n",
    "print(f\"📅 Using: {weekly.name if weekly else '—'}  | Exists: {bool(weekly)}\")\n",
    "if not weekly:\n",
    "    raise SystemExit(\"No weekly schedule JSON found to hydrate.\")\n",
    "\n",
    "hist_path = base / \"history\" / \"wine_campaign_history.json\"\n",
    "if not hist_path.exists():\n",
    "    raise SystemExit(\"No wine_campaign_history.json found. Run the history cell first.\")\n",
    "\n",
    "hist = json.loads(hist_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# Accept both nested and flat history shapes\n",
    "if \"by_id\" in hist:\n",
    "    last_by_id   = {str(k): (v.get(\"last_campaign_date\",\"\") if isinstance(v,dict) else str(v or \"\")) for k, v in hist.get(\"by_id\", {}).items()}\n",
    "    last_by_name = {str(k): (v.get(\"last_campaign_date\",\"\") if isinstance(v,dict) else str(v or \"\")) for k, v in hist.get(\"by_name\", {}).items()}\n",
    "    emails_by_id   = {str(k): int(v or 0) for k, v in hist.get(\"emails_sent_by_id\", {}).items()}\n",
    "    emails_by_name = {str(k): int(v or 0) for k, v in hist.get(\"emails_sent_by_name\", {}).items()}\n",
    "else:\n",
    "    # Flat map from Cell 5: { key -> {wine, vintage, last_campaign_date, emails_total} }\n",
    "    last_by_id, last_by_name, emails_by_id, emails_by_name = {}, {}, {}, {}\n",
    "    for k, val in hist.items():\n",
    "        if isinstance(val, dict):\n",
    "            lcd = val.get(\"last_campaign_date\", \"\") or \"\"\n",
    "            # NOTE: use emails_total (Cell 5) with fallback to emails_sent\n",
    "            eml = val.get(\"emails_total\", val.get(\"emails_sent\", 0)) or 0\n",
    "        else:\n",
    "            lcd, eml = (str(val or \"\"), 0)\n",
    "        key = str(k)\n",
    "        if re.fullmatch(r\"\\d+\", key):\n",
    "            last_by_id[key] = lcd\n",
    "            emails_by_id[key] = int(eml)\n",
    "        else:\n",
    "            last_by_name[key] = lcd\n",
    "            emails_by_name[key] = int(eml)\n",
    "\n",
    "# Optional: supplement from campaign_index.json when available\n",
    "try:\n",
    "    ci_path = base / \"campaign_index.json\"\n",
    "    if ci_path.exists():\n",
    "        ci = json.loads(ci_path.read_text(encoding=\"utf-8\"))\n",
    "        for k, v in (ci.get(\"emails_sent_by_id\", {}) or {}).items():\n",
    "            emails_by_id[str(k)] = int(v or 0)\n",
    "        for k, v in (ci.get(\"emails_sent_by_name\", {}) or {}).items():\n",
    "            emails_by_name[str(k)] = int(v or 0)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "\n",
    "def _name_key(item: dict) -> str:\n",
    "    w = str(item.get(\"wine\") or item.get(\"name\") or \"\").strip()\n",
    "    v = str(item.get(\"vintage\") or \"NV\").strip()\n",
    "    return f\"{w}::{v}\"\n",
    "\n",
    "data = json.loads(weekly.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "changed = 0\n",
    "cards = 0\n",
    "for day, items in (data.items() if isinstance(data, dict) else []):\n",
    "    if not isinstance(items, list): \n",
    "        continue\n",
    "    for it in items:\n",
    "        if not isinstance(it, dict):\n",
    "            continue\n",
    "        cards += 1\n",
    "        wid = str(it.get(\"id\") or \"\").strip()\n",
    "        nk  = _name_key(it)\n",
    "\n",
    "        lcd = (last_by_id.get(wid) or last_by_name.get(nk) or \"\")\n",
    "        em  = emails_by_id.get(wid)\n",
    "        if em is None:\n",
    "            em = emails_by_name.get(nk, 0)\n",
    "        em = int(em or 0)\n",
    "\n",
    "        # detect change before overwriting\n",
    "        prev_lcd = it.get(\"last_campaign_date\", None)\n",
    "        prev_em  = it.get(\"emails_sent\", None)\n",
    "        if prev_lcd != lcd: changed += 1\n",
    "        if prev_em != em:   changed += 1\n",
    "\n",
    "        # 🔒 FORCE keys to exist on every card\n",
    "        it[\"last_campaign_date\"] = lcd\n",
    "        it[\"emails_sent\"] = em\n",
    "\n",
    "weekly.write_text(json.dumps(data, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# Verify across ALL cards (not just the first)\n",
    "missing_counts = {\"emails_sent\": 0, \"last_campaign_date\": 0}\n",
    "for day, items in (data.items() if isinstance(data, dict) else []):\n",
    "    if not isinstance(items, list): \n",
    "        continue\n",
    "    for it in items:\n",
    "        if \"emails_sent\" not in it: missing_counts[\"emails_sent\"] += 1\n",
    "        if \"last_campaign_date\" not in it: missing_counts[\"last_campaign_date\"] += 1\n",
    "\n",
    "print(f\"🧩 Cards total: {cards}\")\n",
    "print(\"✅ Hydration:\", \"updated fields\" if changed else \"no changes needed\")\n",
    "if any(missing_counts.values()):\n",
    "    print(\"❌ Missing keys:\", {k:v for k,v in missing_counts.items() if v})\n",
    "else:\n",
    "    print(\"✅ Required keys present on ALL cards: emails_sent, last_campaign_date\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f413a622-e46c-4cae-98cd-3ddd36a63cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 Weekly: weekly_campaign_schedule_2025_week_38.json | Cards: 35\n",
      "✅ Keys present on all cards\n",
      "ℹ️ Cards with any 'Unknown' label: 10\n",
      "❌ Duplicates by wine_key: 8\n",
      "✅ campaign_index.json keys present: by_id/by_name/meta_by_id\n",
      "✅ Card last_campaign_date matches history for sampled cards\n",
      "💰 Budget offers this week: 2 (target ≥ 3)\n",
      "⚠️ Fewer than 3 Budget offers detected — upstream Cell 12 should have injected extras. Re-run Cell 12 if needed.\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 17\n",
    "from pathlib import Path\n",
    "import json, re\n",
    "import pandas as pd\n",
    "\n",
    "base = Path(OUTPUT_PATH)\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Weekly file integrity\n",
    "# ---------------------------\n",
    "yr = int(globals().get(\"calendar_year\", pd.Timestamp.now().year))\n",
    "wk = int(globals().get(\"week_number\", pd.Timestamp.now().isocalendar()[1]))\n",
    "\n",
    "weekly = base / f\"weekly_campaign_schedule_{yr}_week_{wk}.json\"\n",
    "if not weekly.exists():\n",
    "    weekly = base / f\"weekly_campaign_schedule_week_{wk}.json\"\n",
    "\n",
    "if not weekly.exists():\n",
    "    print(f\"❌ No weekly schedule JSON found for {yr}-W{wk}. Aborting checks.\")\n",
    "    raise SystemExit(0)\n",
    "\n",
    "try:\n",
    "    data = json.loads(weekly.read_text(encoding=\"utf-8\"))\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to read weekly schedule: {e}\")\n",
    "    raise SystemExit(0)\n",
    "\n",
    "cards = sum(len(v) for v in data.values() if isinstance(v, list))\n",
    "missing = {\"emails_sent\": 0, \"last_campaign_date\": 0}\n",
    "unknowns = 0\n",
    "dupe_keys = set()\n",
    "seen_keys = set()\n",
    "\n",
    "def _wine_key(c):\n",
    "    # Prefer id; fallback to wine::vintage\n",
    "    wid = str(c.get(\"id\") or \"\").strip()\n",
    "    if wid:\n",
    "        return wid\n",
    "    w = str(c.get(\"wine\") or c.get(\"name\") or \"\").strip()\n",
    "    v = str(c.get(\"vintage\") or \"NV\").strip()\n",
    "    return f\"{w}::{v}\"\n",
    "\n",
    "for day, items in (data.items() if isinstance(data, dict) else []):\n",
    "    if not isinstance(items, list): \n",
    "        continue\n",
    "    for c in items:\n",
    "        # required keys\n",
    "        for k in missing:\n",
    "            if k not in c:\n",
    "                missing[k] += 1\n",
    "        # “Unknown” labels\n",
    "        if (c.get(\"wine\") in (None,\"\",\"Unknown\")) or \\\n",
    "           (c.get(\"region_group\") in (None,\"\",\"Unknown\")) or \\\n",
    "           (c.get(\"full_type\") in (None,\"\",\"Unknown\")):\n",
    "            unknowns += 1\n",
    "        # duplicate detection (per-week)\n",
    "        key = _wine_key(c)\n",
    "        if key in seen_keys: \n",
    "            dupe_keys.add(key)\n",
    "        else: \n",
    "            seen_keys.add(key)\n",
    "\n",
    "print(f\"📅 Weekly: {weekly.name} | Cards: {cards}\")\n",
    "print(\"✅ Keys present on all cards\" if not any(missing.values()) else f\"❌ Missing keys: {missing}\")\n",
    "print(f\"ℹ️ Cards with any 'Unknown' label: {unknowns}\")\n",
    "print(\"✅ No per-week duplicates by wine_key\" if not dupe_keys else f\"❌ Duplicates by wine_key: {len(dupe_keys)}\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2) History / campaign_index parity & lookups\n",
    "# ---------------------------------------------\n",
    "hist_path = base / \"history\" / \"wine_campaign_history.json\"\n",
    "idx_path  = base / \"campaign_index.json\"\n",
    "\n",
    "hist = {}\n",
    "if hist_path.exists():\n",
    "    try:\n",
    "        hist = json.loads(hist_path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not parse history file: {e}\")\n",
    "else:\n",
    "    print(\"⚠️ history/wine_campaign_history.json not found (hydration still okay if Cell 16 ran).\")\n",
    "\n",
    "idx = {}\n",
    "if idx_path.exists():\n",
    "    try:\n",
    "        idx = json.loads(idx_path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not parse campaign_index.json: {e}\")\n",
    "\n",
    "def _flat_hist_get_last(k):\n",
    "    \"\"\"Tolerate both flat and structured history payloads.\"\"\"\n",
    "    if isinstance(hist, dict) and \"by_id\" in hist:\n",
    "        v = hist.get(\"by_id\", {}).get(k) or hist.get(\"by_name\", {}).get(k) or {}\n",
    "        return v.get(\"last_campaign_date\",\"\") if isinstance(v, dict) else (v or \"\")\n",
    "    v = hist.get(k, {})\n",
    "    return v.get(\"last_campaign_date\",\"\") if isinstance(v, dict) else (v or \"\")\n",
    "\n",
    "if idx:\n",
    "    if all(k in idx for k in (\"by_id\",\"by_name\",\"meta_by_id\")):\n",
    "        print(\"✅ campaign_index.json keys present: by_id/by_name/meta_by_id\")\n",
    "    else:\n",
    "        print(\"❌ campaign_index.json missing expected keys (by_id/by_name/meta_by_id)\")\n",
    "\n",
    "# spot-check (one mismatch per day max, like a sample)\n",
    "mismatch = 0\n",
    "for day, items in (data.items() if isinstance(data, dict) else []):\n",
    "    if not isinstance(items, list): \n",
    "        continue\n",
    "    for c in items:\n",
    "        wid = str(c.get(\"id\") or \"\").strip()\n",
    "        nk  = f\"{str(c.get('wine') or c.get('name') or '').strip()}::{str(c.get('vintage') or 'NV').strip()}\"\n",
    "        lcd_hist = _flat_hist_get_last(wid or nk)\n",
    "        if (c.get(\"last_campaign_date\") or \"\") != (lcd_hist or \"\"):\n",
    "            mismatch += 1\n",
    "            break\n",
    "print(\"✅ Card last_campaign_date matches history for sampled cards\" if mismatch==0 else f\"⚠️ {mismatch} day(s) show a mismatch vs history\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 3) Budget coverage sanity (need ≥ 3 per week)\n",
    "# ---------------------------------------------\n",
    "def _canon_tier(x):\n",
    "    s = str(x or \"\").strip().lower().replace(\"-\", \" \")\n",
    "    if \"ultra\" in s: return \"Ultra Luxury\"\n",
    "    if \"luxury\" in s: return \"Luxury\"\n",
    "    if \"premium\" in s: return \"Premium\"\n",
    "    if \"mid\" in s: return \"Mid-range\"\n",
    "    if \"budget\" in s: return \"Budget\"\n",
    "    return s.title() if s else \"\"\n",
    "\n",
    "budget_list = []\n",
    "for day, items in (data.items() if isinstance(data, dict) else []):\n",
    "    if not isinstance(items, list): \n",
    "        continue\n",
    "    for c in items:\n",
    "        tier = _canon_tier(c.get(\"price_tier\"))\n",
    "        if tier == \"Budget\":\n",
    "            budget_list.append((day, c.get(\"wine\") or c.get(\"name\") or c.get(\"id\")))\n",
    "\n",
    "print(f\"💰 Budget offers this week: {len(budget_list)} (target ≥ 3)\")\n",
    "if len(budget_list) < 3:\n",
    "    print(\"⚠️ Fewer than 3 Budget offers detected — upstream Cell 12 should have injected extras. Re-run Cell 12 if needed.\")\n",
    "else:\n",
    "    # quick summary by day\n",
    "    per_day = {}\n",
    "    for d, _ in budget_list:\n",
    "        per_day[d] = per_day.get(d, 0) + 1\n",
    "    print(\"   Breakdown by day:\", per_day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a629813-b6e9-4076-af20-1cc474a35409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 Weekly: weekly_campaign_schedule_2025_week_38.json\n",
      "🧽 De-dup complete → removed 0 duplicate card(s).\n",
      "🔢 Cards now per day: {'Monday': 5, 'Tuesday': 5, 'Wednesday': 5, 'Thursday': 5, 'Friday': 5, 'Saturday': 5, 'Sunday': 5}\n",
      "🎉 Wednesday overflow: UNLIMITED\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 18: Weekly de-dup (one card per wine_key; allow Wednesday overflow) ---\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "\n",
    "NUM_SLOTS = int(globals().get(\"NUM_SLOTS\", 5))\n",
    "DAYS_FULL = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    "\n",
    "# ---- Config: Wednesday overflow ----\n",
    "# If WEDNESDAY_MAX_SLOTS is:\n",
    "#   - A number  -> use that many slots on Wednesday\n",
    "#   - \"0\" or \"\" -> treat as unlimited\n",
    "# Not set       -> unlimited (i.e., don't clamp Wednesday)\n",
    "_wed_cap_env = str(os.getenv(\"WEDNESDAY_MAX_SLOTS\", \"\")).strip()\n",
    "if _wed_cap_env == \"\" or _wed_cap_env == \"0\":\n",
    "    WEDNESDAY_CAP = None  # unlimited\n",
    "else:\n",
    "    try:\n",
    "        WEDNESDAY_CAP = int(_wed_cap_env)\n",
    "    except Exception:\n",
    "        WEDNESDAY_CAP = None  # default unlimited if parse fails\n",
    "\n",
    "base = Path(OUTPUT_PATH)\n",
    "yr = int(globals().get(\"calendar_year\", pd.Timestamp.now().year))\n",
    "wk = int(globals().get(\"week_number\", pd.Timestamp.now().isocalendar()[1]))\n",
    "\n",
    "weekly = base / f\"weekly_campaign_schedule_{yr}_week_{wk}.json\"\n",
    "if not weekly.exists():\n",
    "    weekly = base / f\"weekly_campaign_schedule_week_{wk}.json\"\n",
    "\n",
    "data = json.loads(weekly.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "def wine_key(card: dict) -> str:\n",
    "    _id = str(card.get(\"id\") or \"\").strip()\n",
    "    if _id:\n",
    "        return _id\n",
    "    w = str(card.get(\"wine\") or card.get(\"name\") or \"\").strip()\n",
    "    v = str(card.get(\"vintage\") or \"NV\").strip()\n",
    "    return f\"{w}::{v}\"\n",
    "\n",
    "def parse_date(s):\n",
    "    if not s: return pd.NaT\n",
    "    try:\n",
    "        return pd.to_datetime(s, errors=\"coerce\").date()\n",
    "    except Exception:\n",
    "        return pd.NaT\n",
    "\n",
    "def _to_int_or0(x):\n",
    "    try:\n",
    "        v = pd.to_numeric(x, errors=\"coerce\")\n",
    "        return int(v) if pd.notna(v) else 0\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "# Flatten with day info for stable ordering\n",
    "rows = []\n",
    "for d in DAYS_FULL:\n",
    "    for c in (data.get(d) or []):\n",
    "        rows.append({\n",
    "            \"day\": d,\n",
    "            \"card\": c,\n",
    "            \"wine_key\": wine_key(c),\n",
    "            \"locked\": bool(c.get(\"locked\", False)),\n",
    "            \"emails_sent\": _to_int_or0(c.get(\"emails_sent\")),\n",
    "            \"last_campaign_date\": parse_date(c.get(\"last_campaign_date\"))\n",
    "        })\n",
    "\n",
    "# Rank: Locked > higher emails_sent > newer last_campaign_date\n",
    "def rank_tuple(r):\n",
    "    return (\n",
    "        1 if r[\"locked\"] else 0,\n",
    "        r[\"emails_sent\"] or 0,\n",
    "        r[\"last_campaign_date\"] or date.min\n",
    "    )\n",
    "\n",
    "keep_for_key = {}\n",
    "first_seen_day_order = {d: i for i, d in enumerate(DAYS_FULL)}\n",
    "\n",
    "for r in rows:\n",
    "    k = r[\"wine_key\"]\n",
    "    if k not in keep_for_key:\n",
    "        keep_for_key[k] = r\n",
    "    else:\n",
    "        cur = keep_for_key[k]\n",
    "        r_rank, c_rank = rank_tuple(r), rank_tuple(cur)\n",
    "        if r_rank > c_rank:\n",
    "            keep_for_key[k] = r\n",
    "        elif r_rank == c_rank:\n",
    "            # prefer the one that appears earlier in the week\n",
    "            if first_seen_day_order[r[\"day\"]] < first_seen_day_order[cur[\"day\"]]:\n",
    "                keep_for_key[k] = r\n",
    "\n",
    "# Rebuild the calendar: preserve original day order, clamp per-day with Wednesday overflow\n",
    "deduped = {d: [] for d in DAYS_FULL}\n",
    "kept = {id(r): True for r in keep_for_key.values()}  # identity set for speed\n",
    "\n",
    "for d in DAYS_FULL:\n",
    "    day_items = [r for r in rows if r[\"day\"] == d and kept.get(id(keep_for_key[r[\"wine_key\"]]), False)]\n",
    "    # keep original order within the day\n",
    "    deduped[d] = [r[\"card\"] for r in day_items]\n",
    "\n",
    "    # clamp: all days to NUM_SLOTS, except Wednesday (optional overflow)\n",
    "    if d != \"Wednesday\":\n",
    "        deduped[d] = deduped[d][:NUM_SLOTS]\n",
    "    else:\n",
    "        if WEDNESDAY_CAP is not None:  # only clamp if an explicit cap is set\n",
    "            deduped[d] = deduped[d][:WEDNESDAY_CAP]\n",
    "\n",
    "# Write back atomically\n",
    "tmp = weekly.with_suffix(weekly.suffix + \".tmp\")\n",
    "tmp.write_text(json.dumps(deduped, indent=2), encoding=\"utf-8\")\n",
    "os.replace(tmp, weekly)\n",
    "\n",
    "# Diagnostics\n",
    "orig_cards = sum(len(v) for v in data.values() if isinstance(v, list))\n",
    "new_cards  = sum(len(v) for v in deduped.values() if isinstance(v, list))\n",
    "dupes_removed = orig_cards - new_cards\n",
    "\n",
    "print(f\"📅 Weekly: {weekly.name}\")\n",
    "print(f\"🧽 De-dup complete → removed {dupes_removed} duplicate card(s).\")\n",
    "print(\"🔢 Cards now per day:\", {d: len(deduped[d]) for d in DAYS_FULL})\n",
    "if WEDNESDAY_CAP is None:\n",
    "    print(\"🎉 Wednesday overflow: UNLIMITED\")\n",
    "else:\n",
    "    print(f\"🎉 Wednesday overflow: capped at {WEDNESDAY_CAP}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9026b2b7-e587-4732-9ff4-2dcc68ba8d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+ Extras from stock (optional/tooltips):\n",
      "{\n",
      "  \"producer\": \"Marchesi Mazzei Castello di Fonterutoli\",\n",
      "  \"chf_price\": 975.0,\n",
      "  \"grape_list\": \"Red Blend\",\n",
      "  \"bottle_size_ml\": 1800,\n",
      "  \"type_class\": \"Red\",\n",
      "  \"body\": 4,\n",
      "  \"sweetness\": 3,\n",
      "  \"occasion\": \"Gifting\",\n",
      "  \"avg_score\": 95.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 19: Make `extras` JSON-serializable (deep-safe) ---\n",
    "import json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from decimal import Decimal\n",
    "\n",
    "def _to_py_scalar(o):\n",
    "    \"\"\"Convert a single scalar to a JSON-serializable Python type.\"\"\"\n",
    "    # pandas / numpy missing\n",
    "    if o is None or (isinstance(o, float) and math.isnan(o)):\n",
    "        return None\n",
    "    try:\n",
    "        if pd.isna(o):  # handles NaN/NaT/None gracefully\n",
    "            return None\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # pandas time-like\n",
    "    if isinstance(o, pd.Timestamp):\n",
    "        return o.isoformat()\n",
    "    if isinstance(o, pd.Timedelta):\n",
    "        return o.total_seconds()\n",
    "\n",
    "    # stdlib time-like\n",
    "    if isinstance(o, (dt.datetime, dt.date, dt.time)):\n",
    "        return o.isoformat()\n",
    "\n",
    "    # numpy scalars\n",
    "    if isinstance(o, (np.integer,)):\n",
    "        return int(o)\n",
    "    if isinstance(o, (np.floating,)):\n",
    "        return float(o)\n",
    "    if isinstance(o, (np.bool_,)):\n",
    "        return bool(o)\n",
    "    if isinstance(o, (np.datetime64,)):\n",
    "        ts = pd.to_datetime(o, errors=\"coerce\")\n",
    "        return ts.isoformat() if pd.notna(ts) else None\n",
    "\n",
    "    # decimals\n",
    "    if isinstance(o, Decimal):\n",
    "        return float(o)\n",
    "\n",
    "    return o\n",
    "\n",
    "def _jsonify(o):\n",
    "    \"\"\"Deep-convert dict/list/tuple/set/Series/DataFrame into JSON-serializable types.\"\"\"\n",
    "    if isinstance(o, dict):\n",
    "        return {str(k): _jsonify(v) for k, v in o.items()}\n",
    "    if isinstance(o, (list, tuple, set)):\n",
    "        return [_jsonify(v) for v in o]\n",
    "    if isinstance(o, np.ndarray):\n",
    "        return [_jsonify(v) for v in o.tolist()]\n",
    "    if isinstance(o, pd.Series):\n",
    "        return _jsonify(o.to_dict())\n",
    "    if isinstance(o, pd.DataFrame):\n",
    "        return [_jsonify(r) for r in o.to_dict(orient=\"records\")]\n",
    "    # scalar / other\n",
    "    return _to_py_scalar(o)\n",
    "\n",
    "# Convert and print\n",
    "extras_py = _jsonify(extras) if 'extras' in globals() else {}\n",
    "print(\"\\n+ Extras from stock (optional/tooltips):\")\n",
    "print(json.dumps(extras_py, indent=2, ensure_ascii=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda avu-base)",
   "language": "python",
   "name": "avu-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
